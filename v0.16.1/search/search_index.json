{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Trivy has a native Kubernetes Operator which continuously scans your Kubernetes cluster for security issues, and generates security reports as Kubernetes Custom Resources. It does it by watching Kubernetes for state changes and automatically triggering scans in response to changes, for example initiating a vulnerability scan when a new Pod is created.</p> <p>Trivy Operator is based on existing Aqua OSS project - [Starboard], and shares some of the design, principles and code with it. Existing content that relates to Starboard Operator might also be relevant for Trivy Operator. To learn more about the transition from Starboard to Trivy, see the announcement discussion.</p> Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects."},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#why-do-you-duplicate-instances-of-vulnerabilityreports-for-the-same-image-digest","title":"Why do you duplicate instances of VulnerabilityReports for the same image digest?","text":"<p>Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Trivy-operator relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command:</p> <pre><code>kubectl get vulnerabilityreports \\\n  -l trivy-operator.resource.kind=Deployment \\\n  -l trivy-operator.resource.name=wordpress\n</code></pre> <p>Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the <code>wordpress</code> Deployment is deleted, all related VulnerabilityReports are automatically garbage collected.</p>"},{"location":"faq/#why-do-you-create-an-instance-of-the-vulnerabilityreport-for-each-container","title":"Why do you create an instance of the VulnerabilityReport for each container?","text":"<p>The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.</p>"},{"location":"settings/","title":"Settings","text":"<p>Trivy Operator read configuration settings from ConfigMaps, as well as Secrets that holds confidential settings (such as a GitHub token). Trivy-Operator plugins read configuration and secret data from ConfigMaps and Secrets named after the plugin. For example, Trivy configuration is stored in the ConfigMap and Secret named <code>trivy-operator-trivy-config</code>.</p> <p>You can change the default settings with <code>kubectl patch</code> or <code>kubectl edit</code> commands. For example, by default Trivy displays vulnerabilities with all severity levels (<code>UNKNOWN</code>, <code>LOW</code>, <code>MEDIUM</code>, <code>HIGH</code>, <code>CRITICAL</code>). However, you can display only <code>HIGH</code> and <code>CRITICAL</code> vulnerabilities by patching the <code>trivy.severity</code> value in the <code>trivy-operator-trivy-config</code> ConfigMap:</p> <pre><code>TRIVY_OPERATOR_NAMESPACE=&lt;your trivy operator namespace&gt;\n</code></pre> <pre><code>kubectl patch cm trivy-operator-trivy-config -n $TRIVY_OPERATOR_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.severity\": \"HIGH,CRITICAL\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>To set the GitHub token used by Trivy add the <code>trivy.githubToken</code> value to the <code>trivy-operator-trivy-config</code> Secret:</p> <pre><code>TRIVY_OPERATOR_NAMESPACE=&lt;your trivy opersator namespace&gt;\nGITHUB_TOKEN=&lt;your token&gt;\n</code></pre> <pre><code>kubectl patch secret trivy-operator-trivy-config -n $TRIVY_OPERATOR_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>The following table lists available settings with their default values. Check plugins' documentation to see configuration settings for common use cases. For example, switch Trivy from [Standalone] to ClientServer mode.</p> CONFIG   KEY DEFAULT DESCRIPTION <code>vulnerabilityReports.scanner</code> <code>Trivy</code> The name of the plugin that generates vulnerability reports. Either <code>Trivy</code> or <code>Aqua</code>. <code>vulnerabilityReports.scanJobsInSameNamespace</code> <code>\"false\"</code> Whether to run vulnerability scan jobs in same namespace of workload. Set <code>\"true\"</code> to enable. <code>configAuditReports.scanner</code> <code>Trivy</code> The name of the plugin that generates config audit reports. <code>scanJob.tolerations</code> N/A JSON representation of the tolerations to be applied to the scanner pods and node-collector so that they can run on nodes with matching taints. Example: <code>'[{\"key\":\"key1\", \"operator\":\"Equal\", \"value\":\"value1\", \"effect\":\"NoSchedule\"}]'</code> <code>nodeCollector.volumeMounts</code> see helm/values.yaml node-collector pod volumeMounts definition for collecting config files information <code>nodeCollector.volumes</code> see helm/values.yaml node-collector pod volumes definition for collecting config files information <code>scanJob.nodeSelector</code> N/A JSON representation of the [nodeSelector] to be applied to the scanner pods so that they can run on nodes with matching labels. Example: <code>'{\"example.com/node-type\":\"worker\", \"cpu-type\": \"sandylake\"}'</code> <code>scanJob.annotations</code> N/A One-line comma-separated representation of the annotations which the user wants the scanner pods to be annotated with. Example: <code>foo=bar,env=stage</code> will annotate the scanner pods with the annotations <code>foo: bar</code> and <code>env: stage</code> <code>scanJob.templateLabel</code> N/A One-line comma-separated representation of the template labels which the user wants the scanner pods to be labeled with. Example: <code>foo=bar,env=stage</code> will labeled the scanner pods with the labels <code>foo: bar</code> and <code>env: stage</code> <code>scanJob.podTemplatePodSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner and node collector pods to be secured with. Example: <code>{\"RunAsUser\": 1000, \"RunAsGroup\": 1000, \"RunAsNonRoot\": true}</code> <code>scanJob.podTemplateContainerSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner and node collector containers (and their initContainers) to be amended with. Example: <code>{\"allowPrivilegeEscalation\": false, \"capabilities\": { \"drop\": [\"ALL\"]},\"privileged\": false, \"readOnlyRootFilesystem\": true }</code> <code>scanJob.podPriorityClassName</code> <code>\"\"</code> The value of the priorityClassName for job <code>compliance.failEntriesLimit</code> <code>\"10\"</code> Limit the number of fail entries per control check in the cluster compliance detail report. <code>compliance.reportType</code> <code>summary</code> this flag control the type of report generated summary or all <code>compliance.cron</code> <code>0 */6 * * *</code> this flag control the cron interval for compliance report generation <code>scanJob.compressLogs</code> <code>\"true\"</code> Control whether scanjob output should be compressed <code>nodeCollector.excludeNodes</code> <code>\"\"</code> excludeNodes comma-separated node labels that the node-collector job should exclude from scanning (example kubernetes.io/arch=arm64,team=dev) <p>Tip</p> <p>You can delete a configuration key.For example, the following <code>kubectl patch</code> command deletes the <code>trivy.httpProxy</code> key: <pre><code>TRIVY_OPERATOR_NAMESPACE=&lt;your trivy operator namespace&gt;\n</code></pre> <pre><code>kubectl patch cm trivy-operator-trivy-config -n $TRIVY_OPERATOR_NAMESPACE \\\n  --type json \\\n  -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'\n</code></pre></p>"},{"location":"docs/compliance/compliance/","title":"Compliance Reports","text":"<p>EXPERIMENTAL</p> <p>This feature might change without preserving backwards compatibility.</p> <p>Trivy-operator compliance crds allow you create a specific set of checks into a report. There are hundreds of different checks for many different components and configurations, but sometimes you already know which specific checks you are interested in. Often this would be an industry accepted set of checks such as CIS, or some vendor specific guideline, or your own organization policy that you want to comply with. These are all possible using the flexible compliance infrastructure that's built into Trivy-operator. Compliance reports are defined as simple YAML documents that select checks to include in the report.</p> <p>The compliance report will be generated every six hours by default.</p> <p>The compliance report is composed of two parts :</p> <ul> <li> <p><code>spec</code>: represents the compliance control checks specification, check details, and the mapping to the security scanner</p> </li> <li> <p><code>status</code>: represents the compliance control checks results</p> </li> <li> <p><code>report types</code> : compliance report can be produced in two formats , summary and detail (all)</p> </li> </ul> <p>Spec can be customized by amending the control checks <code>report type (summary / all)</code> , <code>severity</code> and <code>cron</code> expression (report execution interval). As an example, let's enter <code>vi</code> edit mode and change the <code>cron</code> expression. <pre><code>kubectl edit compliance\n</code></pre> Once the report has been generated, you can fetch and review its results section. As an example, let's fetch the compliance status report in JSON format</p> <pre><code>kubectl get compliance nsa  -o=jsonpath='{.status}' | jq .\n</code></pre>"},{"location":"docs/compliance/compliance/#you-can-create-your-own-custom-compliance-report-a-compliance-report-is-a-simple-yaml-document-in-the-following-format","title":"You can create your own custom compliance report. A compliance report is a simple YAML document in the following format:","text":"customer compliance report <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\ncreationTimestamp: \"2022-12-04T18:25:27Z\"\nlabels:\napp.kubernetes.io/instance: trivy-operator\napp.kubernetes.io/managed-by: kubectl\napp.kubernetes.io/name: trivy-operator\napp.kubernetes.io/version: 0.8.0\nname: nsa # report unique identifier. this should not container spaces.\nspec:\ncron: '* * * * *'\nreportType: summary\ncompliance:\ntitle: nsa # report title. Any one-line title.\ndescription: NSA, Kubernetes Hardening  # description of the report. Any text.\nversion: \"1.0\"\ncontrols:\n- checks:\n- id: AVD-KSV-0012 # check ID. Must start with `AVD-` \ndescription: Check that container is not running as root   # Description (appears in the report as is). Any text.\nid: \"1.0\"\nname: Non-root containers # Name for the control (appears in the report as is). Any one-line name.\nseverity: MEDIUM  (note that checks severity isn't used)\n</code></pre>"},{"location":"docs/compliance/compliance/#built-in-reports","title":"Built in reports","text":"<p>The following reports are available out of the box:</p> Compliance Name for command More info NSA, CISA Kubernetes Hardening Guidance v1.2 <code>nsa</code> Link CIS Benchmark for Kubernetes v1.23 <code>cis</code> Link Kubernetes pss-restricted <code>pss-restricted</code> Link Kubernetes pss-baseline <code>pss-baseline</code> Link"},{"location":"docs/configuration-auditing/","title":"Configuration Auditing","text":""},{"location":"docs/configuration-auditing/#setting","title":"Setting","text":"<p>the following flags can be set with the <code>trivy-operator-trivy-cofnig</code> configmap in order to impact scanning</p> CONFIGMAP KEY DEFAULT DESCRIPTION <code>trivy.useBuiltinRegoPolicies</code> <code>true</code> The Flag to enable the usage of builtin rego policies by default <code>trivy.supportedConfigAuditKinds</code> <code>Workload,Service,Role,ClusterRole,NetworkPolicy,Ingress,LimitRange,ResourceQuota</code> The Flag is the list of supported kinds separated by comma delimiter to be scanned by the config audit scanner <p>As your organization deploys containerized workloads in Kubernetes environments, you will be faced with many configuration choices related to images, containers, control plane, and data plane. Setting these configurations improperly creates a high-impact security and compliance risk. DevOps, and platform owners need the ability to continuously assess build artifacts, workloads, and infrastructure against configuration hardening standards to remediate any violations.</p> <p>trivy-operator configuration audit capabilities are purpose-built for Kubernetes environments. In particular, trivy Operator continuously checks images, workloads, and Kubernetes infrastructure components against common configurations security standards and generates detailed assessment reports, which are then stored in the default Kubernetes database.</p> <p>Kubernetes applications and other core configuration objects, such as Ingress, NetworkPolicy and ResourceQuota resources, are evaluated against Built-in Policies.  Additionally, application and infrastructure owners can integrate these reports into incident response workflows for active remediation.</p>"},{"location":"docs/configuration-auditing/built-in-policies/","title":"Built-in Configuration Audit Policies","text":"<p>The following sections list built-in configuration audit policies installed with trivy-operator. They are stored in the <code>trivy-operator-policies-config</code> ConfigMap created in the installation namespace (e.g. <code>trivy-system</code>). You can modify them or add a new policy. For example, follow the [Writing Custom Configuration Audit Policies] tutorial to add a custom policy that checks for recommended Kubernetes labels on any resource kind.</p>"},{"location":"docs/configuration-auditing/built-in-policies/#processing-information","title":"Processing information","text":"<p>The Trivy Operator pulls the information from the defsec respository.</p> <p>Once the Trivy Operator is installed inside the Kubernetes cluster, it will perform the following processes:</p> <ol> <li>The node-collector will collect infrastructure data from node filesystem and processes</li> <li>It then outputs the data as JSON output </li> <li>The the Rego policies (cis-controls) from defsec are executed against the infrastructure data ( JSON ) and return evaluation results as ConfigAudit and Compliance reports</li> </ol>"},{"location":"docs/configuration-auditing/built-in-policies/#general","title":"General","text":"NAME DESCRIPTION KINDS CPU not limited Enforcing CPU limits prevents DoS via resource exhaustion. Workload CPU requests not specified When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention. Workload SYS_ADMIN capability added SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root. Workload Default capabilities not dropped The container should drop all default capabilities and add only those that are needed for its execution. Workload Root file system is not read-only An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk. Workload Memory not limited Enforcing memory limits prevents DoS via resource exhaustion. Workload Memory requests not specified When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention. Workload hostPath volume mounted with docker.sock Mounting docker.sock from the host can give the container full root access to the host. Workload Runs with low group ID Force the container to run with group ID &gt; 10000 to avoid conflicts with the host\u2019s user table. Workload Runs with low user ID Force the container to run with user ID &gt; 10000 to avoid conflicts with the host\u2019s user table. Workload Tiller Is Deployed Check if Helm Tiller component is deployed. Workload Image tag ':latest' used It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version. Workload"},{"location":"docs/configuration-auditing/built-in-policies/#advanced","title":"Advanced","text":"NAME DESCRIPTION KINDS Unused capabilities should be dropped (drop any) Security best practices require containers to run with minimal required capabilities. Workload hostAliases is set Managing /etc/hosts aliases can prevent the container engine from modifying the file after a pod\u2019s containers have already been started. Workload User Pods should not be placed in kube-system namespace ensure that User pods are not placed in kube-system namespace Workload Protecting Pod service account tokens ensure that Pod specifications disable the secret token being mounted by setting automountServiceAccountToken: false Workload Selector usage in network policies ensure that network policies selectors are applied to pods or namespaces to restricted ingress and egress traffic within the pod network NetworkPolicy limit range usage ensure limit range policy has configure in order to limit resource usage for namespaces or nodes LimitRange resource quota usage ensure resource quota policy has configure in order to limit aggregate resource usage within namespace ResourceQuota All container images must start with the *.azurecr.io domain Containers should only use images from trusted registries. Workload All container images must start with a GCR domain Containers should only use images from trusted GCR registries. Workload"},{"location":"docs/configuration-auditing/built-in-policies/#pod-security-standard","title":"Pod Security Standard","text":""},{"location":"docs/configuration-auditing/built-in-policies/#baseline","title":"Baseline","text":"NAME DESCRIPTION KINDS Access to host IPC namespace Sharing the host\u2019s IPC namespace allows container processes to communicate with processes on the host. Workload Access to host network Sharing the host\u2019s network namespace permits processes in the pod to communicate with processes bound to the host\u2019s loopback adapter. Workload Access to host PID Sharing the host\u2019s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration. Workload Privileged container Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges. Workload Non-default capabilities added Adding NET_RAW or capabilities beyond the default set must be disallowed. Workload hostPath volumes mounted HostPath volumes must be forbidden. Workload Access to host ports HostPorts should be disallowed, or at minimum restricted to a known list. Workload Default AppArmor profile not set A program inside the container can bypass AppArmor protection policies. Workload SELinux custom options set Setting a custom SELinux user or role option should be forbidden. Workload Non-default /proc masks set The default /proc masks are set up to reduce attack surface, and should be required. Workload Unsafe sysctl options set Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed 'safe' subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node. Workload"},{"location":"docs/configuration-auditing/built-in-policies/#restricted","title":"Restricted","text":"NAME DESCRIPTION KINDS Non-ephemeral volume types used In addition to restricting HostPath volumes, usage of non-ephemeral volume types should be limited to those defined through PersistentVolumes. Workload Process can elevate its own privileges A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node. Workload Runs as root user 'runAsNonRoot' forces the running image to run as a non-root user to ensure least privileges. Workload A root primary or supplementary GID set Containers should be forbidden from running with a root primary or supplementary GID. Workload Default Seccomp profile not set The RuntimeDefault seccomp profile must be required, or allow specific additional profiles. Workload"},{"location":"docs/configuration-auditing/built-in-policies/#rbac","title":"RBAC","text":"NAME DESCRIPTION KINDS Allow Role Clusterrolebindings Associate Privileged Cluster Role Check whether role permits creating role ClusterRoleBindings and association with privileged cluster role. Role/ClusterRole Deny Create Update Malicious Pod Check whether role permits update/create of a malicious pod contention. Role/ClusterRole Do Not Allow Role Binding Associate Privileged Role Check whether role permits creating role bindings and associating to privileged role/clusterrole. Role/ClusterRole No Attaching Shell Pods Check whether role permits attaching to shell on pods. Role/ClusterRole No Delete Pod Logs Used to cover attacker\u2019s tracks, but most clusters ship logs quickly off-cluster. Role/ClusterRole No Getting Shell Pods Check whether role permits getting shell on pods Role/ClusterRole No Impersonate Privileged Groups Do not allow impersonation of privileged groups Role/ClusterRole No Manage Configmaps Do not allow management of configmaps Role/ClusterRole No Manage Networking Resources Do not allow management of networking resources Role/ClusterRole No Manage Rbac Resources An effective level of access equivalent to cluster-admin should not be provided. Role/ClusterRole No Manage Secrets Do not allow management of secrets Role/ClusterRole Do Privilege Escalation From Node Proxy Do not allow privilege escalation from node proxy Role/ClusterRole No Wildcard Resource Role No wildcard resource roles Role/ClusterRole No Wildcard Verb Resource Role No wildcard verb and resource roles Role/ClusterRole No Wildcard Verb Role No wildcard verb roles Role/ClusterRole View All Secrets Do not allow users in a rolebinding to add other users to their rolebindings Role/ClusterRole"},{"location":"docs/crds/","title":"Overview","text":"<p>This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a programmable way.</p> NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport configauditreports configaudit,configaudits aquasecurity.github.io true ConfigAuditReport exposedsecretsreports exposedsecret,exposedsecrets aquasecurity.github.io true ExposedSecretReport rbacassessmentreports rbacassessmentreports,rbacassessmentreport aquasecurity.github.io true RbacAssessmentReport clusterrbacassessmentreports clusterrbacassessmentreports,clusterrbacassessmentreport aquasecurity.github.io true ClusterRbacAssessmentReport clusterinfraassessmentreports clusterinfraassessmentreports,clusterinfraassessmentreport aquasecurity.github.io true ClusterInfraAssessmentReport infraassessmentreports infraassessmentreports,infraassessmentreport aquasecurity.github.io true InfraAssessmentReport sbomreports sbomreports,sbomreport aquasecurity.github.io true SbomReport"},{"location":"docs/crds/clustercompliance-report/","title":"ClusterComplianceReport","text":"<p>The ClusterComplianceReport is a cluster-scoped resource, which represents the latest compliance control checks results. The report spec defines a mapping between pre-defined compliance control check ids to security scanners check ids. Currently, only <code>config-audit</code> security scanners are supported.</p>"},{"location":"docs/crds/clustercompliance-report/#built-in-reports","title":"Built in reports","text":"<p>The following reports are available out of the box:</p> Compliance Name for command More info NSA, CISA Kubernetes Hardening Guidance v1.2 <code>nsa</code> Link CIS Benchmark for Kubernetes v1.23 <code>cis</code> Link"},{"location":"docs/crds/clustercompliance-report/#the-compliance-report-structure","title":"The compliance report structure","text":"<ul> <li><code>spec:</code> represents the compliance control checks specification, check details, and the mapping to the security scanner   (this part is defined by the user)</li> <li><code>status:</code> represents the compliance control checks (as defined by spec mapping) results extracted from the security   scanners reports (this part is output by trivy-operator)</li> </ul> <p>The following shows a sample ClusterComplianceReport NSA specification associated with the <code>cluster</code> in summary format:</p> NSA, CISA Kubernetes Hardening Guidance v1.2 <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\ncreationTimestamp: \"2022-12-04T18:25:27Z\"\ngeneration: 3\nlabels:\napp.kubernetes.io/instance: trivy-operator\napp.kubernetes.io/managed-by: kubectl\napp.kubernetes.io/name: trivy-operator\napp.kubernetes.io/version: 0.8.0\nname: nsa\nresourceVersion: \"69736\"\nuid: d9991808-fb2f-4756-842f-8e9205e85b71\nspec:\ncompliance:\ncontrols:\n- checks:\n- id: AVD-KSV-0012\ndescription: Check that container is not running as root\nid: \"1.0\"\nname: Non-root containers\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0014\ndescription: Check that container root file system is immutable\nid: \"1.1\"\nname: Immutable container file systems\nseverity: LOW\n- checks:\n- id: AVD-KSV-0017\ndescription: Controls whether Pods can run privileged containers\nid: \"1.2\"\nname: Preventing privileged containers\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0008\ndescription: Controls whether containers can share process namespaces\nid: \"1.3\"\nname: Share containers process namespaces\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0009\ndescription: Controls whether share host process namespaces\nid: \"1.4\"\nname: Share host process namespaces\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0010\ndescription: Controls whether containers can use the host network\nid: \"1.5\"\nname: Use the host network\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0029\ndescription: Controls whether container applications can run with root privileges\nor with root group membership\nid: \"1.6\"\nname: Run with root privileges or with root group membership\nseverity: LOW\n- checks:\n- id: AVD-KSV-0001\ndescription: Control check restrictions escalation to root privileges\nid: \"1.7\"\nname: Restricts escalation to root privileges\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0002\ndescription: Control checks if pod sets the SELinux context of the container\nid: \"1.8\"\nname: Sets the SELinux context of the container\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0030\ndescription: Control checks the restriction of containers access to resources\nwith AppArmor\nid: \"1.9\"\nname: Restrict a container's access to resources with AppArmor\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0030\ndescription: Control checks the sets the seccomp profile used to sandbox containers\nid: \"1.10\"\nname: Sets the seccomp profile used to sandbox containers.\nseverity: LOW\n- checks:\n- id: AVD-KSV-0036\ndescription: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\nfalse'\nid: \"1.11\"\nname: Protecting Pod service account tokens\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0037\ndefaultStatus: FAIL\ndescription: Control check whether Namespace kube-system is not be used by users\nid: \"1.12\"\nname: Namespace kube-system should not be used by users\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0038\ndefaultStatus: FAIL\ndescription: Control check validate the pod and/or namespace Selectors usage\nid: \"2.0\"\nname: Pod and/or namespace Selectors usage\nseverity: MEDIUM\n- defaultStatus: FAIL\ndescription: Control check whether check cni plugin installed\nid: \"3.0\"\nname: Use CNI plugin that supports NetworkPolicy API (Manual)\nseverity: CRITICAL\n- checks:\n- id: AVD-KSV-0040\ndefaultStatus: FAIL\ndescription: Control check the use of ResourceQuota policy to limit aggregate\nresource usage within namespace\nid: \"4.0\"\nname: Use ResourceQuota policies to limit resources\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0039\ndefaultStatus: FAIL\ndescription: Control check the use of LimitRange policy limit resource usage\nfor namespaces or nodes\nid: \"4.1\"\nname: Use LimitRange policies to limit resources\nseverity: MEDIUM\n- defaultStatus: FAIL\ndescription: Control check whether control plan disable insecure port\nid: \"5.0\"\nname: Control plan disable insecure port (Manual)\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0030\ndescription: Control check whether etcd communication is encrypted\nid: \"5.1\"\nname: Encrypt etcd communication\nseverity: CRITICAL\n- defaultStatus: FAIL\ndescription: Control check whether kube config file permissions\nid: \"6.0\"\nname: Ensure kube config file permission (Manual)\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0029\ndescription: Control checks whether encryption resource has been set\nid: \"6.1\"\nname: Check that encryption resource has been set\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0004\ndescription: Control checks whether encryption provider has been set\nid: \"6.2\"\nname: Check encryption provider\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0001\ndescription: Control checks whether anonymous-auth is unset\nid: \"7.0\"\nname: Make sure anonymous-auth is unset\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0008\ndescription: Control check whether RBAC permission is in use\nid: \"7.1\"\nname: Make sure -authorization-mode=RBAC\nseverity: CRITICAL\n- defaultStatus: FAIL\ndescription: Control check whether audit policy is configure\nid: \"8.0\"\nname: Audit policy is configure (Manual)\nseverity: HIGH\n- checks:\n- id: AVD-KCV-0019\ndescription: Control check whether audit log path is configure\nid: \"8.1\"\nname: Audit log path is configure\nseverity: MEDIUM\n- checks:\n- id: AVD-KCV-0020\ndescription: Control check whether audit log aging is configure\nid: \"8.2\"\nname: Audit log aging\nseverity: MEDIUM\ndescription: National Security Agency - Kubernetes Hardening Guidance\nid: \"0001\"\nrelatedResources:\n- https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/\ntitle: nsa\nversion: \"1.0\"\ncron: '* * * * *'\nreportType: summary\nstatus:\nsummaryReport:\ncontrolCheck:\n- id: \"1.0\"\nname: Non-root containers\nseverity: MEDIUM\ntotalFail: 8\n- id: \"1.1\"\nname: Immutable container file systems\nseverity: LOW\ntotalFail: 7\n- id: \"1.2\"\nname: Preventing privileged containers\nseverity: HIGH\ntotalFail: 1\n- id: \"1.3\"\nname: Share containers process namespaces\nseverity: HIGH\ntotalFail: 0\n- id: \"1.4\"\nname: Share host process namespaces\nseverity: HIGH\ntotalFail: 6\n- id: \"1.5\"\nname: Use the host network\nseverity: HIGH\ntotalFail: 0\n- id: \"1.6\"\nname: Run with root privileges or with root group membership\nseverity: LOW\ntotalFail: 0\n- id: \"1.7\"\nname: Restricts escalation to root privileges\nseverity: MEDIUM\ntotalFail: 7\n- id: \"1.8\"\nname: Sets the SELinux context of the container\nseverity: MEDIUM\ntotalFail: 0\n- id: \"1.9\"\nname: Restrict a container's access to resources with AppArmor\nseverity: MEDIUM\ntotalFail: 8\n- id: \"1.10\"\nname: Sets the seccomp profile used to sandbox containers.\nseverity: LOW\ntotalFail: 8\n- id: \"1.11\"\nname: Protecting Pod service account tokens\nseverity: MEDIUM\ntotalFail: 0\n- id: \"1.12\"\nname: Namespace kube-system should not be used by users\nseverity: MEDIUM\ntotalFail: 4\n- id: \"2.0\"\nname: Pod and/or namespace Selectors usage\nseverity: MEDIUM\ntotalFail: 0\n- id: \"3.0\"\nname: Use CNI plugin that supports NetworkPolicy API (Manual)\nseverity: CRITICAL\n- id: \"4.0\"\nname: Use ResourceQuota policies to limit resources\nseverity: MEDIUM\ntotalFail: 0\n- id: \"4.1\"\nname: Use LimitRange policies to limit resources\nseverity: MEDIUM\ntotalFail: 0\n- id: \"5.0\"\nname: Control plan disable insecure port (Manual)\nseverity: CRITICAL\n- id: \"5.1\"\nname: Encrypt etcd communication\nseverity: CRITICAL\ntotalFail: 0\n- id: \"6.0\"\nname: Ensure kube config file permission (Manual)\nseverity: CRITICAL\n- id: \"6.1\"\nname: Check that encryption resource has been set\nseverity: CRITICAL\ntotalFail: 1\n- id: \"6.2\"\nname: Check encryption provider\nseverity: CRITICAL\ntotalFail: 0\n- id: \"7.0\"\nname: Make sure anonymous-auth is unset\nseverity: CRITICAL\ntotalFail: 1\n- id: \"7.1\"\nname: Make sure -authorization-mode=RBAC\nseverity: CRITICAL\ntotalFail: 0\n- id: \"8.0\"\nname: Audit policy is configure (Manual)\nseverity: HIGH\n- id: \"8.1\"\nname: Audit log path is configure\nseverity: MEDIUM\ntotalFail: 1\n- id: \"8.2\"\nname: Audit log aging\nseverity: MEDIUM\ntotalFail: 1\nid: \"0001\"\ntitle: nsa\ntotalCounts:\nfailCount: 12\npassCount: 15\nupdateTimestamp: \"2022-12-05T12:21:30Z\"\n</code></pre>  The following shows a sample ClusterComplianceReport NSA specification associated with the `cluster` in detail(all) format:  <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\nannotations:\ncreationTimestamp: \"2022-12-04T18:25:27Z\"\ngeneration: 2\nlabels:\napp.kubernetes.io/instance: trivy-operator\napp.kubernetes.io/managed-by: kubectl\napp.kubernetes.io/name: trivy-operator\napp.kubernetes.io/version: 0.8.0\nname: nsa\nresourceVersion: \"50896\"\nuid: d9991808-fb2f-4756-842f-8e9205e85b71\nspec:\ncompliance:\ncontrols:\n- checks:\n- id: AVD-KSV-0012\ndescription: Check that container is not running as root\nid: \"1.0\"\nname: Non-root containers\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0014\ndescription: Check that container root file system is immutable\nid: \"1.1\"\nname: Immutable container file systems\nseverity: LOW\n- checks:\n- id: AVD-KSV-0017\ndescription: Controls whether Pods can run privileged containers\nid: \"1.2\"\nname: Preventing privileged containers\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0008\ndescription: Controls whether containers can share process namespaces\nid: \"1.3\"\nname: Share containers process namespaces\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0009\ndescription: Controls whether share host process namespaces\nid: \"1.4\"\nname: Share host process namespaces\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0010\ndescription: Controls whether containers can use the host network\nid: \"1.5\"\nname: Use the host network\nseverity: HIGH\n- checks:\n- id: AVD-KSV-0029\ndescription: Controls whether container applications can run with root privileges\nor with root group membership\nid: \"1.6\"\nname: Run with root privileges or with root group membership\nseverity: LOW\n- checks:\n- id: AVD-KSV-0001\ndescription: Control check restrictions escalation to root privileges\nid: \"1.7\"\nname: Restricts escalation to root privileges\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0002\ndescription: Control checks if pod sets the SELinux context of the container\nid: \"1.8\"\nname: Sets the SELinux context of the container\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0030\ndescription: Control checks the restriction of containers access to resources\nwith AppArmor\nid: \"1.9\"\nname: Restrict a container's access to resources with AppArmor\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0030\ndescription: Control checks the sets the seccomp profile used to sandbox containers\nid: \"1.10\"\nname: Sets the seccomp profile used to sandbox containers.\nseverity: LOW\n- checks:\n- id: AVD-KSV-0036\ndescription: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\nfalse'\nid: \"1.11\"\nname: Protecting Pod service account tokens\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0037\ndefaultStatus: FAIL\ndescription: Control check whether Namespace kube-system is not be used by users\nid: \"1.12\"\nname: Namespace kube-system should not be used by users\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0038\ndefaultStatus: FAIL\ndescription: Control check validate the pod and/or namespace Selectors usage\nid: \"2.0\"\nname: Pod and/or namespace Selectors usage\nseverity: MEDIUM\n- defaultStatus: FAIL\ndescription: Control check whether check cni plugin installed\nid: \"3.0\"\nname: Use CNI plugin that supports NetworkPolicy API (Manual)\nseverity: CRITICAL\n- checks:\n- id: AVD-KSV-0040\ndefaultStatus: FAIL\ndescription: Control check the use of ResourceQuota policy to limit aggregate\nresource usage within namespace\nid: \"4.0\"\nname: Use ResourceQuota policies to limit resources\nseverity: MEDIUM\n- checks:\n- id: AVD-KSV-0039\ndefaultStatus: FAIL\ndescription: Control check the use of LimitRange policy limit resource usage\nfor namespaces or nodes\nid: \"4.1\"\nname: Use LimitRange policies to limit resources\nseverity: MEDIUM\n- defaultStatus: FAIL\ndescription: Control check whether control plan disable insecure port\nid: \"5.0\"\nname: Control plan disable insecure port (Manual)\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0030\ndescription: Control check whether etcd communication is encrypted\nid: \"5.1\"\nname: Encrypt etcd communication\nseverity: CRITICAL\n- defaultStatus: FAIL\ndescription: Control check whether kube config file permissions\nid: \"6.0\"\nname: Ensure kube config file permission (Manual)\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0029\ndescription: Control checks whether encryption resource has been set\nid: \"6.1\"\nname: Check that encryption resource has been set\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0004\ndescription: Control checks whether encryption provider has been set\nid: \"6.2\"\nname: Check encryption provider\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0001\ndescription: Control checks whether anonymous-auth is unset\nid: \"7.0\"\nname: Make sure anonymous-auth is unset\nseverity: CRITICAL\n- checks:\n- id: AVD-KCV-0008\ndescription: Control check whether RBAC permission is in use\nid: \"7.1\"\nname: Make sure -authorization-mode=RBAC\nseverity: CRITICAL\n- defaultStatus: FAIL\ndescription: Control check whether audit policy is configure\nid: \"8.0\"\nname: Audit policy is configure (Manual)\nseverity: HIGH\n- checks:\n- id: AVD-KCV-0019\ndescription: Control check whether audit log path is configure\nid: \"8.1\"\nname: Audit log path is configure\nseverity: MEDIUM\n- checks:\n- id: AVD-KCV-0020\ndescription: Control check whether audit log aging is configure\nid: \"8.2\"\nname: Audit log aging\nseverity: MEDIUM\ndescription: National Security Agency - Kubernetes Hardening Guidance\nid: \"0001\"\nrelatedResources:\n- https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/\ntitle: nsa\nversion: \"1.0\"\ncron: '* * * * *'\nreportType: all\nstatus:\ndetailReport:\ndescription: National Security Agency - Kubernetes Hardening Guidance\nid: \"0001\"\nrelatedVersion:\n- https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/\nresults:\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-etcd-kind-control-plane\ntitle: Runs as root user\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/replicaset-coredns-558bd4d5db\ntitle: Runs as root user\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/daemonset-kindnet\ntitle: Runs as root user\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Runs as root user\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: Runs as root user\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-kube-scheduler-kind-control-plane\ntitle: Runs as root user\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: local-path-storage/replicaset-local-path-provisioner-547f784dff\ntitle: Runs as root user\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0012\ndescription: '''runAsNonRoot'' forces the running image to run as a non-root\nuser to ensure least privileges.'\nmessages:\n- '''runAsNonRoot'' forces the running image to run as a non-root user to\nensure least privileges.'\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-kube-controller-manager-kind-control-plane\ntitle: Runs as root user\ndescription: Check that container is not running as root\nid: \"1.0\"\nname: Non-root containers\nseverity: MEDIUM\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0014\ndescription: An immutable root file system prevents applications from writing\nto their local disk. This can limit intrusions, as attackers will not be\nable to tamper with the file system or write foreign executables to disk.\nmessages:\n- An immutable root file system prevents applications from writing to their\nlocal disk. This can limit intrusions, as attackers will not be able to\ntamper with the file system or write foreign executables to disk.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-etcd-kind-control-plane\ntitle: Root file system is not read-only\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0014\ndescription: An immutable root file system prevents applications from writing\nto their local disk. This can limit intrusions, as attackers will not be\nable to tamper with the file system or write foreign executables to disk.\nmessages:\n- An immutable root file system prevents applications from writing to their\nlocal disk. This can limit intrusions, as attackers will not be able to\ntamper with the file system or write foreign executables to disk.\nseverity: LOW\nsuccess: false\ntarget: kube-system/daemonset-kindnet\ntitle: Root file system is not read-only\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0014\ndescription: An immutable root file system prevents applications from writing\nto their local disk. This can limit intrusions, as attackers will not be\nable to tamper with the file system or write foreign executables to disk.\nmessages:\n- An immutable root file system prevents applications from writing to their\nlocal disk. This can limit intrusions, as attackers will not be able to\ntamper with the file system or write foreign executables to disk.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Root file system is not read-only\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0014\ndescription: An immutable root file system prevents applications from writing\nto their local disk. This can limit intrusions, as attackers will not be\nable to tamper with the file system or write foreign executables to disk.\nmessages:\n- An immutable root file system prevents applications from writing to their\nlocal disk. This can limit intrusions, as attackers will not be able to\ntamper with the file system or write foreign executables to disk.\nseverity: LOW\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: Root file system is not read-only\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0014\ndescription: An immutable root file system prevents applications from writing\nto their local disk. This can limit intrusions, as attackers will not be\nable to tamper with the file system or write foreign executables to disk.\nmessages:\n- An immutable root file system prevents applications from writing to their\nlocal disk. This can limit intrusions, as attackers will not be able to\ntamper with the file system or write foreign executables to disk.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-scheduler-kind-control-plane\ntitle: Root file system is not read-only\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0014\ndescription: An immutable root file system prevents applications from writing\nto their local disk. This can limit intrusions, as attackers will not be\nable to tamper with the file system or write foreign executables to disk.\nmessages:\n- An immutable root file system prevents applications from writing to their\nlocal disk. This can limit intrusions, as attackers will not be able to\ntamper with the file system or write foreign executables to disk.\nseverity: LOW\nsuccess: false\ntarget: local-path-storage/replicaset-local-path-provisioner-547f784dff\ntitle: Root file system is not read-only\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0014\ndescription: An immutable root file system prevents applications from writing\nto their local disk. This can limit intrusions, as attackers will not be\nable to tamper with the file system or write foreign executables to disk.\nmessages:\n- An immutable root file system prevents applications from writing to their\nlocal disk. This can limit intrusions, as attackers will not be able to\ntamper with the file system or write foreign executables to disk.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-controller-manager-kind-control-plane\ntitle: Root file system is not read-only\ndescription: Check that container root file system is immutable\nid: \"1.1\"\nname: Immutable container file systems\nseverity: LOW\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0017\ndescription: Privileged containers share namespaces with the host system and\ndo not offer any security. They should be used exclusively for system containers\nthat require high privileges.\nmessages:\n- Privileged containers share namespaces with the host system and do not offer\nany security. They should be used exclusively for system containers that\nrequire high privileges.\nseverity: HIGH\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: Privileged container\ndescription: Controls whether Pods can run privileged containers\nid: \"1.2\"\nname: Preventing privileged containers\nseverity: HIGH\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Controls whether containers can share process namespaces\nid: \"1.3\"\nname: Share containers process namespaces\nseverity: HIGH\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0009\ndescription: Sharing the host\u2019s network namespace permits processes in the\npod to communicate with processes bound to the host\u2019s loopback adapter.\nmessages:\n- Sharing the host\u2019s network namespace permits processes in the pod to communicate\nwith processes bound to the host\u2019s loopback adapter.\nseverity: HIGH\nsuccess: false\ntarget: kube-system/pod-etcd-kind-control-plane\ntitle: Access to host network\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0009\ndescription: Sharing the host\u2019s network namespace permits processes in the\npod to communicate with processes bound to the host\u2019s loopback adapter.\nmessages:\n- Sharing the host\u2019s network namespace permits processes in the pod to communicate\nwith processes bound to the host\u2019s loopback adapter.\nseverity: HIGH\nsuccess: false\ntarget: kube-system/daemonset-kindnet\ntitle: Access to host network\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0009\ndescription: Sharing the host\u2019s network namespace permits processes in the\npod to communicate with processes bound to the host\u2019s loopback adapter.\nmessages:\n- Sharing the host\u2019s network namespace permits processes in the pod to communicate\nwith processes bound to the host\u2019s loopback adapter.\nseverity: HIGH\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Access to host network\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0009\ndescription: Sharing the host\u2019s network namespace permits processes in the\npod to communicate with processes bound to the host\u2019s loopback adapter.\nmessages:\n- Sharing the host\u2019s network namespace permits processes in the pod to communicate\nwith processes bound to the host\u2019s loopback adapter.\nseverity: HIGH\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: Access to host network\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0009\ndescription: Sharing the host\u2019s network namespace permits processes in the\npod to communicate with processes bound to the host\u2019s loopback adapter.\nmessages:\n- Sharing the host\u2019s network namespace permits processes in the pod to communicate\nwith processes bound to the host\u2019s loopback adapter.\nseverity: HIGH\nsuccess: false\ntarget: kube-system/pod-kube-scheduler-kind-control-plane\ntitle: Access to host network\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0009\ndescription: Sharing the host\u2019s network namespace permits processes in the\npod to communicate with processes bound to the host\u2019s loopback adapter.\nmessages:\n- Sharing the host\u2019s network namespace permits processes in the pod to communicate\nwith processes bound to the host\u2019s loopback adapter.\nseverity: HIGH\nsuccess: false\ntarget: kube-system/pod-kube-controller-manager-kind-control-plane\ntitle: Access to host network\ndescription: Controls whether share host process namespaces\nid: \"1.4\"\nname: Share host process namespaces\nseverity: HIGH\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Controls whether containers can use the host network\nid: \"1.5\"\nname: Use the host network\nseverity: HIGH\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Controls whether container applications can run with root privileges\nor with root group membership\nid: \"1.6\"\nname: Run with root privileges or with root group membership\nseverity: LOW\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0001\ndescription: A program inside the container can elevate its own privileges\nand run as root, which might give the program control over the container\nand node.\nmessages:\n- A program inside the container can elevate its own privileges and run as\nroot, which might give the program control over the container and node.\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-etcd-kind-control-plane\ntitle: Process can elevate its own privileges\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0001\ndescription: A program inside the container can elevate its own privileges\nand run as root, which might give the program control over the container\nand node.\nmessages:\n- A program inside the container can elevate its own privileges and run as\nroot, which might give the program control over the container and node.\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/daemonset-kindnet\ntitle: Process can elevate its own privileges\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0001\ndescription: A program inside the container can elevate its own privileges\nand run as root, which might give the program control over the container\nand node.\nmessages:\n- A program inside the container can elevate its own privileges and run as\nroot, which might give the program control over the container and node.\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Process can elevate its own privileges\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0001\ndescription: A program inside the container can elevate its own privileges\nand run as root, which might give the program control over the container\nand node.\nmessages:\n- A program inside the container can elevate its own privileges and run as\nroot, which might give the program control over the container and node.\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: Process can elevate its own privileges\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0001\ndescription: A program inside the container can elevate its own privileges\nand run as root, which might give the program control over the container\nand node.\nmessages:\n- A program inside the container can elevate its own privileges and run as\nroot, which might give the program control over the container and node.\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-kube-scheduler-kind-control-plane\ntitle: Process can elevate its own privileges\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0001\ndescription: A program inside the container can elevate its own privileges\nand run as root, which might give the program control over the container\nand node.\nmessages:\n- A program inside the container can elevate its own privileges and run as\nroot, which might give the program control over the container and node.\nseverity: MEDIUM\nsuccess: false\ntarget: local-path-storage/replicaset-local-path-provisioner-547f784dff\ntitle: Process can elevate its own privileges\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0001\ndescription: A program inside the container can elevate its own privileges\nand run as root, which might give the program control over the container\nand node.\nmessages:\n- A program inside the container can elevate its own privileges and run as\nroot, which might give the program control over the container and node.\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-kube-controller-manager-kind-control-plane\ntitle: Process can elevate its own privileges\ndescription: Control check restrictions escalation to root privileges\nid: \"1.7\"\nname: Restricts escalation to root privileges\nseverity: MEDIUM\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control checks if pod sets the SELinux context of the container\nid: \"1.8\"\nname: Sets the SELinux context of the container\nseverity: MEDIUM\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-etcd-kind-control-plane\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/replicaset-coredns-558bd4d5db\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/daemonset-kindnet\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-scheduler-kind-control-plane\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: local-path-storage/replicaset-local-path-provisioner-547f784dff\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-controller-manager-kind-control-plane\ntitle: Default Seccomp profile not set\ndescription: Control checks the restriction of containers access to resources\nwith AppArmor\nid: \"1.9\"\nname: Restrict a container's access to resources with AppArmor\nseverity: MEDIUM\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-etcd-kind-control-plane\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/replicaset-coredns-558bd4d5db\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/daemonset-kindnet\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-scheduler-kind-control-plane\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: local-path-storage/replicaset-local-path-provisioner-547f784dff\ntitle: Default Seccomp profile not set\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0030\ndescription: The RuntimeDefault/Localhost seccomp profile must be required,\nor allow specific additional profiles.\nmessages:\n- The RuntimeDefault/Localhost seccomp profile must be required, or allow\nspecific additional profiles.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-controller-manager-kind-control-plane\ntitle: Default Seccomp profile not set\ndescription: Control checks the sets the seccomp profile used to sandbox containers\nid: \"1.10\"\nname: Sets the seccomp profile used to sandbox containers.\nseverity: LOW\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\nfalse'\nid: \"1.11\"\nname: Protecting Pod service account tokens\nseverity: MEDIUM\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0037\ndescription: ensure that User pods are not placed in kube-system namespace\nmessages:\n- ensure that User pods are not placed in kube-system namespace\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/replicaset-coredns-558bd4d5db\ntitle: User Pods should not be placed in kube-system namespace\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0037\ndescription: ensure that User pods are not placed in kube-system namespace\nmessages:\n- ensure that User pods are not placed in kube-system namespace\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/service-kube-dns\ntitle: User Pods should not be placed in kube-system namespace\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0037\ndescription: ensure that User pods are not placed in kube-system namespace\nmessages:\n- ensure that User pods are not placed in kube-system namespace\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/daemonset-kindnet\ntitle: User Pods should not be placed in kube-system namespace\n- category: Kubernetes Security Check\ncheckID: AVD-KSV-0037\ndescription: ensure that User pods are not placed in kube-system namespace\nmessages:\n- ensure that User pods are not placed in kube-system namespace\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/daemonset-kube-proxy\ntitle: User Pods should not be placed in kube-system namespace\ndescription: Control check whether Namespace kube-system is not be used by users\nid: \"1.12\"\nname: Namespace kube-system should not be used by users\nseverity: MEDIUM\nstatus: FAIL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check validate the pod and/or namespace Selectors usage\nid: \"2.0\"\nname: Pod and/or namespace Selectors usage\nseverity: MEDIUM\nstatus: FAIL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check whether check cni plugin installed\nid: \"3.0\"\nname: Use CNI plugin that supports NetworkPolicy API (Manual)\nseverity: CRITICAL\nstatus: FAIL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check the use of ResourceQuota policy to limit aggregate\nresource usage within namespace\nid: \"4.0\"\nname: Use ResourceQuota policies to limit resources\nseverity: MEDIUM\nstatus: FAIL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check the use of LimitRange policy limit resource usage\nfor namespaces or nodes\nid: \"4.1\"\nname: Use LimitRange policies to limit resources\nseverity: MEDIUM\nstatus: FAIL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check whether control plan disable insecure port\nid: \"5.0\"\nname: Control plan disable insecure port (Manual)\nseverity: CRITICAL\nstatus: FAIL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check whether etcd communication is encrypted\nid: \"5.1\"\nname: Encrypt etcd communication\nseverity: CRITICAL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check whether kube config file permissions\nid: \"6.0\"\nname: Ensure kube config file permission (Manual)\nseverity: CRITICAL\nstatus: FAIL\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KCV-0029\ndescription: etcd should be configured to make use of TLS encryption for client\nconnections.\nmessages:\n- etcd should be configured to make use of TLS encryption for client connections.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Ensure that the --etcd-cafile argument is set as appropriate\ndescription: Control checks whether encryption resource has been set\nid: \"6.1\"\nname: Check that encryption resource has been set\nseverity: CRITICAL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control checks whether encryption provider has been set\nid: \"6.2\"\nname: Check encryption provider\nseverity: CRITICAL\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KCV-0001\ndescription: Disable anonymous requests to the API server.\nmessages:\n- Disable anonymous requests to the API server.\nseverity: MEDIUM\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Ensure that the --anonymous-auth argument is set to false\ndescription: Control checks whether anonymous-auth is unset\nid: \"7.0\"\nname: Make sure anonymous-auth is unset\nseverity: CRITICAL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check whether RBAC permission is in use\nid: \"7.1\"\nname: Make sure -authorization-mode=RBAC\nseverity: CRITICAL\n- checks:\n- checkID: \"\"\nseverity: \"\"\nsuccess: true\ndescription: Control check whether audit policy is configure\nid: \"8.0\"\nname: Audit policy is configure (Manual)\nseverity: HIGH\nstatus: FAIL\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KCV-0019\ndescription: Enable auditing on the Kubernetes API Server and set the desired\naudit log path.\nmessages:\n- Enable auditing on the Kubernetes API Server and set the desired audit log\npath.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Ensure that the --audit-log-path argument is set\ndescription: Control check whether audit log path is configure\nid: \"8.1\"\nname: Audit log path is configure\nseverity: MEDIUM\n- checks:\n- category: Kubernetes Security Check\ncheckID: AVD-KCV-0020\ndescription: Retain the logs for at least 30 days or as appropriate.\nmessages:\n- Retain the logs for at least 30 days or as appropriate.\nseverity: LOW\nsuccess: false\ntarget: kube-system/pod-kube-apiserver-kind-control-plane\ntitle: Ensure that the --audit-log-maxage argument is set to 30 or as appropriate\ndescription: Control check whether audit log aging is configure\nid: \"8.2\"\nname: Audit log aging\nseverity: MEDIUM\ntitle: nsa\nversion: \"1.0\"\ntotalCounts:\nfailCount: 12\npassCount: 15\nupdateTimestamp: \"2022-12-05T08:43:10Z\"\n</code></pre> Kubernetes CIS Benchmark 1.23 <pre><code>{\n\"apiVersion\": \"aquasecurity.github.io/v1alpha1\",\n\"kind\": \"ClusterComplianceReport\",\n\"metadata\": {\n\"creationTimestamp\": \"2023-01-01T10:27:01Z\",\n\"generation\": 1,\n\"labels\": {\n\"app.kubernetes.io/instance\": \"trivy-operator\",\n\"app.kubernetes.io/managed-by\": \"kubectl\",\n\"app.kubernetes.io/name\": \"trivy-operator\",\n\"app.kubernetes.io/version\": \"0.16.1\"\n},\n\"name\": \"cis\",\n\"resourceVersion\": \"8985\",\n\"uid\": \"698f0de6-16dd-410c-b102-9cb068bc5c0d\"\n},\n\"spec\": {\n\"compliance\": {\n\"controls\": [\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0001\"\n}\n],\n\"description\": \"Disable anonymous requests to the API server\",\n\"id\": \"1.2.1\",\n\"name\": \"Ensure that the --anonymous-auth argument is set to false\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0002\"\n}\n],\n\"description\": \"Do not use token based authentication.\",\n\"id\": \"1.2.2\",\n\"name\": \"Ensure that the --token-auth-file parameter is not set\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0003\"\n}\n],\n\"description\": \"This admission controller rejects all net-new usage of the Service field externalIPs.\",\n\"id\": \"1.2.3\",\n\"name\": \"Ensure that the --DenyServiceExternalIPs is not set\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0004\"\n}\n],\n\"description\": \"Use https for kubelet connections.\",\n\"id\": \"1.2.4\",\n\"name\": \"Ensure that the --kubelet-https argument is set to true\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0005\"\n}\n],\n\"description\": \"Enable certificate based kubelet authentication.\",\n\"id\": \"1.2.5\",\n\"name\": \"Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0006\"\n}\n],\n\"description\": \"Verify kubelets certificate before establishing connection.\",\n\"id\": \"1.2.6\",\n\"name\": \"Ensure that the --kubelet-certificate-authority argument is set as appropriate\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0007\"\n}\n],\n\"description\": \"Do not always authorize all requests.\",\n\"id\": \"1.2.7\",\n\"name\": \"Ensure that the --authorization-mode argument is not set to AlwaysAllow\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0008\"\n}\n],\n\"description\": \"Restrict kubelet nodes to reading only objects associated with them.\",\n\"id\": \"1.2.8\",\n\"name\": \"Ensure that the --authorization-mode argument includes Node\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0009\"\n}\n],\n\"description\": \"Turn on Role Based Access Control.\",\n\"id\": \"1.2.9\",\n\"name\": \"Ensure that the --authorization-mode argument includes RBAC\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0010\"\n}\n],\n\"description\": \"Limit the rate at which the API server accepts requests.\",\n\"id\": \"1.2.10\",\n\"name\": \"Ensure that the admission control plugin EventRateLimit is set\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0011\"\n}\n],\n\"description\": \"Do not allow all requests\",\n\"id\": \"1.2.11\",\n\"name\": \"Ensure that the admission control plugin AlwaysAdmit is not set\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0012\"\n}\n],\n\"description\": \"Always pull images\",\n\"id\": \"1.2.12\",\n\"name\": \"Ensure that the admission control plugin AlwaysPullImages is set\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0013\"\n}\n],\n\"description\": \"The SecurityContextDeny admission controller can be used to deny pods which make use of some SecurityContext fields which could allow for privilege escalation in the cluster. This should be used where PodSecurityPolicy is not in place within the cluster.\",\n\"id\": \"1.2.13\",\n\"name\": \"Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0014\"\n}\n],\n\"description\": \"Automate service accounts management.\",\n\"id\": \"1.2.14\",\n\"name\": \"Ensure that the admission control plugin ServiceAccount is set\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0015\"\n}\n],\n\"description\": \"Reject creating objects in a namespace that is undergoing termination.\",\n\"id\": \"1.2.15\",\n\"name\": \"Ensure that the admission control plugin NamespaceLifecycle is set\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0016\"\n}\n],\n\"description\": \"Limit the Node and Pod objects that a kubelet could modify.\",\n\"id\": \"1.2.16\",\n\"name\": \"Ensure that the admission control plugin NodeRestriction is set\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0017\"\n}\n],\n\"description\": \"Do not disable the secure port\",\n\"id\": \"1.2.17\",\n\"name\": \"Ensure that the --secure-port argument is not set to 0\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0018\"\n}\n],\n\"description\": \"Disable profiling, if not needed.\",\n\"id\": \"1.2.18\",\n\"name\": \"Ensure that the --profiling argument is set to false\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0019\"\n}\n],\n\"description\": \"Enable auditing on the Kubernetes API Server and set the desired audit log path.\",\n\"id\": \"1.2.19\",\n\"name\": \"Ensure that the --audit-log-path argument is set\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0020\"\n}\n],\n\"description\": \"Retain the logs for at least 30 days or as appropriate.\",\n\"id\": \"1.2.20\",\n\"name\": \"Ensure that the --audit-log-maxage argument is set to 30 or as appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0021\"\n}\n],\n\"description\": \"Retain 10 or an appropriate number of old log file.\",\n\"id\": \"1.2.21\",\n\"name\": \"Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0022\"\n}\n],\n\"description\": \"Rotate log files on reaching 100 MB or as appropriate.\",\n\"id\": \"1.2.22\",\n\"name\": \"Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0024\"\n}\n],\n\"description\": \"Validate service account before validating token.\",\n\"id\": \"1.2.24\",\n\"name\": \"Ensure that the --service-account-lookup argument is set to true\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0025\"\n}\n],\n\"description\": \"Explicitly set a service account public key file for service accounts on the apiserver.\",\n\"id\": \"1.2.25\",\n\"name\": \"Ensure that the --service-account-key-file argument is set as appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0026\"\n}\n],\n\"description\": \"etcd should be configured to make use of TLS encryption for client connections.\",\n\"id\": \"1.2.26\",\n\"name\": \"Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0027\"\n}\n],\n\"description\": \"Setup TLS connection on the API server.\",\n\"id\": \"1.2.27\",\n\"name\": \"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0028\"\n}\n],\n\"description\": \"Setup TLS connection on the API server.\",\n\"id\": \"1.2.28\",\n\"name\": \"Ensure that the --client-ca-file argument is set appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0029\"\n}\n],\n\"description\": \"etcd should be configured to make use of TLS encryption for client connections.\",\n\"id\": \"1.2.29\",\n\"name\": \"Ensure that the --etcd-cafile argument is set as appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0030\"\n}\n],\n\"description\": \"Encrypt etcd key-value store.\",\n\"id\": \"1.2.30\",\n\"name\": \"Ensure that the --encryption-provider-config argument is set as appropriate\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0033\"\n}\n],\n\"description\": \"Activate garbage collector on pod termination, as appropriate.\",\n\"id\": \"1.3.1\",\n\"name\": \"Ensure that the --terminated-pod-gc-threshold argument is set as appropriate\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0035\"\n}\n],\n\"description\": \"Use individual service account credentials for each controller.\",\n\"id\": \"1.3.3\",\n\"name\": \"Ensure that the --use-service-account-credentials argument is set to true\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0036\"\n}\n],\n\"description\": \"Explicitly set a service account private key file for service accounts on the controller manager.\",\n\"id\": \"1.3.4\",\n\"name\": \"Ensure that the --service-account-private-key-file argument is set as appropriate\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0037\"\n}\n],\n\"description\": \"Allow pods to verify the API servers serving certificate before establishing connections.\",\n\"id\": \"1.3.5\",\n\"name\": \"Ensure that the --root-ca-file argument is set as appropriate\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0038\"\n}\n],\n\"description\": \"Enable kubelet server certificate rotation on controller-manager.\",\n\"id\": \"1.3.6\",\n\"name\": \"Ensure that the RotateKubeletServerCertificate argument is set to true\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0039\"\n}\n],\n\"description\": \"Do not bind the scheduler service to non-loopback insecure addresses.\",\n\"id\": \"1.3.7\",\n\"name\": \"Ensure that the --bind-address argument is set to 127.0.0.1\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0034\"\n}\n],\n\"description\": \"Disable profiling, if not needed.\",\n\"id\": \"1.4.1\",\n\"name\": \"Ensure that the --profiling argument is set to false\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0041\"\n}\n],\n\"description\": \"Do not bind the scheduler service to non-loopback insecure addresses.\",\n\"id\": \"1.4.2\",\n\"name\": \"Ensure that the --bind-address argument is set to 127.0.0.1\",\n\"severity\": \"CRITICAL\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0042\"\n}\n],\n\"description\": \"Configure TLS encryption for the etcd service.\",\n\"id\": \"2.1\",\n\"name\": \"Ensure that the --cert-file and --key-file arguments are set as appropriate\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0043\"\n}\n],\n\"description\": \"Enable client authentication on etcd service.\",\n\"id\": \"2.2\",\n\"name\": \"Ensure that the --client-cert-auth argument is set to true\",\n\"severity\": \"CRITICAL\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0044\"\n}\n],\n\"description\": \"Do not use self-signed certificates for TLS.\",\n\"id\": \"2.3\",\n\"name\": \"Ensure that the --auto-tls argument is not set to true\",\n\"severity\": \"CRITICAL\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0045\"\n}\n],\n\"description\": \"etcd should be configured to make use of TLS encryption for peer connections.\",\n\"id\": \"2.4\",\n\"name\": \"Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate\",\n\"severity\": \"CRITICAL\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0046\"\n}\n],\n\"description\": \"etcd should be configured for peer authentication.\",\n\"id\": \"2.5\",\n\"name\": \"Ensure that the --peer-client-cert-auth argument is set to true\",\n\"severity\": \"CRITICAL\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KCV-0047\"\n}\n],\n\"description\": \"Do not use self-signed certificates for TLS.\",\n\"id\": \"2.6\",\n\"name\": \"Ensure that the --peer-auto-tls argument is not set to true\",\n\"severity\": \"HIGH\"\n},\n{\n\"description\": \"Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose.\",\n\"id\": \"3.1.1\",\n\"name\": \"Client certificate authentication should not be used for users (Manual)\",\n\"severity\": \"HIGH\"\n},\n{\n\"description\": \"Kubernetes can audit the details of requests made to the API server. The --audit- policy-file flag must be set for this logging to be enabled.\",\n\"id\": \"3.2.1\",\n\"name\": \"Ensure that a minimal audit policy is created (Manual)\",\n\"severity\": \"HIGH\"\n},\n{\n\"description\": \"Ensure that the audit policy created for the cluster covers key security concerns.\",\n\"id\": \"3.2.2\",\n\"name\": \"Ensure that the audit policy covers key security concerns (Manual)\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0111\"\n}\n],\n\"description\": \"The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.\",\n\"id\": \"5.1.1\",\n\"name\": \"Ensure that the cluster-admin role is only used where required\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0041\"\n}\n],\n\"description\": \"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster\",\n\"id\": \"5.1.2\",\n\"name\": \"Minimize access to secrets\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0044\"\n},\n{\n\"id\": \"AVD-KSV-0045\"\n},\n{\n\"id\": \"AVD-KSV-0046\"\n}\n],\n\"description\": \"Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard \\\"*\\\" which matches all items\",\n\"id\": \"5.1.3\",\n\"name\": \"Minimize wildcard use in Roles and ClusterRoles\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0036\"\n}\n],\n\"description\": \"Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server\",\n\"id\": \"5.1.6\",\n\"name\": \"Ensure that Service Account Tokens are only mounted where necessary\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0043\"\n}\n],\n\"description\": \"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required\",\n\"id\": \"5.1.8\",\n\"name\": \"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0017\"\n}\n],\n\"description\": \"Do not generally permit containers to be run with the securityContext.privileged flag set to true.\",\n\"id\": \"5.2.2\",\n\"name\": \"Minimize the admission of privileged containers\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0010\"\n}\n],\n\"description\": \"Do not generally permit containers to be run with the hostPID flag set to true.\",\n\"id\": \"5.2.3\",\n\"name\": \"Minimize the admission of containers wishing to share the host process ID namespace\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0008\"\n}\n],\n\"description\": \"Do not generally permit containers to be run with the hostIPC flag set to true.\",\n\"id\": \"5.2.4\",\n\"name\": \"Minimize the admission of containers wishing to share the host IPC namespace\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0009\"\n}\n],\n\"description\": \"Do not generally permit containers to be run with the hostNetwork flag set to true.\",\n\"id\": \"5.2.5\",\n\"name\": \"Minimize the admission of containers wishing to share the host network namespace\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0001\"\n}\n],\n\"description\": \"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true\",\n\"id\": \"5.2.6\",\n\"name\": \"Minimize the admission of containers with allowPrivilegeEscalation\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0012\"\n}\n],\n\"description\": \"Do not generally permit containers to be run as the root user.\",\n\"id\": \"5.2.7\",\n\"name\": \"Minimize the admission of root containers\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0022\"\n}\n],\n\"description\": \"Do not generally permit containers with the potentially dangerous NET_RAW capability.\",\n\"id\": \"5.2.8\",\n\"name\": \"Minimize the admission of containers with the NET_RAW capability\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0004\"\n}\n],\n\"description\": \"Do not generally permit containers with capabilities assigned beyond the default set.\",\n\"id\": \"5.2.9\",\n\"name\": \"Minimize the admission of containers with added capabilities\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0003\"\n}\n],\n\"description\": \"Do not generally permit containers with capabilities\",\n\"id\": \"5.2.10\",\n\"name\": \"Minimize the admission of containers with capabilities assigned\",\n\"severity\": \"LOW\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0103\"\n}\n],\n\"description\": \"Do not generally permit containers with capabilities\",\n\"id\": \"5.2.11\",\n\"name\": \"Minimize the admission of containers with capabilities assigned\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0023\"\n}\n],\n\"description\": \"Do not generally admit containers which make use of hostPath volumes.\",\n\"id\": \"5.2.12\",\n\"name\": \"Minimize the admission of HostPath volumes\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0024\"\n}\n],\n\"description\": \"Do not generally permit containers which require the use of HostPorts.\",\n\"id\": \"5.2.13\",\n\"name\": \"Minimize the admission of containers which use HostPorts\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"description\": \"There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.\",\n\"id\": \"5.3.1\",\n\"name\": \"Ensure that the CNI in use supports Network Policies (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0038\"\n}\n],\n\"description\": \"Use network policies to isolate traffic in your cluster network.\",\n\"id\": \"5.3.2\",\n\"name\": \"Ensure that all Namespaces have Network Policies defined\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"description\": \"Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.\",\n\"id\": \"5.4.1\",\n\"name\": \"Prefer using secrets as files over secrets as environment variables (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"description\": \"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs.\",\n\"id\": \"5.4.2\",\n\"name\": \"Consider external secret storage (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"description\": \"Configure Image Provenance for your deployment.\",\n\"id\": \"5.5.1\",\n\"name\": \"Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"description\": \"Use namespaces to isolate your Kubernetes objects.\",\n\"id\": \"5.7.1\",\n\"name\": \"Create administrative boundaries between resources using namespaces (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0104\"\n}\n],\n\"description\": \"Enable docker/default seccomp profile in your pod definitions.\",\n\"id\": \"5.7.2\",\n\"name\": \"Ensure that the seccomp profile is set to docker/default in your pod definitions\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0021\"\n},\n{\n\"id\": \"AVD-KSV-0020\"\n},\n{\n\"id\": \"AVD-KSV-0005\"\n},\n{\n\"id\": \"AVD-KSV-0025\"\n},\n{\n\"id\": \"AVD-KSV-0104\"\n},\n{\n\"id\": \"AVD-KSV-0030\"\n}\n],\n\"description\": \"Apply Security Context to Your Pods and Containers\",\n\"id\": \"5.7.3\",\n\"name\": \"Apply Security Context to Your Pods and Containers\",\n\"severity\": \"HIGH\"\n},\n{\n\"checks\": [\n{\n\"id\": \"AVD-KSV-0110\"\n}\n],\n\"description\": \"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them\",\n\"id\": \"5.7.4\",\n\"name\": \"The default namespace should not be used\",\n\"severity\": \"MEDIUM\"\n}\n],\n\"description\": \"CIS Kubernetes Benchmarks\",\n\"id\": \"cis\",\n\"relatedResources\": [\n\"https://www.cisecurity.org/benchmark/kubernetes\"\n],\n\"title\": \"CIS Kubernetes Benchmarks v1.23\",\n\"version\": \"1.0\"\n},\n\"cron\": \"** ** *\",\n\"reportType\": \"summary\"\n},\n\"status\": {\n\"summary\": {\n\"failCount\": 24,\n\"passCount\": 48\n},\n\"summaryReport\": {\n\"controlCheck\": [\n{\n\"id\": \"1.2.1\",\n\"name\": \"Ensure that the --anonymous-auth argument is set to false\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.2\",\n\"name\": \"Ensure that the --token-auth-file parameter is not set\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.3\",\n\"name\": \"Ensure that the --DenyServiceExternalIPs is not set\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.4\",\n\"name\": \"Ensure that the --kubelet-https argument is set to true\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.5\",\n\"name\": \"Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.6\",\n\"name\": \"Ensure that the --kubelet-certificate-authority argument is set as appropriate\",\n\"severity\": \"HIGH\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.7\",\n\"name\": \"Ensure that the --authorization-mode argument is not set to AlwaysAllow\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.8\",\n\"name\": \"Ensure that the --authorization-mode argument includes Node\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.9\",\n\"name\": \"Ensure that the --authorization-mode argument includes RBAC\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.10\",\n\"name\": \"Ensure that the admission control plugin EventRateLimit is set\",\n\"severity\": \"HIGH\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.11\",\n\"name\": \"Ensure that the admission control plugin AlwaysAdmit is not set\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.12\",\n\"name\": \"Ensure that the admission control plugin AlwaysPullImages is set\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.13\",\n\"name\": \"Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.14\",\n\"name\": \"Ensure that the admission control plugin ServiceAccount is set\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.15\",\n\"name\": \"Ensure that the admission control plugin NamespaceLifecycle is set\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.16\",\n\"name\": \"Ensure that the admission control plugin NodeRestriction is set\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.17\",\n\"name\": \"Ensure that the --secure-port argument is not set to 0\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.18\",\n\"name\": \"Ensure that the --profiling argument is set to false\",\n\"severity\": \"LOW\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.19\",\n\"name\": \"Ensure that the --audit-log-path argument is set\",\n\"severity\": \"LOW\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.20\",\n\"name\": \"Ensure that the --audit-log-maxage argument is set to 30 or as appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.21\",\n\"name\": \"Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.22\",\n\"name\": \"Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.24\",\n\"name\": \"Ensure that the --service-account-lookup argument is set to true\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.25\",\n\"name\": \"Ensure that the --service-account-key-file argument is set as appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.26\",\n\"name\": \"Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.27\",\n\"name\": \"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.2.28\",\n\"name\": \"Ensure that the --client-ca-file argument is set appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.29\",\n\"name\": \"Ensure that the --etcd-cafile argument is set as appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.2.30\",\n\"name\": \"Ensure that the --encryption-provider-config argument is set as appropriate\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.3.1\",\n\"name\": \"Ensure that the --terminated-pod-gc-threshold argument is set as appropriate\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.3.3\",\n\"name\": \"Ensure that the --use-service-account-credentials argument is set to true\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.3.4\",\n\"name\": \"Ensure that the --service-account-private-key-file argument is set as appropriate\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.3.5\",\n\"name\": \"Ensure that the --root-ca-file argument is set as appropriate\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.3.6\",\n\"name\": \"Ensure that the RotateKubeletServerCertificate argument is set to true\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.3.7\",\n\"name\": \"Ensure that the --bind-address argument is set to 127.0.0.1\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"1.4.1\",\n\"name\": \"Ensure that the --profiling argument is set to false\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 1\n},\n{\n\"id\": \"1.4.2\",\n\"name\": \"Ensure that the --bind-address argument is set to 127.0.0.1\",\n\"severity\": \"CRITICAL\",\n\"totalFail\": 0\n},\n{\n\"id\": \"2.1\",\n\"name\": \"Ensure that the --cert-file and --key-file arguments are set as appropriate\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"2.2\",\n\"name\": \"Ensure that the --client-cert-auth argument is set to true\",\n\"severity\": \"CRITICAL\",\n\"totalFail\": 0\n},\n{\n\"id\": \"2.3\",\n\"name\": \"Ensure that the --auto-tls argument is not set to true\",\n\"severity\": \"CRITICAL\",\n\"totalFail\": 0\n},\n{\n\"id\": \"2.4\",\n\"name\": \"Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate\",\n\"severity\": \"CRITICAL\",\n\"totalFail\": 0\n},\n{\n\"id\": \"2.5\",\n\"name\": \"Ensure that the --peer-client-cert-auth argument is set to true\",\n\"severity\": \"CRITICAL\",\n\"totalFail\": 0\n},\n{\n\"id\": \"2.6\",\n\"name\": \"Ensure that the --peer-auto-tls argument is not set to true\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"3.1.1\",\n\"name\": \"Client certificate authentication should not be used for users (Manual)\",\n\"severity\": \"HIGH\"\n},\n{\n\"id\": \"3.2.1\",\n\"name\": \"Ensure that a minimal audit policy is created (Manual)\",\n\"severity\": \"HIGH\"\n},\n{\n\"id\": \"3.2.2\",\n\"name\": \"Ensure that the audit policy covers key security concerns (Manual)\",\n\"severity\": \"HIGH\"\n},\n{\n\"id\": \"5.1.1\",\n\"name\": \"Ensure that the cluster-admin role is only used where required\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.1.2\",\n\"name\": \"Minimize access to secrets\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.1.3\",\n\"name\": \"Minimize wildcard use in Roles and ClusterRoles\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.1.6\",\n\"name\": \"Ensure that Service Account Tokens are only mounted where necessary\",\n\"severity\": \"HIGH\",\n\"totalFail\": 1\n},\n{\n\"id\": \"5.1.8\",\n\"name\": \"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.2.2\",\n\"name\": \"Minimize the admission of privileged containers\",\n\"severity\": \"HIGH\",\n\"totalFail\": 1\n},\n{\n\"id\": \"5.2.3\",\n\"name\": \"Minimize the admission of containers wishing to share the host process ID namespace\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.2.4\",\n\"name\": \"Minimize the admission of containers wishing to share the host IPC namespace\",\n\"severity\": \"HIGH\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.2.5\",\n\"name\": \"Minimize the admission of containers wishing to share the host network namespace\",\n\"severity\": \"HIGH\",\n\"totalFail\": 6\n},\n{\n\"id\": \"5.2.6\",\n\"name\": \"Minimize the admission of containers with allowPrivilegeEscalation\",\n\"severity\": \"HIGH\",\n\"totalFail\": 9\n},\n{\n\"id\": \"5.2.7\",\n\"name\": \"Minimize the admission of root containers\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 10\n},\n{\n\"id\": \"5.2.8\",\n\"name\": \"Minimize the admission of containers with the NET_RAW capability\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 2\n},\n{\n\"id\": \"5.2.9\",\n\"name\": \"Minimize the admission of containers with added capabilities\",\n\"severity\": \"LOW\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.2.10\",\n\"name\": \"Minimize the admission of containers with capabilities assigned\",\n\"severity\": \"LOW\",\n\"totalFail\": 9\n},\n{\n\"id\": \"5.2.11\",\n\"name\": \"Minimize the admission of containers with capabilities assigned\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.2.12\",\n\"name\": \"Minimize the admission of HostPath volumes\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 6\n},\n{\n\"id\": \"5.2.13\",\n\"name\": \"Minimize the admission of containers which use HostPorts\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.3.1\",\n\"name\": \"Ensure that the CNI in use supports Network Policies (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"id\": \"5.3.2\",\n\"name\": \"Ensure that all Namespaces have Network Policies defined\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.4.1\",\n\"name\": \"Prefer using secrets as files over secrets as environment variables (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"id\": \"5.4.2\",\n\"name\": \"Consider external secret storage (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"id\": \"5.5.1\",\n\"name\": \"Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"id\": \"5.7.1\",\n\"name\": \"Create administrative boundaries between resources using namespaces (Manual)\",\n\"severity\": \"MEDIUM\"\n},\n{\n\"id\": \"5.7.2\",\n\"name\": \"Ensure that the seccomp profile is set to docker/default in your pod definitions\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 0\n},\n{\n\"id\": \"5.7.3\",\n\"name\": \"Apply Security Context to Your Pods and Containers\",\n\"severity\": \"HIGH\",\n\"totalFail\": 30\n},\n{\n\"id\": \"5.7.4\",\n\"name\": \"The default namespace should not be used\",\n\"severity\": \"MEDIUM\",\n\"totalFail\": 3\n}\n],\n\"id\": \"cis\",\n\"title\": \"CIS Kubernetes Benchmarks v1.23\"\n},\n\"updateTimestamp\": \"2023-01-01T10:28:00Z\"\n}\n}\n</code></pre>"},{"location":"docs/crds/clustercompliancedetail-report/","title":"ClusterComplianceDetailReport","text":"<p>The ClusterComplianceDetailReport is a cluster-scoped resource, which represents the latest result of the Cluster Compliance Detail report. The report data provide granular information on control checks failures that occur in <code>ClusterComplianceReport</code> for further investigation.</p> <p>The compliance detail report provides granular information insight on control check failures:</p> <ul> <li>Failing resource kind</li> <li>Name of the failing resource</li> <li>Namespace of the failing resource</li> <li>Failure error message</li> <li>Remediation</li> </ul> <p>The following listing shows a sample ClusterComplianceDetailReport for NSA specification associated with the <code>cluster</code></p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceDetailReport\nmetadata:\ncreationTimestamp: '2022-03-27T07:04:21Z'\ngeneration: 6\nname: nsa-details\nresourceVersion: '15788'\nuid: 9d36889d-086a-4fb3-b660-a3a3ecffe3c6\nreport:\ncontrolCheck:\n- checkResults:\n- details:\n- msg: ReplicaSet 'coredns-96cc4f57d' should not be set with 'kube-system' namespace\nname: replicaset-coredns-96cc4f57d\nnamespace: kube-system\nstatus: FAIL\n- msg: ReplicaSet 'coredns-5789895cd' should not be set with 'kube-system' namespace\nname: replicaset-coredns-5789895cd\nnamespace: kube-system\nstatus: FAIL\n- msg: ReplicaSet 'traefik-56c4b88c4b' should not be set with 'kube-system'\nnamespace\nname: replicaset-traefik-56c4b88c4b\nnamespace: kube-system\nstatus: FAIL\n- msg: ReplicaSet 'metrics-server-ff9dbcb6c' should not be set with 'kube-system'\nnamespace\nname: replicaset-metrics-server-ff9dbcb6c\nnamespace: kube-system\nstatus: FAIL\n- msg: ReplicaSet 'local-path-provisioner-84bb864455' should not be set with\n'kube-system' namespace\nname: replicaset-local-path-provisioner-84bb864455\nnamespace: kube-system\nstatus: FAIL\nid: KSV037\nobjectType: ReplicaSet\n- details:\n- msg: DaemonSet 'svclb-traefik' should not be set with 'kube-system' namespace\nname: daemonset-svclb-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV037\nobjectType: DaemonSet\n- details:\n- msg: Job 'helm-install-traefik-crd' should not be set with 'kube-system' namespace\nname: job-helm-install-traefik-crd\nnamespace: kube-system\nstatus: FAIL\n- msg: Job 'helm-install-traefik' should not be set with 'kube-system' namespace\nname: job-helm-install-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV037\nobjectType: Job\ndescription: Control check whether Namespace kube-system is not be used by users\nid: '1.12'\nname: Namespace kube-system should not be used by users\nseverity: MEDIUM\n- checkResults:\n- details:\n- msg: Resource do not exist in cluster\nstatus: FAIL\nobjectType: ResourceQuota\ndescription: Control check the use of ResourceQuota policy to limit aggregate\nresource usage within namespace\nid: '4.0'\nname: Use ResourceQuota policies to limit resources\nseverity: MEDIUM\n- checkResults:\n- details:\n- msg: Container 'traefik' of ReplicaSet 'traefik-56c4b88c4b' should set 'securityContext.allowPrivilegeEscalation'\nto false\nname: replicaset-traefik-56c4b88c4b\nnamespace: kube-system\nstatus: FAIL\n- msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\nshould set 'securityContext.allowPrivilegeEscalation' to false\nname: replicaset-local-path-provisioner-84bb864455\nnamespace: kube-system\nstatus: FAIL\nid: KSV001\nobjectType: ReplicaSet\n- details:\n- msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.allowPrivilegeEscalation'\nto false\nname: daemonset-svclb-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV001\nobjectType: DaemonSet\n- details:\n- msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.allowPrivilegeEscalation'\nto false\nname: job-helm-install-traefik-crd\nnamespace: kube-system\nstatus: FAIL\n- msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.allowPrivilegeEscalation'\nto false\nname: job-helm-install-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV001\nobjectType: Job\n- details:\n- msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.allowPrivilegeEscalation'\nto false\nname: pod-nginx-jr99v\nnamespace: trivy-operator-itest\nstatus: FAIL\nid: KSV001\nobjectType: Pod\ndescription: Control check restrictions escalation to root privileges\nid: '1.7'\nname: Restricts escalation to root privileges\nseverity: MEDIUM\n- checkResults:\n- details:\n- msg: Resource do not exist in cluster\nstatus: FAIL\nobjectType: ResourceQuota\ndescription: Control check the use of LimitRange policy limit resource usage for\nnamespaces or nodes\nid: '4.1'\nname: Use LimitRange policies to limit resources\nseverity: MEDIUM\n- checkResults:\n- details:\n- msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\nshould set 'securityContext.readOnlyRootFilesystem' to true\nname: replicaset-local-path-provisioner-84bb864455\nnamespace: kube-system\nstatus: FAIL\nid: KSV014\nobjectType: ReplicaSet\n- details:\n- msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.readOnlyRootFilesystem'\nto true\nname: daemonset-svclb-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV014\nobjectType: DaemonSet\n- details:\n- msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.readOnlyRootFilesystem'\nto true\nname: job-helm-install-traefik-crd\nnamespace: kube-system\nstatus: FAIL\n- msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.readOnlyRootFilesystem'\nto true\nname: job-helm-install-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV014\nobjectType: Job\n- details:\n- msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.readOnlyRootFilesystem'\nto true\nname: pod-nginx-jr99v\nnamespace: trivy-operator-itest\nstatus: FAIL\nid: KSV014\nobjectType: Pod\ndescription: Check that container root file system is immutable\nid: '1.1'\nname: Immutable container file systems\nseverity: LOW\n- checkResults:\n- details:\n- msg: ReplicaSet 'traefik-56c4b88c4b' should set 'spec.securityContext.runAsGroup',\n'spec.securityContext.supplementalGroups[*]' and 'spec.securityContext.fsGroup'\nto integer greater than 0\nname: replicaset-traefik-56c4b88c4b\nnamespace: kube-system\nstatus: FAIL\nid: KSV029\nobjectType: ReplicaSet\ndescription: Controls whether container applications can run with root privileges\nor with root group membership\nid: '1.6'\nname: Run with root privileges or with root group membership\nseverity: LOW\n- checkResults:\n- details:\n- msg: Container of Pod 'nginx-jr99v' should set 'spec.automountServiceAccountToken'\nto false\nname: pod-nginx-jr99v\nnamespace: trivy-operator-itest\nstatus: FAIL\nid: KSV036\nobjectType: Pod\ndescription: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\nfalse'\nid: '1.11'\nname: Protecting Pod service account tokens\nseverity: MEDIUM\n- checkResults:\n- details:\n- msg: Resource do not exist in cluster\nstatus: FAIL\nobjectType: NetworkPolicy\ndescription: Control check validate the pod and/or namespace Selectors usage\nid: '2.0'\nname: Pod and/or namespace Selectors usage\nseverity: MEDIUM\n- checkResults:\n- details:\n- msg: Container 'trivy-operator' of ReplicaSet 'trivy-operator-7cf866c47b'\nshould set 'securityContext.runAsNonRoot' to true\nname: replicaset-trivy-operator-7cf866c47b\nnamespace: trivy-system\nstatus: FAIL\n- msg: Container 'coredns' of ReplicaSet 'coredns-96cc4f57d' should set 'securityContext.runAsNonRoot'\nto true\nname: replicaset-coredns-96cc4f57d\nnamespace: kube-system\nstatus: FAIL\n- msg: Container 'coredns' of ReplicaSet 'coredns-5789895cd' should set 'securityContext.runAsNonRoot'\nto true\nname: replicaset-coredns-5789895cd\nnamespace: kube-system\nstatus: FAIL\n- msg: Container 'trivy-operator' of ReplicaSet 'trivy-operator-c94dd56d'\nshould set 'securityContext.runAsNonRoot' to true\nname: replicaset-trivy-operator-c94dd56d\nnamespace: trivy-system\nstatus: FAIL\n- msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\nshould set 'securityContext.runAsNonRoot' to true\nname: replicaset-local-path-provisioner-84bb864455\nnamespace: kube-system\nstatus: FAIL\nid: KSV012\nobjectType: ReplicaSet\n- details:\n- msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.runAsNonRoot'\nto true\nname: daemonset-svclb-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV012\nobjectType: DaemonSet\n- details:\n- msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.runAsNonRoot'\nto true\nname: job-helm-install-traefik-crd\nnamespace: kube-system\nstatus: FAIL\n- msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.runAsNonRoot'\nto true\nname: job-helm-install-traefik\nnamespace: kube-system\nstatus: FAIL\nid: KSV012\nobjectType: Job\n- details:\n- msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.runAsNonRoot'\nto true\nname: pod-nginx-jr99v\nnamespace: trivy-operator-itest\nstatus: FAIL\nid: KSV012\nobjectType: Pod\ndescription: Check that container is not running as root\nid: '1.0'\nname: Non-root containers\nseverity: MEDIUM\nsummary:\nfailCount: 33\npassCount: 113\ntype:\ndescription: national security agency - kubernetes hardening guidance\nname: nsa-details\nversion: '1.0'\nupdateTimestamp: '2022-03-27T07:09:00Z'\n</code></pre>"},{"location":"docs/crds/clusterconfigaudit-report/","title":"ClusterConfigAuditReport","text":"<p>ClusterConfigAuditReport is equivalent to ConfigAuditReport for cluster-scoped objects such as ClusterRoles, ClusterRoleBindings, and CustomResourceDefinitions.</p>"},{"location":"docs/crds/clusterrbacassessment-report/","title":"ClusterRbacAssessmentReport","text":"<p>ClusterRbacAssessmentReport is equivalent to RbacAssessmentReport for cluster-scoped objects such as ClusterRoles, ClusterRoleBindings.</p>"},{"location":"docs/crds/configaudit-report/","title":"ConfigAuditReport","text":"<p>An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as [Trivy], against a Kubernetes object's configuration. For example, check that a given container image runs as non-root user or that a container has resource requests and limits set. Checks might relate to Kubernetes workloads and other namespaced Kubernetes objects such as Services, ConfigMaps, Roles, and RoleBindings.</p> <p>Each report is owned by the underlying Kubernetes object and is stored in the same namespace, following the <code>&lt;workload-kind&gt;-&lt;workload-name&gt;</code> naming convention.</p> <p>The following listing shows a sample ConfigAuditReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\nname: replicaset-nginx-6d4cf56db6\nnamespace: default\nlabels:\ntrivy-operator.resource.kind: ReplicaSet\ntrivy-operator.resource.name: nginx-6d4cf56db6\ntrivy-operator.resource.namespace: default\nplugin-config-hash: 7f65d98b75\nresource-spec-hash: 7cb64cb677\nuid: d5cf8847-c96d-4534-beb9-514a34230302\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: false\ncontroller: true\nkind: ReplicaSet\nname: nginx-6d4cf56db6\nuid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\nupdateTimestamp: '2021-05-20T12:38:10Z'\nscanner:\nname: Trivy vendor: Aqua Security\nversion: '0.16.1'\nsummary:\ncriticalCount: 2\nhighCount: 0\nlowCount: 9\nmediumCount: 0\nchecks:\n- category: Security\ncheckID: hostPIDSet\nmessages:\n- Host PID is not configured\nseverity: CRITICAL\nsuccess: true\n- category: Security\ncheckID: hostIPCSet\nmessages:\n- Host IPC is not configured\nseverity: CRITICAL\nsuccess: true\n- category: Security\ncheckID: hostNetworkSet\nmessages:\n- Host network is not configured\nseverity: LOW\nsuccess: true\n- category: Security\ncheckID: notReadOnlyRootFilesystem\nmessages:\n- Filesystem should be read only\nscope:\ntype: Container\nvalue: nginx\nseverity: LOW\nsuccess: false\n- category: Security\ncheckID: privilegeEscalationAllowed\nmessages:\n- Privilege escalation should not be allowed\nscope:\ntype: Container\nvalue: nginx\nseverity: CRITICAL\nsuccess: false\n</code></pre> <p>Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with trivy-operator.</p> <p>Note</p> <p>The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls.</p>"},{"location":"docs/crds/exposedsecret-report/","title":"ExposedSecretReport","text":"<p>An instance of the ExposedSecretReport represents the secrets found in a container image of a given Kubernetes workload. It consists of a list exposed secrets with a summary grouped by severity. For a multi-container workload trivy-operator creates multiple instances of ExposedSecretsReports in the workload's namespace with the owner reference set to that workload. Each report follows the naming convention <code>&lt;workload kind&gt;-&lt;workload name&gt;-&lt;container-name&gt;</code>.</p> <p>The following listing shows a sample ExposedSecretReport associated with the ReplicaSet named <code>app-574ddcb559</code> in the <code>default</code> namespace that has the <code>app</code> container.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ExposedSecretReport\nmetadata:\ncreationTimestamp: \"2022-06-29T14:25:54Z\"\ngeneration: 2\nlabels:\nresource-spec-hash: 8495697ff5\ntrivy-operator.container.name: app\ntrivy-operator.resource.kind: ReplicaSet\ntrivy-operator.resource.name: app-67b77f5965\ntrivy-operator.resource.namespace: default\nname: replicaset-app-67b77f5965-app\nnamespace: default\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: false\ncontroller: true\nkind: ReplicaSet\nname: app-67b77f5965\nuid: 04a744fe-1126-42d5-bb8b-0917bdb51a28\nresourceVersion: \"1420\"\nuid: 2b2697bb-d528-4d4d-8312-a74dcab6ac65\nreport:\nartifact:\nrepository: myimagewithsecret\ntag: v0.16.1\nregistry:\nserver: index.docker.io\nscanner:\nname: Trivy\nvendor: Aqua Security\nversion: 0.35.0\nsecrets:\n- category: Stripe\nmatch: 'publishable_key: *****'\nruleID: stripe-access-token\nseverity: HIGH\ntarget: \"/app/config/secret.yaml\"\ntitle: Stripe\n- category: Stripe\nmatch: 'secret_key: *****'\nruleID: stripe-access-token\nseverity: HIGH\ntarget: \"/app/config/secret.yaml\"\ntitle: Stripe\nsummary:\ncriticalCount: 0\nhighCount: 2\nlowCount: 0\nmediumCount: 0\nupdateTimestamp: \"2022-06-29T14:29:37Z\"\n</code></pre>"},{"location":"docs/crds/infraassessment-report/","title":"Kubernetes InfraAssessmentReport","text":"<p>An instance of the InfraAssessmentReport represents checks performed by [Trivy],  against a Kubernetes infra core components (etcd,apiserver,scheduler,controller-manager and etc) setting and configuration.</p> <p>The performed checks are based on the k8s cis-benchmarks controls and more.</p> <p>For example, check that api-server <code>Ensure that the --authorization-mode argument is not set to AlwaysAllow</code>.</p> <p>Each report is owned by the underlying Kubernetes object and is stored in the same namespace, following the <code>&lt;workload-kind&gt;-&lt;workload-name&gt;</code> naming convention.</p> <p>The following listing shows a sample InfraAssessmentReport associated with the Pod named <code>kube-apiserver-minikube</code> in the <code>kube-system</code> namespace.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: InfraAssessmentReport\nmetadata:\nannotations:\ntrivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s\ncreationTimestamp: \"2022-11-08T16:27:08Z\"\ngeneration: 1\nlabels:\nplugin-config-hash: 659b7b9c46\nresource-spec-hash: 56fd79dd67\ntrivy-operator.resource.kind: Pod\ntrivy-operator.resource.name: kube-apiserver-minikube\ntrivy-operator.resource.namespace: kube-system\nname: pod-kube-apiserver-minikube\nnamespace: kube-system\nownerReferences:\n- apiVersion: v1\nblockOwnerDeletion: false\ncontroller: true\nkind: Pod\nname: kube-apiserver-minikube\nuid: 60587bf5-1b24-4167-8b77-fe7fa42c0216\nresourceVersion: \"11046\"\nuid: 00f2214a-31c8-4e7c-b0ba-23c7ed0eec2b\nreport:\nchecks:\n- category: Kubernetes Security Check\ncheckID: KCV0020\ndescription: Retain the logs for at least 30 days or as appropriate.\nmessages:\n- Ensure that the --audit-log-maxage argument is set to 30 or as appropriate\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --audit-log-maxage argument is set to 30 or as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0010\ndescription: Limit the rate at which the API server accepts requests.\nmessages:\n- Ensure that the admission control plugin EventRateLimit is set\nseverity: LOW\nsuccess: false\ntitle: Ensure that the admission control plugin EventRateLimit is set\n- category: Kubernetes Security Check\ncheckID: KCV0047\ndescription: Do not use self-signed certificates for TLS.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --peer-auto-tls argument is not set to true\n- category: Kubernetes Security Check\ncheckID: KCV0046\ndescription: etcd should be configured for peer authentication.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --peer-client-cert-auth argument is set to true\n- category: Kubernetes Security Check\ncheckID: KCV0024\ndescription: Validate service account before validating token.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --service-account-lookup argument is set to true\n- category: Kubernetes Security Check\ncheckID: KCV0041\ndescription: Do not bind the scheduler service to non-loopback insecure addresses.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --bind-address argument is set to 127.0.0.1\n- category: Kubernetes Security Check\ncheckID: KCV0013\ndescription: The SecurityContextDeny admission controller can be used to deny\npods which make use of some SecurityContext fields which could allow for privilege\nescalation in the cluster. This should be used where PodSecurityPolicy is not\nin place within the cluster.\nmessages:\n- Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy\nis not used\nseverity: LOW\nsuccess: false\ntitle: Ensure that the admission control plugin SecurityContextDeny is set if\nPodSecurityPolicy is not used\n- category: Kubernetes Security Check\ncheckID: KCV0006\ndescription: Verify kubelet's certificate before establishing connection.\nmessages:\n- Ensure that the --kubelet-certificate-authority argument is set as appropriate\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --kubelet-certificate-authority argument is set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0019\ndescription: Enable auditing on the Kubernetes API Server and set the desired\naudit log path.\nmessages:\n- Ensure that the --audit-log-path argument is set\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --audit-log-path argument is set\n- category: Kubernetes Security Check\ncheckID: KCV0021\ndescription: Retain 10 or an appropriate number of old log files.\nmessages:\n- Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0015\ndescription: Reject creating objects in a namespace that is undergoing termination.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the admission control plugin NamespaceLifecycle is set\n- category: Kubernetes Security Check\ncheckID: KCV0008\ndescription: Restrict kubelet nodes to reading only objects associated with them.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --authorization-mode argument includes Node\n- category: Kubernetes Security Check\ncheckID: KCV0017\ndescription: Do not disable the secure port.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --secure-port argument is not set to 0\n- category: Kubernetes Security Check\ncheckID: KCV0030\ndescription: Encrypt etcd key-value store.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --encryption-provider-config argument is set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0034\ndescription: Disable profiling, if not needed.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --profiling argument is set to false\n- category: Kubernetes Security Check\ncheckID: KCV0007\ndescription: Do not always authorize all requests.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --authorization-mode argument is not set to AlwaysAllow\n- category: Kubernetes Security Check\ncheckID: KCV0009\ndescription: Turn on Role Based Access Control.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --authorization-mode argument includes RBAC\n- category: Kubernetes Security Check\ncheckID: KCV0038\ndescription: Enable kubelet server certificate rotation on controller-manager.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the RotateKubeletServerCertificate argument is set to true\n- category: Kubernetes Security Check\ncheckID: KCV0014\ndescription: Automate service accounts management.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the admission control plugin ServiceAccount is set\n- category: Kubernetes Security Check\ncheckID: KCV0040\ndescription: Disable profiling, if not needed.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --profiling argument is set to false\n- category: Kubernetes Security Check\ncheckID: KCV0022\ndescription: Rotate log files on reaching 100 MB or as appropriate.\nmessages:\n- Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0003\ndescription: This admission controller rejects all net-new usage of the Service\nfield externalIPs.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --DenyServiceExternalIPs is not set\n- category: Kubernetes Security Check\ncheckID: KCV0044\ndescription: Do not use self-signed certificates for TLS.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --auto-tls argument is not set to true\n- category: Kubernetes Security Check\ncheckID: KCV0039\ndescription: Do not bind the scheduler service to non-loopback insecure addresses.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --bind-address argument is set to 127.0.0.1\n- category: Kubernetes Security Check\ncheckID: KCV0005\ndescription: Enable certificate based kubelet authentication.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments\nare set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0042\ndescription: Configure TLS encryption for the etcd service.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --cert-file and --key-file arguments are set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0018\ndescription: Disable profiling, if not needed.\nmessages:\n- Ensure that the --profiling argument is set to false\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --profiling argument is set to false\n- category: Kubernetes Security Check\ncheckID: KCV0025\ndescription: Explicitly set a service account public key file for service accounts\non the apiserver.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --service-account-key-file argument is set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0135\ndescription: Use individual service account credentials for each controller.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --use-service-account-credentials argument is set to true\n- category: Kubernetes Security Check\ncheckID: KCV0043\ndescription: Enable client authentication on etcd service.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --client-cert-auth argument is set to true\n- category: Kubernetes Security Check\ncheckID: KCV0028\ndescription: Setup TLS connection on the API server.\nmessages:\n- Ensure that the --client-ca-file argument is set as appropriate\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --client-ca-file argument is set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0004\ndescription: Use https for kubelet connections.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --kubelet-https argument is set to true\n- category: Kubernetes Security Check\ncheckID: KCV0045\ndescription: etcd should be configured to make use of TLS encryption for peer\nconnections.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --peer-cert-file and --peer-key-file arguments are set\nas appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0033\ndescription: Activate garbage collector on pod termination, as appropriate.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --terminated-pod-gc-threshold argument is set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0011\ndescription: Do not allow all requests.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the admission control plugin AlwaysAdmit is not set\n- category: Kubernetes Security Check\ncheckID: KCV0002\ndescription: Do not use token based authentication.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --token-auth-file parameter is not set\n- category: Kubernetes Security Check\ncheckID: KCV0001\ndescription: Disable anonymous requests to the API server.\nmessages:\n- Ensure that the --anonymous-auth argument is set to false\nseverity: MEDIUM\nsuccess: false\ntitle: Ensure that the --anonymous-auth argument is set to false\n- category: Kubernetes Security Check\ncheckID: KCV0016\ndescription: Limit the Node and Pod objects that a kubelet could modify.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the admission control plugin NodeRestriction is set\n- category: Kubernetes Security Check\ncheckID: KCV0029\ndescription: etcd should be configured to make use of TLS encryption for client\nconnections.\nmessages:\n- Ensure that the --etcd-cafile argument is set as appropriate\nseverity: LOW\nsuccess: false\ntitle: Ensure that the --etcd-cafile argument is set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0027\ndescription: Setup TLS connection on the API server.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --tls-cert-file and --tls-private-key-file arguments are\nset as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0026\ndescription: etcd should be configured to make use of TLS encryption for client\nconnections.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as\nappropriate\n- category: Kubernetes Security Check\ncheckID: KCV0036\ndescription: Explicitly set a service account private key file for service accounts\non the controller manager.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --service-account-private-key-file argument is set as appropriate\n- category: Kubernetes Security Check\ncheckID: KCV0037\ndescription: Allow pods to verify the API server's serving certificate before\nestablishing connections.\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Ensure that the --root-ca-file argument is set as appropriate\nscanner:\nname: Trivy\nvendor: Aqua Security\nversion: dev\nsummary:\ncriticalCount: 0\nhighCount: 0\nlowCount: 10\nmediumCount: 1\n</code></pre>"},{"location":"docs/crds/rbacassessment-report/","title":"RbacAssessmentReport","text":"<p>An instance of the RbacAssessmentReport represents checks performed by configuration auditing tools, such as [Trivy], against a Kubernetes rbac assessment. For example, check that a given Role do not expose permission to secret for all groups</p> <p>Each report is owned by the underlying Kubernetes object and is stored in the same namespace, following the <code>&lt;Role&gt;-&lt;role-name&gt;</code> naming convention.</p> <p>The following listing shows a sample RbacAssessmentReport associated with the Role named <code>role-868458b9d6</code> in the <code>default</code> namespace.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: RbacAssessmentReport\nmetadata:\nannotations:\ntrivy-operator.resource.name: system:controller:token-cleaner\ncreationTimestamp: \"2022-07-04T07:23:07Z\"\ngeneration: 1\nlabels:\nplugin-config-hash: 659b7b9c46\nresource-spec-hash: 59b6bf95c6\ntrivy-operator.resource.kind: Role\ntrivy-operator.resource.name-hash: 868458b9d6\ntrivy-operator.resource.namespace: default\nname: role-868458b9d6\nnamespace: kube-system\nownerReferences:\n- apiVersion: rbac.authorization.k8s.io/v1\nblockOwnerDeletion: false\ncontroller: true\nkind: Role\nname: system:controller:token-cleaner\nuid: 44c6229b-d410-4bc5-9529-fff41de39d03\nresourceVersion: \"7301\"\nuid: 372d7c32-795f-4180-ae84-efd931efaf6e\nreport:\nchecks:\n- category: Kubernetes Security Check\ncheckID: KSV051\ndescription: Check whether role permits creating role bindings and associating\nto privileged role/clusterrole\nmessages:\n- \"\"\nseverity: HIGH\nsuccess: true\ntitle: Do not allow role binding creation and association with privileged role/clusterrole\n- category: Kubernetes Security Check\ncheckID: KSV056\ndescription: The ability to control which pods get service traffic directed to\nthem allows for interception attacks. Controlling network policy allows for\nbypassing lateral movement restrictions.\nmessages:\n- \"\"\nseverity: HIGH\nsuccess: true\ntitle: Do not allow management of networking resources\n- category: Kubernetes Security Check\ncheckID: KSV041\ndescription: Check whether role permits managing secrets\nmessages:\n- Role permits management of secret(s)\nseverity: CRITICAL\nsuccess: false\ntitle: Do not allow management of secrets\n- category: Kubernetes Security Check\ncheckID: KSV047\ndescription: Check whether role permits privilege escalation from node proxy\nmessages:\n- \"\"\nseverity: HIGH\nsuccess: true\ntitle: Do not allow privilege escalation from node proxy\n- category: Kubernetes Security Check\ncheckID: KSV045\ndescription: Check whether role permits wildcard verb on specific resources\nmessages:\n- \"\"\nseverity: CRITICAL\nsuccess: true\ntitle: No wildcard verb roles\n- category: Kubernetes Security Check\ncheckID: KSV054\ndescription: Check whether role permits attaching to shell on pods\nmessages:\n- \"\"\nseverity: HIGH\nsuccess: true\ntitle: Do not allow attaching to shell on pods\n- category: Kubernetes Security Check\ncheckID: KSV044\ndescription: Check whether role permits wildcard verb on wildcard resource\nmessages:\n- \"\"\nseverity: CRITICAL\nsuccess: true\ntitle: No wildcard verb and resource roles\n- category: Kubernetes Security Check\ncheckID: KSV050\ndescription: An effective level of access equivalent to cluster-admin should not\nbe provided.\nmessages:\n- \"\"\nseverity: CRITICAL\nsuccess: true\ntitle: Do not allow management of RBAC resources\n- category: Kubernetes Security Check\ncheckID: KSV046\ndescription: Check whether role permits specific verb on wildcard resources\nmessages:\n- \"\"\nseverity: CRITICAL\nsuccess: true\ntitle: No wildcard resource roles\n- category: Kubernetes Security Check\ncheckID: KSV055\ndescription: Check whether role permits allowing users in a rolebinding to add\nother users to their rolebindings\nmessages:\n- \"\"\nseverity: LOW\nsuccess: true\ntitle: Do not allow users in a rolebinding to add other users to their rolebindings\n- category: Kubernetes Security Check\ncheckID: KSV052\ndescription: Check whether role permits creating role ClusterRoleBindings and\nassociation with privileged cluster role\nmessages:\n- \"\"\nseverity: HIGH\nsuccess: true\ntitle: Do not allow role to create ClusterRoleBindings and association with privileged\nrole\n- category: Kubernetes Security Check\ncheckID: KSV053\ndescription: Check whether role permits getting shell on pods\nmessages:\n- \"\"\nseverity: HIGH\nsuccess: true\ntitle: Do not allow getting shell on pods\n- category: Kubernetes Security Check\ncheckID: KSV042\ndescription: Used to cover attacker\u2019s tracks, but most clusters ship logs quickly\noff-cluster.\nmessages:\n- \"\"\nseverity: MEDIUM\nsuccess: true\ntitle: Do not allow deletion of pod logs\n- category: Kubernetes Security Check\ncheckID: KSV049\ndescription: Some workloads leverage configmaps to store sensitive data or configuration\nparameters that affect runtime behavior that can be modified by an attacker\nor combined with another issue to potentially lead to compromise.\nmessages:\n- \"\"\nseverity: MEDIUM\nsuccess: true\ntitle: Do not allow management of configmaps\n- category: Kubernetes Security Check\ncheckID: KSV043\ndescription: Check whether role permits impersonating privileged groups\nmessages:\n- \"\"\nseverity: CRITICAL\nsuccess: true\ntitle: Do not allow impersonation of privileged groups\n- category: Kubernetes Security Check\ncheckID: KSV048\ndescription: Check whether role permits update/create of a malicious pod\nmessages:\n- \"\"\nseverity: HIGH\nsuccess: true\ntitle: Do not allow update/create of a malicious pod\nscanner:\nname: Trivy\nvendor: Aqua Security\nversion: '0.16.1'\nsummary:\ncriticalCount: 1\nhighCount: 0\nlowCount: 0\nmediumCount: 0\nupdateTimestamp: null\n</code></pre>"},{"location":"docs/crds/sbom-report/","title":"SbomReport","text":"<p>An instance of the SbomReport represents the latest sbom (software bill of metarials) found in a container image of a given Kubernetes workload. It consists of a list of OS package and application bil of metarial with a summary of components and dependencies. For a multi-container workload trivy-operator creates multiple instances of SbomReports in the workload's namespace with the owner reference set to that workload. Each report follows the naming convention <code>&lt;workload kind&gt;-&lt;workload name&gt;-&lt;container-name&gt;</code>.</p> <p>The following listing shows a sample SbomReport associated with the Pod named <code>kube-apiserver-kind-control-plane</code> in the <code>kube-system</code> namespace that has the <code>kube-apiserver</code> container.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: SbomReport\nmetadata:\ncreationTimestamp: \"2023-07-10T09:37:21Z\"\ngeneration: 1\nlabels:\nresource-spec-hash: 796669cd5d\ntrivy-operator.container.name: kube-apiserver\ntrivy-operator.resource.kind: Pod\ntrivy-operator.resource.name: kube-apiserver-kind-control-plane\ntrivy-operator.resource.namespace: kube-system\nname: pod-kube-apiserver-kind-control-plane-kube-apiserver\nnamespace: kube-system\nownerReferences:\n- apiVersion: v1\nblockOwnerDeletion: false\ncontroller: true\nkind: Pod\nname: kube-apiserver-kind-control-plane\nuid: 732b4aa7-91f8-40a3-8b21-9627a98a910b\nresourceVersion: \"6148\"\nuid: 2a5000fe-b97e-46d0-9de7-62fb5fbc6555\nreport:\nartifact:\nrepository: kube-apiserver\ntag: v1.21.1\ncomponents:\nbomFormat: CycloneDX\ncomponents:\n- bom-ref: 9464f5f9-750d-4ea0-8705-c8d067b25b29\nname: debian\nproperties:\n- name: aquasecurity:trivy:Class\nvalue: os-pkgs\n- name: aquasecurity:trivy:Type\nvalue: debian\nsupplier: {}\ntype: operating-system\nversion: \"10.9\"\n- bom-ref: pkg:deb/debian/base-files@10.3+deb10u9?arch=amd64&amp;distro=debian-10.9\nlicenses:\n- expression: GPL-3.0\nlicense: {}\nname: base-files\nproperties:\n- name: aquasecurity:trivy:LayerDiffID\nvalue: sha256:417cb9b79adeec55f58b890dc9831e252e3523d8de5fd28b4ee2abb151b7dc8b\n- name: aquasecurity:trivy:LayerDigest\nvalue: sha256:5dea5ec2316d4a067b946b15c3c4f140b4f2ad607e73e9bc41b673ee5ebb99a3\n- name: aquasecurity:trivy:PkgID\nvalue: base-files@10.3+deb10u9\n- name: aquasecurity:trivy:PkgType\nvalue: debian\n- name: aquasecurity:trivy:SrcName\nvalue: base-files\n- name: aquasecurity:trivy:SrcVersion\nvalue: 10.3+deb10u9\npurl: pkg:deb/debian/base-files@10.3+deb10u9?arch=amd64&amp;distro=debian-10.9\nsupplier:\nname: Santiago Vila &lt;sanvila@debian.org&gt;\ntype: library\nversion: 10.3+deb10u9\n- bom-ref: pkg:deb/debian/netbase@5.6?arch=all&amp;distro=debian-10.9\nlicenses:\n- expression: GPL-2.0\nlicense: {}\nname: netbase\nproperties:\n- name: aquasecurity:trivy:LayerDiffID\nvalue: sha256:417cb9b79adeec55f58b890dc9831e252e3523d8de5fd28b4ee2abb151b7dc8b\n- name: aquasecurity:trivy:LayerDigest\nvalue: sha256:5dea5ec2316d4a067b946b15c3c4f140b4f2ad607e73e9bc41b673ee5ebb99a3\n- name: aquasecurity:trivy:PkgID\nvalue: netbase@5.6\n- name: aquasecurity:trivy:PkgType\nvalue: debian\n- name: aquasecurity:trivy:SrcName\nvalue: netbase\n- name: aquasecurity:trivy:SrcVersion\nvalue: \"5.6\"\npurl: pkg:deb/debian/netbase@5.6?arch=all&amp;distro=debian-10.9\nsupplier:\nname: Marco d'Itri &lt;md@linux.it&gt;\ntype: library\nversion: \"5.6\"\n- bom-ref: pkg:deb/debian/tzdata@2021a-0+deb10u1?arch=all&amp;distro=debian-10.9\nname: tzdata\nproperties:\n- name: aquasecurity:trivy:LayerDiffID\nvalue: sha256:417cb9b79adeec55f58b890dc9831e252e3523d8de5fd28b4ee2abb151b7dc8b\n- name: aquasecurity:trivy:LayerDigest\nvalue: sha256:5dea5ec2316d4a067b946b15c3c4f140b4f2ad607e73e9bc41b673ee5ebb99a3\n- name: aquasecurity:trivy:PkgID\nvalue: tzdata@2021a-0+deb10u1\n- name: aquasecurity:trivy:PkgType\nvalue: debian\n- name: aquasecurity:trivy:SrcName\nvalue: tzdata\n- name: aquasecurity:trivy:SrcRelease\nvalue: 0+deb10u1\n- name: aquasecurity:trivy:SrcVersion\nvalue: 2021a\npurl: pkg:deb/debian/tzdata@2021a-0+deb10u1?arch=all&amp;distro=debian-10.9\nsupplier:\nname: GNU Libc Maintainers &lt;debian-glibc@lists.debian.org&gt;\ntype: library\nversion: 2021a-0+deb10u1\ndependencies:\n- dependsOn:\n- pkg:deb/debian/base-files@10.3+deb10u9?arch=amd64&amp;distro=debian-10.9\n- pkg:deb/debian/netbase@5.6?arch=all&amp;distro=debian-10.9\n- pkg:deb/debian/tzdata@2021a-0+deb10u1?arch=all&amp;distro=debian-10.9\nref: 9464f5f9-750d-4ea0-8705-c8d067b25b29\n- dependsOn: []\nref: pkg:deb/debian/base-files@10.3+deb10u9?arch=amd64&amp;distro=debian-10.9\n- dependsOn: []\nref: pkg:deb/debian/netbase@5.6?arch=all&amp;distro=debian-10.9\n- dependsOn: []\nref: pkg:deb/debian/tzdata@2021a-0+deb10u1?arch=all&amp;distro=debian-10.9\n- dependsOn:\n- 9464f5f9-750d-4ea0-8705-c8d067b25b29\nref: pkg:oci/kube-apiserver@sha256:53a13cd1588391888c5a8ac4cef13d3ee6d229cd904038936731af7131d193a9?repository_url=k8s.gcr.io%2Fkube-apiserver&amp;arch=amd64\nmetadata:\ncomponent:\nbom-ref: pkg:oci/kube-apiserver@sha256:53a13cd1588391888c5a8ac4cef13d3ee6d229cd904038936731af7131d193a9?repository_url=k8s.gcr.io%2Fkube-apiserver&amp;arch=amd64\nname: k8s.gcr.io/kube-apiserver:v1.21.1\nproperties:\n- name: aquasecurity:trivy:DiffID\nvalue: sha256:417cb9b79adeec55f58b890dc9831e252e3523d8de5fd28b4ee2abb151b7dc8b,sha256:b50131762317bbe47def2d426d5c78a353a08b966d36bed4a04aee99dde4e12b,sha256:1e6ed7621dee7e03dd779486ed469a65af6fb13071d13bd3a89c079683e3b1f0\n- name: aquasecurity:trivy:ImageID\nvalue: sha256:771ffcf9ca634e37cbd3202fd86bd7e2df48ecba4067d1992541bfa00e88a9bb\n- name: aquasecurity:trivy:RepoDigest\nvalue: k8s.gcr.io/kube-apiserver@sha256:53a13cd1588391888c5a8ac4cef13d3ee6d229cd904038936731af7131d193a9\n- name: aquasecurity:trivy:RepoTag\nvalue: k8s.gcr.io/kube-apiserver:v1.21.1\n- name: aquasecurity:trivy:SchemaVersion\nvalue: \"2\"\npurl: pkg:oci/kube-apiserver@sha256:53a13cd1588391888c5a8ac4cef13d3ee6d229cd904038936731af7131d193a9?repository_url=k8s.gcr.io%2Fkube-apiserver&amp;arch=amd64\nsupplier: {}\ntype: container\ntimestamp: \"2023-07-10T09:37:21+00:00\"\ntools:\n- name: trivy\nvendor: aquasecurity\nserialNumber: urn:uuid:50dbce86-28c5-4caf-9d08-a4aadf23233e\nspecVersion: 1.4\nversion: 1\nregistry:\nserver: k8s.gcr.io\nscanner:\nname: Trivy\nvendor: Aqua Security\nversion: 0.45.0\nsummary:\ncomponentsCount: 5\ndependenciesCount: 5\nupdateTimestamp: \"2023-07-10T09:37:21Z\"\n</code></pre>"},{"location":"docs/crds/vulnerability-report/","title":"VulnerabilityReport","text":"<p>An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For a multi-container workload trivy-operator creates multiple instances of VulnerabilityReports in the workload's namespace with the owner reference set to that workload. Each report follows the naming convention <code>&lt;workload kind&gt;-&lt;workload name&gt;-&lt;container-name&gt;</code>.</p> <p>The following listing shows a sample VulnerabilityReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace that has the <code>nginx</code> container without any additional options.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\nname: replicaset-nginx-6d4cf56db6-nginx\nnamespace: default\nlabels:\ntrivy-operator.container.name: nginx\ntrivy-operator.resource.kind: ReplicaSet\ntrivy-operator.resource.name: nginx-6d4cf56db6\ntrivy-operator.resource.namespace: default\nresource-spec-hash: 7cb64cb677\nuid: 8aa1a7cb-a319-4b93-850d-5a67827dfbbf\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: false\ncontroller: true\nkind: ReplicaSet\nname: nginx-6d4cf56db6\nuid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\nartifact:\nrepository: library/nginx\ntag: '1.16'\nregistry:\nserver: index.docker.io\nscanner:\nname: Trivy\nvendor: Aqua Security\nversion: 0.35.0\nsummary:\ncriticalCount: 2\nhighCount: 0\nlowCount: 0\nmediumCount: 0\nunknownCount: 0\nvulnerabilities:\n- fixedVersion: 0.9.1-2+deb10u1\ninstalledVersion: 0.9.1-2\nlinks: []\nprimaryLink: 'https://avd.aquasec.com/nvd/cve-2019-20367'\nresource: libbsd0\nscore: 9.1\nseverity: CRITICAL\ntarget: library/nginx:1.21.6\ntitle: ''\nvulnerabilityID: CVE-2019-20367\n- fixedVersion: ''\ninstalledVersion: 0.6.1-2\nlinks: []\nprimaryLink: 'https://avd.aquasec.com/nvd/cve-2018-25009'\nresource: libwebp6\nscore: 9.1\nseverity: CRITICAL\ntarget: library/nginx:1.16\ntitle: 'libwebp: out-of-bounds read in WebPMuxCreateInternal'\nvulnerabilityID: CVE-2018-25009\n</code></pre> <p>Note</p> <p>For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288).</p> <p>Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with trivy-operator. You can find the list of available integrations here.</p> <p>It's possible to get more information from report, like Description, Links, CVSS and Target. The following listing shows a sample of extended VulnerabilityReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace that has the <code>nginx</code> container with additional options. Please refer to the \"Vulnerability Scanner Configuration\" how to make it. Use with caution, because Links can generate lots of information and report can exceed the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\nname: replicaset-nginx-6d4cf56db6-nginx\nnamespace: default\nlabels:\ntrivy-operator.container.name: nginx\ntrivy-operator.resource.kind: ReplicaSet\ntrivy-operator.resource.name: nginx-6d4cf56db6\ntrivy-operator.resource.namespace: default\nresource-spec-hash: 7cb64cb677\nuid: 8aa1a7cb-a319-4b93-850d-5a67827dfbbf\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: false\ncontroller: true\nkind: ReplicaSet\nname: nginx-6d4cf56db6\nuid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\nartifact:\nrepository: library/nginx\ntag: '1.16'\nregistry:\nserver: index.docker.io\nscanner:\nname: Trivy\nvendor: Aqua Security\nversion: 0.35.0\nsummary:\ncriticalCount: 2\nhighCount: 0\nlowCount: 0\nmediumCount: 0\nunknownCount: 0\nvulnerabilities:\n- cvss:\nnvd:\nV2Score: 4.6\nV2Vector: AV:L/AC:L/Au:N/C:P/I:P/A:P\nV3Score: 5.7\nV3Vector: CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:L/I:L/A:L\nredhat:\nV3Score: 5.7\nV3Vector: CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:L/I:L/A:L\ndescription: 'APT had several integer overflows and underflows while parsing .deb\npackages, aka GHSL-2020-168 GHSL-2020-169, in files apt-pkg/contrib/extracttar.cc,\napt-pkg/deb/debfile.cc, and apt-pkg/contrib/arfile.cc. This issue affects: apt\n1.2.32ubuntu0 versions prior to 1.2.32ubuntu0.2; 1.6.12ubuntu0 versions prior\nto 1.6.12ubuntu0.2; 2.0.2ubuntu0 versions prior to 2.0.2ubuntu0.2; 2.1.10ubuntu0\nversions prior to 2.1.10ubuntu0.1;'\nfixedVersion: 1.8.2.2\ninstalledVersion: 1.8.2\nlinks:\n- https://access.redhat.com/security/cve/CVE-2020-27350\n- https://bugs.launchpad.net/bugs/1899193\n- https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-27350\n- https://security.netapp.com/advisory/ntap-20210108-0005/\n- https://ubuntu.com/security/notices/USN-4667-1\n- https://ubuntu.com/security/notices/USN-4667-2\n- https://usn.ubuntu.com/usn/usn-4667-1\n- https://www.debian.org/security/2020/dsa-480\nprimaryLink: https://avd.aquasec.com/nvd/cve-2020-27350\nresource: apt\nseverity: MEDIUM\ntarget: nginx:1.16 (debian 10.3)\ntitle: 'apt: integer overflows and underflows while parsing .deb packages'\nvulnerabilityID: CVE-2020-27350\n- cvss:\nnvd:\nV2Score: 4.3\nV2Vector: AV:N/AC:M/Au:N/C:N/I:N/A:P\nV3Score: 5.5\nV3Vector: CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H\ndescription: Missing input validation in the ar/tar implementations of APT before\nversion 2.1.2 could result in denial of service when processing specially crafted\ndeb files.\nfixedVersion: 1.8.2.1\ninstalledVersion: 1.8.2\nlinks:\n- https://bugs.launchpad.net/bugs/1878177\",\n- https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-3810\n- https://github.com/Debian/apt/issues/111\n- https://github.com/julian-klode/apt/commit/de4efadc3c92e26d37272fd310be148ec61dcf36\n- https://lists.debian.org/debian-security-announce/2020/msg00089.html\n- https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/U4PEH357MZM2SUGKETMEHMSGQS652QHH/\n- https://salsa.debian.org/apt-team/apt/-/commit/dceb1e49e4b8e4dadaf056be34088b415939cda6\n- https://salsa.debian.org/jak/apt/-/commit/dceb1e49e4b8e4dadaf056be34088b415939cda6\n- https://tracker.debian.org/news/1144109/accepted-apt-212-source-into-unstable/\n- https://ubuntu.com/security/notices/USN-4359-1\n- https://ubuntu.com/security/notices/USN-4359-2\n- https://usn.ubuntu.com/4359-1/\n- https://usn.ubuntu.com/4359-2/\nprimaryLink: https://avd.aquasec.com/nvd/cve-2020-3810\nresource: apt\nseverity: MEDIUM\ntarget: nginx:1.16 (debian 10.3)\ntitle: Missing input validation in the ar/tar implementations of APT before v\n...\nvulnerabilityID: CVE-2020-3810\n</code></pre>"},{"location":"docs/design/","title":"Design Documents","text":"<p>An index of various (informal) design and explanation documents that were created for different purposes. Mainly to brainstorm how trivy-operator works.</p> <p>NOTE This is not an official documentation of trivy-operator. Some design documents may be out of date.</p>"},{"location":"docs/design/#overview","title":"Overview","text":"File Description caching_scan_result_by_repo_digest.md [DRAFT] Caching Scan Results by Image Reference design_trivy_file_system_scanner.md Scan Container Images with Trivy Filesystem Scanner design_vulnerability_scan_in_same_ns.md Schedule vulnerability scan jobs in the same namespace as scanned workload design_scan_by_image_digest.png Design of vulnerability scanning by image digest (ContainerStatus vs PodSpec). design_vulnerability_scanning_2.0.png Design of most efficient vulnerability scanning that you can imagine. design_namespace_security_report.pdf Design of a security report generated by trivy-operator CLI for a given namespace."},{"location":"docs/design/caching_scan_results_by_repo_digest/","title":"[DRAFT] Caching Scan Results by Image Reference","text":""},{"location":"docs/design/caching_scan_results_by_repo_digest/#tldr","title":"TL;DR","text":"<p>To find vulnerabilities in container images Trivy-Operator creates asynchronous Kubernetes (K8s) Jobs. Even though running a vulnerability scanner as a K8s Job is expensive, Trivy-Operator does not reuse scan results in any way. For example, if a workload refers to the image that has already been scanned, Trivy-Operator will go ahead and create another (similar) K8s Job.</p> <p>To some extent, the problem of wasteful and long-running K8s Jobs can be mitigated by using Trivy-Operator with Trivy in the ClientServer mode instead of the default Standalone mode. In this case a configured Trivy server will cache results of scanning image layers. However, there is still unnecessary overhead for managing K8s Jobs and communication between Trivy client and server. (The only real difference is that some Jobs may complete faster for already scanned images.)</p> <p>To solve the above-mentioned problems, we could cache scan results by image reference. For example, a CRD based implementation can store scan results as instances of ClusterVulnerabilityReport object named after a hash of the repo digest. An alternative implementation may cache vulnerability reports in an AWS S3 bucket or a similar key-value store.</p>"},{"location":"docs/design/caching_scan_results_by_repo_digest/#example","title":"Example","text":"<p>With the proposed cluster-scoped (or global) cache, Trivy-Operator can check if the image with the specified reference has already been scanned. If yes, it will just read the corresponding ClusterVulnerabilityReport, copy its payload, and finally create an instance of a namespaced VulnerabilityReport.</p> <p>Let's consider two <code>nginx:1.16</code> Deployments in two different namespaces <code>foo</code> and <code>bar</code>. In the current implementation Trivy-Operator will spin up two K8s Jobs to run a scanner and eventually create two VulnerabilityReports in <code>foo</code> and <code>bar</code> namespaces respectively.</p> <p>In a cluster where Trivy-Operator is installed for the first time, when we scan the <code>nginx</code> Deployment in the <code>foo</code> namespace there's obviously no ClusterVulnerabilityReport for <code>nginx:1.16</code>. Therefore, Trivy-Operator will spin up a K8s Job and wait for its completion. On completion, it will create a cluster-scoped ClusterVulnerabilityReport named after the hash of <code>nginx:1.16</code>. It will also create a namespaced VulnerabilityReport named after the current revision of the <code>nginx</code> Deployment.</p> <p>NOTE Because a repo digest is not a valid name for a K8s API object, we may, for example, calculate a (safe) hash of the repo digest and use is as name instead.</p> <pre><code>$ kubectl get clustervulnerabilityreports\nNo resources found\n</code></pre> <pre><code>$ trivy-operator scan vulnerabilityreports deploy/nginx -n foo -v 3\nI1008 19:58:19.355462   62385 scanner.go:72] Getting Pod template for workload: {Deployment nginx foo}\nI1008 19:58:19.358802   62385 scanner.go:89] Checking if images were already scanned\nI1008 19:58:19.360411   62385 scanner.go:95] Cached scan reports: 0\nI1008 19:58:19.360421   62385 scanner.go:101] Scanning with options: {ScanJobTimeout:0s DeleteScanJob:true}\nI1008 19:58:19.365155   62385 runner.go:79] Running task and waiting forever\nI1008 19:58:19.365190   62385 runnable_job.go:74] Creating job \"trivy-operator/scan-vulnerabilityreport-cbf8c9b99\"\nI1008 19:58:19.376902   62385 reflector.go:219] Starting reflector *v1.Event (30m0s) from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376920   62385 reflector.go:255] Listing and watching *v1.Event from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376902   62385 reflector.go:219] Starting reflector *v1.Job (30m0s) from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376937   62385 reflector.go:255] Listing and watching *v1.Job from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.386049   62385 runnable_job.go:130] Event: Created pod: scan-vulnerabilityreport-cbf8c9b99-4nzkb (SuccessfulCreate)\nI1008 19:58:51.243554   62385 runnable_job.go:130] Event: Job completed (Completed)\nI1008 19:58:51.247251   62385 runnable_job.go:109] Stopping runnable job on task completion with status: Complete\nI1008 19:58:51.247273   62385 runner.go:83] Stopping runner on task completion with error: &lt;nil&gt;\nI1008 19:58:51.247278   62385 scanner.go:130] Scan job completed: trivy-operator/scan-vulnerabilityreport-cbf8c9b99\nI1008 19:58:51.247297   62385 scanner.go:262] Getting logs for nginx container in job: trivy-operator/scan-vulnerabilityreport-cbf8c9b99\nI1008 19:58:51.674449   62385 scanner.go:123] Deleting scan job: trivy-operator/scan-vulnerabilityreport-cbf8c9b99\n</code></pre> <p>Now, if we scan the <code>nginx</code> Deployment in the <code>bar</code> namespace, Trivy-Operator will see that there's already a ClusterVulnerabilityReport (<code>84bcb5cd46</code>) for the same image reference <code>nginx:1.16</code> and will skip creation of a K8s Job. It will just read and copy the report as VulnerabilityReport object to the <code>bar</code> namespace.</p> <pre><code>$ kubectl get clustervulnerabilityreports -o wide\nNAME         REPOSITORY      TAG    DIGEST   SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\n84bcb5cd46   library/nginx   1.16            Trivy     17s   21         50     33       104   0\n</code></pre> <pre><code>$ trivy-operator scan vulnerabilityreports deploy/nginx -n bar -v 3\nI1008 19:59:23.891718   62478 scanner.go:72] Getting Pod template for workload: {Deployment nginx bar}\nI1008 19:59:23.895310   62478 scanner.go:89] Checking if image nginx:1.16 was already scanned\nI1008 19:59:23.903058   62478 scanner.go:95] Cache hit\nI1008 19:59:23.903078   62478 scanner.go:97] Copying ClusterVulnerabilityReport to VulnerabilityReport\n</code></pre> <p>As you can see, Trivy-Operator eventually created two VulnerabilityReports by spinning up only one K8s Job.</p> <pre><code>$ kubectl get vulnerabilityreports -A\nNAMESPACE   NAME                                REPOSITORY      TAG    SCANNER   AGE\nbar         replicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     5m38s\nfoo         replicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     6m10s\n</code></pre>"},{"location":"docs/design/caching_scan_results_by_repo_digest/#life-cycle-management","title":"Life-cycle management","text":"<p>Just like any other cache it's very important that it's up to date and contains the correct information. To make sure of this we need to have a automated way of automatically cleaning up the ClusterVulnerabilityReport after some time.</p> <p>My suggestion is to solve this problem just like we did in PR #879. For each ClusterVulnerabilityReport created we should annotate the report with <code>trivy-operator.aquasecurity.github.io/cluster-vulnerability-report-ttl</code>. When the TTL ends the other controller will automatically delete the existing ClusterVulnerabilityReport and the next time the image is created in the cluster and normal vulnerabilityreport scan will happen.</p> <p>I suggest that we have a default value of 72 hours for this report. This is a new feature and I don't see why we shouldn't enable it by default.</p>"},{"location":"docs/design/caching_scan_results_by_repo_digest/#vulnerability-reports","title":"Vulnerability reports","text":"<p>From a vulnerability reports point of view we need to have a simple way for cluster admins to know if the vulnerability report is generated from a cache and if so which one?</p> <p>We could ether do this by setting a status on the vulnerability report that gets created but since this feature won't be on by default I suggest we use annotations.</p> <p>For example: <code>trivy-operator.aquasecurity.github.io/ClusterVulnerabilityReportName: 84bcb5cd46</code> would make it easy to find. We can't use something like ownerReference since it would delete all vulnerabilities at the same time if a ClusterVulnerabilityReport was deleted.</p>"},{"location":"docs/design/caching_scan_results_by_repo_digest/#summary","title":"Summary","text":"<ul> <li>This solution might be the first step towards more efficient vulnerability scanning.</li> <li>It's backward compatible and can be implemented as an experimental feature behind   a gate.</li> <li>Both Trivy-Operator CLI and Trivy-Operator Operator can read and leverage ClusterVulnerabilityReports.</li> </ul>"},{"location":"docs/design/design_compliance_report/","title":"Support Compliance Reports","text":""},{"location":"docs/design/design_compliance_report/#overview","title":"Overview","text":"<p>It is required to leverage trivy-operator security tools capabilities by adding the support for building compliance reports example : NSA - Kubernetes Hardening Guidance</p>"},{"location":"docs/design/design_compliance_report/#solution","title":"Solution","text":""},{"location":"docs/design/design_compliance_report/#tldr","title":"TL;DR","text":"<ul> <li>A cluster compliance resource ,nsa-1.0.yaml (example below), with spec definition only will be deployed to kubernetes cluster upon startup</li> <li>the spec definition will include the control check , cron expression for periodical generation, and it's mapping to scanners (kube-bench and audit-config)</li> <li>a new cluster compliance reconcile loop will be introduced to track this cluster compliance resource</li> <li>when the cluster spec is reconcile  it check if cron expression match current time , if so it generates a compliance report and update the status section with report data</li> <li>if cron expression do not match the event will be requeue until next generation time</li> <li>Two new CRDs will be introduced :</li> <li><code>ClusterComplianceReport</code> to provide summary of the compliance per control</li> <li><code>ClusterComplianceDetailReport</code> to provide more detail compliance report for further investigation</li> <li>It is assumed that all scanners (kube-bench / config-audit) are running by default all the time and producing raw data</li> </ul>"},{"location":"docs/design/design_compliance_report/#the-spec-file","title":"The Spec file","text":"<ul> <li>The spec will include the mapping (based on Ids) between the compliance report and tools(kube-bench and config-audit) which generate the raw data</li> <li>The spec file will be loaded from the file system</li> </ul>"},{"location":"docs/design/design_compliance_report/#example-for-spec","title":"Example for spec","text":"<pre><code>---\napiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\nname: nsa\nspec:\nname: nsa\ndescription: National Security Agency - Kubernetes Hardening Guidance\nversion: \"1.0\"\ncron: \"0 */3 * * *\"\ncontrols:\n- name: Non-root containers\ndescription: 'Check that container is not running as root'\nid: '1.0'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV012\nseverity: 'MEDIUM'\n- name: Immutable container file systems\ndescription: 'Check that container root file system is immutable'\nid: '1.1'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV014\nseverity: 'LOW'\n- name: Preventing privileged containers\ndescription: 'Controls whether Pods can run privileged containers'\nid: '1.2'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV017\nseverity: 'HIGH'\n- name: Share containers process namespaces\ndescription: 'Controls whether containers can share process namespaces'\nid: '1.3'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV008\nseverity: 'HIGH'\n- name: Share host process namespaces.\ndescription: 'Controls whether share host process namespaces'\nid: '1.4'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV009\nseverity: 'HIGH'\n- name: use the host network\ndescription: 'Controls whether containers can use the host network'\nid: '1.5'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV010\nseverity: 'HIGH'\n- name:  Run with root privileges or with root group membership\ndescription: 'Controls whether container applications can run with root privileges or with root group membership'\nid: '1.6'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV029\nseverity: 'LOW'\n- name: Restricts escalation to root privileges\ndescription: 'Control check restrictions escalation to root privileges'\nid: '1.7'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV001\nseverity: 'MEDIUM'\n- name: Sets the SELinux context of the container\ndescription: 'Control checks if pod sets the SELinux context of the container'\nid: '1.8'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV002\nseverity: 'MEDIUM'\n- name: Restrict a container's access to resources with AppArmor\ndescription: 'Control checks the restriction of containers access to resources with AppArmor'\nid: '1.9'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV030\nseverity: 'MEDIUM'\n- name: Sets the seccomp profile used to sandbox containers.\ndescription: 'Control checks the sets the seccomp profile used to sandbox containers'\nid: '1.10'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV030\nseverity: 'LOW'\n- name: Protecting Pod service account tokens\ndescription: 'Control check whether disable secret token been mount ,automountServiceAccountToken: false'\nid: '1.11'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV036\nseverity: 'MEDIUM'\n- name: Namespace kube-system should not be used by users\ndescription: 'Control check whether Namespace kube-system is not be used by users'\nid: '1.12'\nkinds:\n- NetworkPolicy\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV037\nseverity: 'MEDIUM'\n- name: Pod and/or namespace Selectors usage\ndescription: 'Control check validate the pod and/or namespace Selectors usage'\nid: '2.0'\nkinds:\n- Workload\nmapping:\nscanner: config-audit\nchecks:\n- id: KSV038\nseverity: 'MEDIUM'\n- name: Use CNI plugin that supports NetworkPolicy API\ndescription: 'Control check whether check cni plugin installed '\nid: '3.0'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 5.3.1\nseverity: 'CRITICAL'\n- name: Use ResourceQuota policies to limit resources\ndescription: 'Control check the use of ResourceQuota policies to limit resources'\nid: '4.0'\nkinds:\n- ResourceQuota\nmapping:\nscanner: config-audit\nchecks:\n- id: \"&lt;check need to be added&gt;\"\nseverity: 'CRITICAL'\n- name: Control plan disable insecure port\ndescription: 'Control check whether control plan disable insecure port'\nid: '5.0'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 1.2.19\nseverity: 'CRITICAL'\n- name: Encrypt etcd communication\ndescription: 'Control check whether etcd communication is encrypted'\nid: '5.1'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: '2.1'\nseverity: 'CRITICAL'\n- name: Ensure kube config file permission\ndescription: 'Control check whether kube config file permissions'\nid: '6.0'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 4.1.3\n- id: 4.1.4\nseverity: 'CRITICAL'\n- name: Check that encryption resource has been set\ndescription: 'Control checks whether encryption resource has been set'\nid: '6.1'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 1.2.31\n- id: 1.2.32\nseverity: 'CRITICAL'\n- name: Check encryption provider\ndescription: 'Control checks whether encryption provider has been set'\nid: '6.2'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 1.2.3\nseverity: 'CRITICAL'\n- name: Make sure anonymous-auth is unset\ndescription: 'Control checks whether anonymous-auth is unset'\nid: '7.0'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 1.2.1\nseverity: 'CRITICAL'\n- name: Make sure -authorization-mode=RBAC\ndescription: 'Control check whether RBAC permission is in use'\nid: '7.1'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 1.2.7\n- id: 1.2.8\nseverity: 'CRITICAL'\n- name: Audit policy is configure\ndescription: 'Control check whether audit policy is configure'\nid: '8.0'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 3.2.1\nseverity: 'HIGH'\n- name: Audit log path is configure\ndescription: 'Control check whether audit log path is configure'\nid: '8.1'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 1.2.22\nseverity: 'MEDIUM'\n- name: Audit log aging\ndescription: 'Control check whether audit log aging is configure'\nid: '8.2'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: 1.2.23\nseverity: 'MEDIUM'\n- name: Service mesh is configure\ndescription: 'Control check whether service mesh is used in cluster'\nid: '9.0'\nkinds:\n- Node\nmapping:\nscanner: kube-bench\nchecks:\n- id: \"&lt;check need to be added&gt;\"\nseverity: 'MEDIUM'\n....\n```\n\n### The logic\n\nUpon trivy-operator start cluster compliance reconcile loop will track the deployed spec file ,nsa-1.0 spec and evaluation the cron expression in spec file,\nif  the cron interval matches , trivy-operator will generate the compliance and compliance detail reports :\n\n- `ClusterComplianceReport` status section will be updated with report data\n- `ClusterComplianceDetailReport` will be generated by and saved to etcd\n\n### The mapping\n\nOnce it is determined that a report need to be generated:\n\n- all reports (cis-benchmark and audit config) raw data will be fetched by `tool` and `resource` types\n- trivy-operator will iterate all fetched raw data and find a match by `ID`\n- once the data has been mapped and aggregated 2 type of reports will be generated to present summary\ndata and detailed data (in case further investigation need to be made)\n\n### Note: once the report has been generated again to reconcile loop start again the process describe in logic\n\n### The Reports\n\n#### Example: Compliance spec and status section (report data)\n\n```json\n{\n\"kind\": \"ClusterComplianceReport\",\n\"apiVersion\": \"aquasecurity.github.io/v1alpha1\",\n\"metadata\": {\n\"name\": \"nsa\",\n\"resourceVersion\": \"1000\",\n\"creationTimestamp\": null\n},\n\"spec\": {\n\"kind\": \"compliance\",\n\"name\": \"nsa\",\n\"description\": \"National Security Agency - Kubernetes Hardening Guidance\",\n\"cron\": \"* * * * *\",\n\"version\": \"1.0\",\n\"controls\": [\n{\n\"id\": \"1.0\",\n\"name\": \"Non-root containers\",\n\"description\": \"\",\n\"resources\": [\n\"Workload\"\n],\n\"mapping\": {\n\"tool\": \"config-audit\",\n\"checks\": [\n{\n\"id\": \"KSV012\"\n}\n]\n}\n},\n{\n\"id\": \"8.2\",\n\"name\": \"Audit log aging\",\n\"description\": \"\",\n\"resources\": [\n\"Node\"\n],\n\"mapping\": {\n\"tool\": \"kube-bench\",\n\"checks\": [\n{\n\"id\": \"1.2.23\"\n}\n]\n}\n}\n]\n},\n\"status\": {\n\"updateTimestamp\": \"2022-02-26T14:11:39Z\",\n\"summary\": {\n\"passCount\": 3,\n\"failCount\": 3\n},\n\"control_check\": [\n{\n\"id\": \"1.1\",\n\"name\": \"Immutable container file systems\",\n\"passTotal\": 0,\n\"failTotal\": 3,\n\"severity\": \"\"\n}\n]\n}\n}\n</code></pre>"},{"location":"docs/design/design_compliance_report/#compliance-details-report","title":"Compliance details report","text":"<pre><code>{\n\"kind\": \"ClusterComplianceDetailReport\",\n\"apiVersion\": \"aquasecurity.github.io/v1alpha1\",\n\"metadata\": {\n\"name\": \"nsa-details\",\n\"resourceVersion\": \"1\"\n},\n\"report\": {\n\"updateTimestamp\": \"2022-02-26T14:05:29Z\",\n\"type\": {\n\"kind\": \"compliance\",\n\"name\": \"nsa-details\",\n\"description\": \"national security agency - kubernetes hardening guidance\",\n\"version\": \"1.0\"\n},\n\"summary\": {\n\"passCount\": 3,\n\"failCount\": 3\n},\n\"controlCheck\": [\n{\n\"id\": \"1.1\",\n\"name\": \"Immutable container file systems\",\n\"checkResults\": [\n{\n\"objectType\": \"Pod\",\n\"id\": \"KSV014\",\n\"remediation\": \"\",\n\"details\": [\n{\n\"name\": \"pod-rss-site\",\n\"namespace\": \"default\",\n\"msg\": \"Container 'front-end' of Pod 'rss-site' should set 'securityContext.readOnlyRootFilesystem' to true\",\n\"status\": \"fail\"\n},\n{\n\"name\": \"pod-rss-site\",\n\"namespace\": \"default\",\n\"msg\": \"Container 'rss-reader' of Pod 'rss-site' should set 'securityContext.readOnlyRootFilesystem' to true\",\n\"status\": \"fail\"\n}\n]\n},\n{\n\"objectType\": \"ReplicaSet\",\n\"id\": \"KSV014\",\n\"remediation\": \"\",\n\"details\": [\n{\n\"name\": \"replicaset-memcached-sample-6c765df685\",\n\"namespace\": \"default\",\n\"msg\": \"Container 'memcached' of ReplicaSet 'memcached-sample-6c765df685' should set 'securityContext.readOnlyRootFilesystem' to true\",\n\"status\": \"fail\"\n}\n]\n}\n]\n},\n{\n\"id\": \"3.0\",\n\"name\": \"Use CNI plugin that supports NetworkPolicy API\",\n\"checkResults\": [\n{\n\"objectType\": \"Node\",\n\"id\": \"5.3.1\",\n\"remediation\": \"If the CNI plugin in use does not support network policies, consideration should be given to\\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\\nin the Kubernetes cluster.\\n\",\n\"details\": [\n{\n\"name\": \"local-control-plane\",\n\"namespace\": \"\",\n\"msg\": \"\",\n\"status\": \"warn\"\n}\n]\n}\n]\n},\n{\n\"id\": \"6.0\",\n\"name\": \"Ensure kube config file permission\",\n\"checkResults\": [\n{\n\"objectType\": \"Node\",\n\"id\": \"4.1.3\",\n\"remediation\": \"Run the below command (based on the file location on your system) on the each worker node.\\nFor example,\\nchmod 644 /etc/kubernetes/proxy.conf\\n\",\n\"details\": [\n{\n\"name\": \"local-control-plane\",\n\"namespace\": \"\",\n\"msg\": \"\",\n\"status\": \"pass\"\n}\n]\n}\n]\n},\n{\n\"id\": \"6.0\",\n\"name\": \"Ensure kube config file permission\",\n\"checkResults\": [\n{\n\"objectType\": \"Node\",\n\"id\": \"4.1.4\",\n\"remediation\": \"Run the below command (based on the file location on your system) on the each worker node.\\nFor example, chown root:root /etc/kubernetes/proxy.conf\\n\",\n\"details\": [\n{\n\"name\": \"local-control-plane\",\n\"namespace\": \"\",\n\"msg\": \"\",\n\"status\": \"pass\"\n}\n]\n}\n]\n}\n]\n}\n}\n</code></pre>"},{"location":"docs/design/design_compliance_report/#the-crds","title":"The CRDs","text":""},{"location":"docs/design/design_compliance_report/#clustercompliancereport-crd","title":"ClusterComplianceReport CRD","text":"<ul> <li>a new CRD <code>clustercompliancereports.crd.yaml</code> will be added to include compliance check report</li> </ul> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\nname: clustercompliancereports.aquasecurity.github.io\nlabels:\napp.kubernetes.io/managed-by: trivy-operator\napp.kubernetes.io/version: \"0.16.1\"\nspec:\ngroup: aquasecurity.github.io\nscope: Cluster\nversions:\n- name: v1alpha1\nserved: true\nstorage: true\nadditionalPrinterColumns:\n- jsonPath: .metadata.creationTimestamp\ntype: date\nname: Age\ndescription: The age of the report\n- jsonPath: .status.summary.failCount\ntype: integer\nname: Fail\npriority: 1\ndescription: The number of checks that failed with Danger status\n- jsonPath: .status.summary.passCount\ntype: integer\nname: Pass\npriority: 1\ndescription: The number of checks that passed\nschema:\nopenAPIV3Schema:\ntype: object\nrequired:\n- apiVersion\n- kind\n- metadata\n- spec\nproperties:\napiVersion:\ntype: string\nkind:\ntype: string\nmetadata:\ntype: object\nspec:\ntype: object\nrequired:\n- name\n- description\n- version\n- cron\n- controls\nproperties:\nname:\ntype: string\ndescription:\ntype: string\nversion:\ntype: string\ncron:\ntype: string\npattern: '^(((([\\*]{1}){1})|((\\*\\/){0,1}(([0-9]{1}){1}|(([1-5]{1}){1}([0-9]{1}){1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([0-9]{1}){1}|(([1]{1}){1}([0-9]{1}){1}){1}|([2]{1}){1}([0-3]{1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([1-9]{1}){1}|(([1-2]{1}){1}([0-9]{1}){1}){1}|([3]{1}){1}([0-1]{1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([1-9]{1}){1}|(([1-2]{1}){1}([0-9]{1}){1}){1}|([3]{1}){1}([0-1]{1}){1}))|(jan|feb|mar|apr|may|jun|jul|aug|sep|okt|nov|dec)) ((([\\*]{1}){1})|((\\*\\/){0,1}(([0-7]{1}){1}))|(sun|mon|tue|wed|thu|fri|sat)))$'\ndescription: 'cron define the intervals for report generation'\ncontrols:\ntype: array\nitems:\ntype: object\nrequired:\n- name\n- id\n- kinds\n- mapping\n- severity\nproperties:\nname:\ntype: string\ndescription:\ntype: string\nid:\ntype: string\ndescription: 'id define the control check id'\nkinds:\ntype: array\nitems:\ntype: string\ndescription: 'kinds define the list of kinds control check apply on , example: Node,Workload '\nmapping:\ntype: object\nrequired:\n- scanner\n- checks\nproperties:\nscanner:\ntype: string\npattern: '^config-audit$|^kube-bench$'\ndescription: 'scanner define the name of the scanner which produce data, currently only config-audit and kube-bench are supported'\nchecks:\ntype: array\nitems:\ntype: object\nrequired:\n- id\nproperties:\nid:\ntype: string\ndescription: 'id define the check id as produced by scanner'\nseverity:\ntype: string\ndescription: 'define the severity of the control'\nenum:\n- CRITICAL\n- HIGH\n- MEDIUM\n- LOW\n- UNKNOWN\nstatus:\nx-kubernetes-preserve-unknown-fields: true\ntype: object\nsubresources:\n# status enables the status subresource.\nstatus: { }\nnames:\nsingular: clustercompliancereport\nplural: clustercompliancereports\nkind: ClusterComplianceReport\nlistKind: ClusterComplianceReportList\ncategories: [ ]\nshortNames:\n- compliance\n</code></pre>"},{"location":"docs/design/design_compliance_report/#clustercompliancedetailreport-crd","title":"ClusterComplianceDetailReport CRD","text":"<ul> <li>a new CRD <code>clustercompliancedetailreports.crd.yaml</code> will be added to include compliance detail check report</li> </ul> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\nname: clustercompliancedetailreports.aquasecurity.github.io\nlabels:\napp.kubernetes.io/managed-by: trivy-operator\napp.kubernetes.io/version: \"0.16.1\"\nspec:\ngroup: aquasecurity.github.io\nversions:\n- name: v1alpha1\nserved: true\nstorage: true\nadditionalPrinterColumns:\n- jsonPath: .metadata.creationTimestamp\ntype: date\nname: Age\ndescription: The age of the report\n- jsonPath: .report.summary.failCount\ntype: integer\nname: Fail\npriority: 1\ndescription: The number of checks that failed with Danger status\n- jsonPath: .report.summary.passCount\ntype: integer\nname: Pass\npriority: 1\ndescription: The number of checks that passed\nschema:\nopenAPIV3Schema:\nx-kubernetes-preserve-unknown-fields: true\ntype: object\nscope: Cluster\nnames:\nsingular: clustercompliancedetailreport\nplural: clustercompliancedetailreports\nkind: ClusterComplianceDetailReport\nlistKind: ClusterComplianceDetailReportList\ncategories: []\nshortNames:\n- compliancedetail </code></pre>"},{"location":"docs/design/design_compliance_report/#permission-changes","title":"Permission changes","text":"<p>it is required to update <code>02-trivy-operator.rbac.yaml</code> rules to include new permissions to support the following tracked resources kind by NSA plugin with (get,list and watch):</p> <p>```yaml - apiGroups: [\"networking.k8s.io\"]   resources:     - networkpolicies   verbs:     - get     - list     - watch <pre><code>```yaml\n- apiGroups:\n      - \"\"\n    resources:\n      - resourcequota\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre></p>"},{"location":"docs/design/design_compliance_report/#nsa-tool-analysis","title":"NSA Tool Analysis","text":"Test Description Kind Tool Test Non-root containers Check that container is not running as root Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield : kubernetes/policies/pss/restricted/3_runs_as_root.rego Immutable container file systems check that container root file system is immutable Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/general/file_system_not_read_only.rego Scan container images vulnerabilities scan container for vulnerabilities and misconfiguration Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Trivy Trivy Privileged container Controls whether Pods can run privileged containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/2_privileged.rego hostIPC Controls whether containers can share host process namespaces Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_ipc.rego hostPID Controls whether containers can share host process namespaces. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_pid.rego hostNetwork Controls whether containers can use the host network. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_network.rego allowedHostPaths Limits containers to specific paths of the host file system. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest Need to be added to appshield : https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems runAsUser , runAsGroup and supplementalGroups Controls whether container applications can run with root privileges or with root group membership Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/4_runs_with_a_root_gid.rego allowPrivilegeEscalation Restricts escalation to root privileges. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/2_can_elevate_its_own_privileges.rego seLinux Sets the SELinux context of the container. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/7_selinux_custom_options_set.rego AppArmor annotations Sets the seccomp profile used to sandbox containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/6_apparmor_policy_disabled.rego seccomp annotations Sets the seccomp profile used to sandbox containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/5_runtime_default_seccomp_profile_not_set.rego Protecting Pod service account tokens disable secret token been mount ,automountServiceAccountToken: false Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/protecting_pod_service_account_tokens.rego kube-system or kube-public namespace kube-system should should not be used by users Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/protect_core_components_namespace.rego Use CNI plugin that supports NetworkPolicy API check cni plugin installed Node Kube-bench 5.3.1 Ensure that the CNI in use supports Network Policies (need to be fixed) Create policies that select Pods using podSelector and/or the namespaceSelector Create policies that select Pods using podSelector and/or the namespaceSelector Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/selector_usage_in_network_policies.rego use a default policy to deny all ingress and egress traffic check that network policy deny all exist NetworkPolicy Kube-bench Add logic to kube-bench https://kubernetes.io/docs/concepts/services-networking/network-policies/ Use LimitRange and ResourceQuota policies to limit resources on a namespace or Pod level check the resource quota resource has been define ResourceQuota Kube-bench Add Logic to kube-bench https://kubernetes.io/docs/concepts/policy/limit-range/ TLS encryption control plan disable insecure port Node Kube-bench 1.2.19 Ensure that the --insecure-port argument is set to 0 Etcd encryption encrypt etcd communication Node Kube-bench 2.1 Ensure that the --cert-file and --key-file arguments are set as appropriate Kubeconfig files ensure file permission Node Kube-bench 4.1.3, 4.1.4 Worker node segmentation node segmentation Node Kube-bench Note sure can be tested Encryption check that encryption resource has been set EncryptionConfiguration Kube-bench Add Logic to kube-bench https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ Encryption / secrets check encryption provider Node Kube-bench 1.2.3 Ensure that the --encryption-provider-config argument is set as authentication make sure anonymous-auth is unset Node Kube-bench 1.2.1 Ensure that the --anonymous-auth argument is set to false Role-based access control make sure -authorization-mode=RBAC Node Kube-bench 1.2.7/1.2.8 Ensure that the --authorization-mode argument is not set to AlwaysAllow Audit policy file check that policy is configure Node Kube-bench 3.2.1 Ensure that a minimal audit policy is created Audit log path check that log path is configure Node Kube-bench 1.2.22 Ensure that the --audit-log-path argument is set Audit log max age check audit log aging Node Kube-bench 1.2.23 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate service mesh usage check service mesh is used in cluster Node Kube-bench Add Logic to kube-bench check service mesh existence"},{"location":"docs/design/design_compliance_report/#open-items","title":"Open Items","text":"<ul> <li>compliance support for CLI</li> </ul>"},{"location":"docs/design/design_trivy_file_system_scanner/","title":"Scan Container Images with Trivy Filesystem Scanner","text":"<p>Authors: Devendra Turkar, Daniel Pacak</p>"},{"location":"docs/design/design_trivy_file_system_scanner/#overview","title":"Overview","text":"<p>Trivy-Operator currently uses Trivy in [Standalone] or [ClientServer] mode to scan and generate VulnerabilityReports for container images by pulling the images from remote registries. Trivy-Operator scans a specified K8s workload by running the Trivy executable as a K8s Job. This approach implies that Trivy does not have access to images cached by the container runtime on cluster nodes. Therefore, to scan images from private registries Trivy-Operator reads ImagePullSecrets specified on workloads or on service accounts used by the workloads, and passes them down to Trivy executable as <code>TRIVY_USERNAME</code> and <code>TRIVY_PASSWORD</code> environment variables.</p> <p>Since ImagePullSecrets are not the only way to provide registry credential, the following alternatives are not currently supported by Trivy-Operator: 1. Pre-pulled images 2. Configuring nodes to authenticate to a private registry 3. Vendor-specific or local extension. For example, methods described on AWS ECR Private registry authentication.</p> <p>Even though we could resolve some of above-mentioned limitations with hostPath volume mounts to the container runtime socket, it would have its own disadvantages that we are trying to avoid. For example, more permissions to schedule scan Jobs and additional information about cluster's infrastructure such as location of the container runtime socket. </p>"},{"location":"docs/design/design_trivy_file_system_scanner/#solution","title":"Solution","text":""},{"location":"docs/design/design_trivy_file_system_scanner/#tldr","title":"TL;DR;","text":"<p>Use Trivy filesystem scanning to scan container images. The main idea, which is discussed in this proposal, is to schedule a scan Job on the same cluster node where the scanned workload. This allows Trivy to scan a filesystem of the container image which is already cached on that node without pulling the image from a remote registry. What's more, Trivy will scan container images from private registries without providing registry credentials (as ImagePullSecret or in any other proprietary way).</p>"},{"location":"docs/design/design_trivy_file_system_scanner/#deep-dive","title":"Deep Dive","text":"<p>To scan a container image of a given K8s workload Trivy-Operator will create a corresponding container of a scan Job and override its entrypoint to invoke Trivy filesystem scanner.</p> <p>This approach requires Trivy executable to be downloaded and made available to the entrypoint. We'll do that by adding the init container to the scan Job. Such init container will use the Trivy container image to copy Trivy executable out to the emptyDir volume, which will be shared with the other containers.</p> <p>Another init container is required to download Trivy vulnerability database and save it to the mounted shared volume.</p> <p>Finally, the scan container will use shared volume with the Trivy executable and Trivy database to perform the actual filesystem scan. (See the provided Example to have a better idea of all containers defined by a scan Job and how they share data via the emptyDir volume.)</p> <p>Note that the second init container is required in [Standalone] mode, which is the only mode supported by Trivy filesystem scanner at the time of writing this proposal.</p> <p>We further restrict scan Jobs to run on the same node where scanned Pod is running and never pull images from remote registries by setting the <code>ImagePullPolicy</code> to <code>Never</code>. To determine the node for a scan Job Trivy-Operator will list active Pods controlled by the scanned workload. If the list is not empty it will take the node name from the first Pod, otherwise it will ignore the workload.</p>"},{"location":"docs/design/design_trivy_file_system_scanner/#example","title":"Example","text":"<p>Let's assume that there's the <code>nginx</code> Deployment in the <code>poc-ns</code> namespace. It runs the <code>example.registry.com/nginx:1.16</code> container image from a private registry <code>example.registry.com</code>. Registry credentials are stored in the <code>private-registry</code> ImagePullSecret. (Alternatively, ImagePullSecret can be attached to a service account referred to by the Deployment.)</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\nname: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: nginx\nname: nginx\nnamespace: poc-ns\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\nimagePullSecrets:\n- name: private-registry\ncontainers:\n- name: nginx\nimage: example.registry.com/nginx:1.16\n</code></pre> <p>To scan the <code>nginx</code> container of the <code>nginx</code> Deployment, Trivy-Operator will create the following scan Job in the <code>trivy-system</code> namespace and observe it until it's Completed or Failed.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: scan-vulnerabilityreport-ab3134\nnamespace: trivy-system\nspec:\nbackoffLimit: 0\ntemplate:\nspec:\nrestartPolicy: Never\n# Explicit nodeName indicates our intention to schedule a scan pod\n# on the same cluster node where the nginx workload is running.\n# This could also imply considering taints and tolerations and other\n# properties respected by K8s scheduler.\nnodeName: kind-control-plane\nvolumes:\n- name: scan-volume\nemptyDir: { }\ninitContainers:\n# The trivy-get-binary init container is used to copy out the trivy executable\n# binary from the upstream Trivy container image, i.e. aquasec/trivy:0.19.2,\n# to a shared emptyDir volume.\n- name: trivy-get-binary\nimage: aquasec/trivy:0.19.2\ncommand:\n- cp\n- -v\n- /usr/local/bin/trivy\n- /var/trivy-operator/trivy\nvolumeMounts:\n- name: scan-volume\nmountPath: /var/trivy-operator\n# The trivy-download-db container is using trivy executable binary\n# from the previous step to download Trivy vulnerability database\n# from GitHub releases page.\n# This won't be required once Trivy supports ClientServer mode\n# for the fs subcommand.\n- name: trivy-download-db\nimage: aquasec/trivy:0.19.2\ncommand:\n- /var/trivy-operator/trivy\n- --download-db-only\n- --cache-dir\n- /var/trivy-operator/trivy-db\nvolumeMounts:\n- name: scan-volume\nmountPath: /var/trivy-operator\ncontainers:\n# The nginx container is based on the container image that\n# we want to scan with Trivy. However, it has overwritten command (entrypoint)\n# to invoke trivy file system scan. The scan results are output to stdout\n# in JSON format, so we can parse them and store as VulnerabilityReport.\n- name: nginx\nimage: example.registry.com/nginx:1.16\n# To scan image layers cached on a cluster node without pulling\n# it from a remote registry.\nimagePullPolicy: Never\nsecurityContext:\n# Trivy must run as root, so we set UID here.\nrunAsUser: 0\ncommand:\n- /var/trivy-operator/trivy\n- --cache-dir\n- /var/trivy-operator/trivy-db\n- fs\n- --format\n- json\n- /\nvolumeMounts:\n- name: scan-volume\nmountPath: /var/trivy-operator\n</code></pre> <p>Notice that the scan Job does not use registry credentials stored in the <code>private-registry</code> ImagePullSecret at all. Also, the <code>ImagePullPolicy</code> for the <code>nginx</code> container is set to <code>Never</code> to avoid pulling the image from the <code>example.registry.com/nginx</code> repository that requires authentication. And finally, the <code>nodeName</code> property is explicitly set to <code>kube-control-plane</code> to make sure that the scan Job is scheduled on the same node as a Pod controlled by the <code>nginx</code> Deployment. (We assumed that there was at least one Pod controlled by the <code>nginx</code> Deployment, and it was scheduled on the <code>kube-control-plane</code> node.)</p> <p>Trivy must run as root so the scan Job defined the <code>securityContext</code> with the <code>runAsUser</code> property set to <code>0</code> UID.</p>"},{"location":"docs/design/design_trivy_file_system_scanner/#remarks","title":"Remarks","text":"<ol> <li>The proposed solution won't work with the AlwaysPullImages admission controller, which might be enabled in    a multitenant cluster so that users can be assured that their private images can only be used by those who    have the credentials to pull them. (Thanks kfox1111 for pointing this out!)</li> <li>We cannot scan K8s workloads scaled down to 0 replicas because we cannot infer on which cluster node a scan Job should    run. (In general, a node name is only set on a running Pod.) But once a workload is scaled up, Trivy Operator    will receive the update event and will have another chance to scan it.</li> <li>It's hard to identify Pods managed by the CronJob controller, therefore we'll skip them.</li> <li>Trivy filesystem command does not work in [ClientServer] mode. Therefore, this solution is subject to the limits of    the [Standalone] mode. We plan to extend Trivy filesystem command to work in ClientServer mode and improve the    implementation of Trivy Operator once it's available.</li> <li>Trivy must run as root and this may be blocked by some Admission Controllers such as PodSecurityPolicy.</li> </ol>"},{"location":"docs/design/design_vuln_scan_job_in_same_namespace_of_workload/","title":"Run Vulnerability Scan Job in Same namespace of workload","text":""},{"location":"docs/design/design_vuln_scan_job_in_same_namespace_of_workload/#overview","title":"Overview","text":"<p>When user runs a workload with private managed registry image(eg. image from ECR, ACR) and user is not using ImagePullSecret method to provide access to registry, then trivy operator has challenges to scan such workloads.</p> <ul> <li>Consider an example of ECR registry, there is one option available in which that user can associate IAM role to service account,  then workloads which are associated with this service account will get authorised to run with the image from that registry.  If user wants to get these images scanned using Trivy operator then currently we have only one way to do that.  User has to associate IAM role to trivy-operator service account, so with when scan job run with <code>trivy-operator</code>service  account, then Trivy will get appropriate permission to pull the image. To know more on how this mechanism works, please  refer to the documents ECR registry configuration, IAM role to service account, but, trivy cannot use permission  set on service account of workload.  </li> </ul> <p>Recently, there is one option added in Trivy plugin with Trivy fs command, In which Trivy scans the image which is cached on a node. And to do that scan job is scheduled on same node where workload is running, so that Trivy can use a cached image from a node. But, if we want to schedule these scan job on any node, then currently we dont have option to do that, coz image might not be available on that node. Also, trivy cannot attach imagePullSecret available on the workload pull the image. We also thought that when we have ImagePullSecret available on a workload, then we can use existing option of Trivy image scan with which we can scan workload. To do that, trivy operator creates another secret from existing ImagePullSecret so that registry credentials are provided to Trivy as Env var. But again, we cannot reuse the same ImagePullSecret available on the workload.</p>"},{"location":"docs/design/design_vuln_scan_job_in_same_namespace_of_workload/#solution","title":"Solution","text":"<p>Consider there is an option given to enable running vulnerability scan jobs in the same namespace of workload. Operator detects it, so it can schedule and monitor scan jobs in same namespace where workload is running. And plugins will act accordingly to utilize the service account and ImagePullSecret available on the workload.</p>"},{"location":"docs/design/design_vuln_scan_job_in_same_namespace_of_workload/#example","title":"Example","text":""},{"location":"docs/design/design_vuln_scan_job_in_same_namespace_of_workload/#example-1","title":"Example 1","text":"<p>Consider trivy operator is running with Trivy image scan mode. And let's assume that there is an <code>nginx</code> deployment in <code>poc-ns</code> namespace. It is running with image <code>12344534.dkr.ecr.us-west-2.amazonaws.com/amazon/nginx:1.16</code>. This deployment is running with service account <code>poc-sa</code>, which is annotated with ARN: <code>arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/IAM_ROLE_NAME</code></p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\nname: poc-ns\n---\napiVersion: v1\nautomountServiceAccountToken: true\nkind: ServiceAccount\nmetadata:\nannotations:\neks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/IAM_ROLE_NAME\nname: poc-sa\nnamespace: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: nginx\nname: nginx\nnamespace: poc-ns\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\nserviceAccountName: poc-sa\ncontainers:\n- name: nginx\nimage: 12344534.dkr.ecr.us-west-2.amazonaws.com/amazon/nginx:1.16\n</code></pre> <p>When a pod(<code>nginx-65b78bbbd4-nb5kl</code>) comes into running state from above deployment then pod will have these env var to get access to ECR registry: <code>AWS_REGION</code>, <code>AWS_ROLE_ARN</code>, <code>AWS_WEB_IDENTITY_TOKEN_FILE</code></p> <p>To scan the <code>nginx</code> deployment, trivy-operator create following scan job in <code>poc-ns</code> namespace. And trivy-operator will monitor this job, and it will parse the result based on completion state of job. This job will run with same service account(<code>poc-sa</code>) of workload.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: scan-vulnerabilityreport-ab3134\nnamespace: poc-ns\nspec:\nbackoffLimit: 0\ntemplate:\nspec:\nserviceAccountName: poc-sa\nrestartPolicy: Never\ncontainers:\n# containers from pod spec returned from existing Trivy plugin\n</code></pre> <p>When a pod(<code>scan-vulnerabilityreport-ab3134-nfkst</code>) gets created from above job spec, then that pod will get injected with these env var which will help scanner to get access to registry image: <code>AWS_REGION</code>, <code>AWS_ROLE_ARN</code>, <code>AWS_WEB_IDENTITY_TOKEN_FILE</code></p> <p>Pod will get injected with respective env vars to get access to registry image and Trivy scanner will use these credentials to pull an image for scanning.</p>"},{"location":"docs/design/design_vuln_scan_job_in_same_namespace_of_workload/#example-2","title":"Example 2","text":"<p>Consider another example, in which we want to perform vulnerability scan using Trivy <code>fs</code> command. Deployment <code>demo-nginx</code> is running in <code>poc-ns</code> namespace. This deployment is running with image <code>example.registry.com/nginx:1.16</code> from private registry <code>example.registry.com</code>. Registry credentials are stored in ImagePullSecret <code>private-registry</code>.</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\nname: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: demo-nginx\nname: demo-nginx\nnamespace: poc-ns\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\nimagePullSecrets:\n- name: private-registry\ncontainers:\n- name: nginx\nimage: example.registry.com/nginx:1.16\n</code></pre> <p>To scan the <code>demo-nginx</code> deployment, trivy-operator create following scan job in <code>poc-ns</code> namespace. And trivy-operator will monitor job, and it will parse the result based on completion state of job.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: scan-vulnerabilityreport-ab3134\nnamespace: poc-ns\nspec:\nbackoffLimit: 0\ntemplate:\nspec:\n# ImagePullSecret value will be copied from workload which we are scanning\nimagePullSecrets:\n- name: private-registry\nrestartPolicy: Never\nvolumes:\n- name: scan-volume\nemptyDir: { }\ninitContainers:\n- name: trivy-get-binary\nimage: aquasec/trivy:0.19.2\ncommand:\n- cp\n- -v\n- /usr/local/bin/trivy\n- /var/trivy-operator/trivy\nvolumeMounts:\n- name: scan-volume\nmountPath: /var/trivy-operator\n- name: trivy-download-db\nimage: aquasec/trivy:0.19.2\ncommand:\n- /var/trivy-operator/trivy\n- --download-db-only\n- --cache-dir\n- /var/trivy-operator/trivy-db\nvolumeMounts:\n- name: scan-volume\nmountPath: /var/trivy-operator\ncontainers:\n- name: nginx\nimage: example.registry.com/nginx:1.16\nimagePullPolicy: IfNotPresent\nsecurityContext:\n# Trivy must run as root, so we set UID here.\nrunAsUser: 0\ncommand:\n- /var/trivy-operator/trivy\n- --cache-dir\n- /var/trivy-operator/trivy-db\n- fs\n- --format\n- json\n- /\nvolumeMounts:\n- name: scan-volume\nmountPath: /var/trivy-operator\n</code></pre> <p>If you observe in the job spec, this scan job will run in <code>poc-ns</code> namespace and it is running with image <code>example.registry.com/nginx:1.16</code>. It is using ImagePullSecret <code>private-registry</code> which is available in same namespace. With this approach trivy operator will not have to worry about managing(create/delete) of secret required for scanning.</p>"},{"location":"docs/design/design_vuln_scan_job_in_same_namespace_of_workload/#notes","title":"Notes","text":"<ol> <li>There are some points to consider before using this option<ul> <li>Scan jobs will run in different namespaces. This will create some activity in each namespace available in the cluster. If we dont use this option then all scan jobs will only run in <code>trivy-operator</code> namespace, and user can see all activity confined to single namespace i.e <code>trivy-operator</code>.</li> <li>As we will run scan job with service account of workload and if there are some very strict PSP defined in the cluster then scan job will be blocked due to the PSP.</li> </ul> </li> </ol>"},{"location":"docs/design/ttl_scans/","title":"TTL scans","text":""},{"location":"docs/design/ttl_scans/#summary","title":"Summary","text":"<p>Add an option to automatically delete old security reports. In this first version focus on vulnerability reports but in the long run we could add similar functionality to other reports as well.</p>"},{"location":"docs/design/ttl_scans/#motivation","title":"Motivation","text":"<p>In 537 we talk about a need to run nightly vulnerability scans of CVE:s. This way we can make sure to get new CVE reports for long time running pods as well.</p>"},{"location":"docs/design/ttl_scans/#proposal","title":"Proposal","text":"<p>Add a environment variable to the operator, for example <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL=86400</code>, this way we can add other ttl values for other reports as well. Adding this environment variable would add a annotation to the generated VulnerabilityReport that we can look for.</p> <p>Create a new controller that looks for changes in vulnerabilityreports and uses RequeueAfter calculated from the TTL annotation. At startup the operator will look through all existing vulnerabilityreports and delete existing ones where TTL have expired. If the TTL haven't expired the the vulnerabilityreports will be requeued and automatically checked again when the TTL have expired.</p> <p>We could calculate the ttl without having creating a new annotation to the reports but the verbosity of showing the users how long each report got a ttl outweighs the \"issue\" of generating a new annotation.</p>"},{"location":"docs/design/ttl_scans/#example","title":"Example","text":"<p>Below you can see a shortened version of the yaml. Notice the <code>metadata.annotations.trivy-operator.aquasecurity.github.io/report-ttl</code> which is new. The operator would automatically apply the <code>trivy-operator.aquasecurity.github.io/report-ttl</code> annotation to all new reports that it generates assuming that the environment variable is set. In theory users could also extend the TTL manually for a specific report by changing the trivy-operator.aquasecurity.github.io/report-ttl annotation per VulnerabilityReport.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  creationTimestamp: \"2021-12-08T12:03:48Z\"\n  annotations:\n    trivy-operator.aquasecurity.github.io/report-ttl: 24h\n  labels:\n    resource-spec-hash: 86b58dcb99\n    trivy-operator.container.name: manager\n    trivy-operator.resource.kind: ReplicaSet\n    trivy-operator.resource.name: source-controller-b5d5cfdf4\n    trivy-operator.resource.namespace: flux-system\n  name: replicaset-source-controller-b5d5cfdf4-manager\nreport:\n  artifact:\n    repository: fluxcd/source-controller\n    tag: v0.16.1\n  registry:\n    server: ghcr.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.19.2\n  summary:\n    criticalCount: 0\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n    unknownCount: 0\n  updateTimestamp: \"2021-12-08T12:03:48Z\"\n  vulnerabilities: []\n</code></pre> <p>Another option is to define a new report entry, the positive thing with that is that we can define the input type, in our case a <code>time.Duration</code>.</p>"},{"location":"docs/design/ttl_scans/#alternatives","title":"Alternatives","text":"<p>Another \"simpler\" option could be to add the same environment variable <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL=2h</code> but instead of using RequeueAfter in the controller we could create a cronjob/job that runs once an hour and check for the same annotation.</p> <p>The bad thing hear is that we would have to manage yet another cronjob/job. We would also have to mange a new binary feature flag to run in cronjob cleanup mode. It would also trigger removal of multiple reports at the same time, compared to the event driven solution that would be much more precise per report and thus spreading out the new reports more.</p> <p>But the good thing is that everyone knows how jobs/cronjobs works especially since it's already well used within the trivy-operator operator.</p>"},{"location":"docs/vulnerability-scanning/","title":"Vulnerability Scanners","text":"<p>Vulnerability scanning is an important way to identify and remediate security gaps in Kubernetes workloads. The process involves scanning container images to check all software on them and report any vulnerabilities found.</p> <p>Trivy Operator automatically discovers and scans all images that are being used in a Kubernetes cluster, including images of application pods and system pods. Scan reports are saved as [VulnerabilityReport] resources, which are owned by a Kubernetes controller.</p> <p>For example, when Trivy scans a Deployment, the corresponding VulnerabilityReport instance is attached to its current revision. In other words, the VulnerabilityReport inherits the life cycle of the Kubernetes controller. This also implies that when a Deployment is rolling updated, it will get scanned automatically, and a new instance of the VulnerabilityReport will be created and attached to the new revision. On the other hand, if the previous revision is deleted, the corresponding VulnerabilityReport will be deleted automatically by the Kubernetes garbage collector.</p> <p>Trivy may scan Kubernetes workloads that run images from Private Registries and certain Managed Registries.</p>"},{"location":"docs/vulnerability-scanning/managed-registries/","title":"Managed Registries","text":""},{"location":"docs/vulnerability-scanning/managed-registries/#amazon-elastic-container-registry-ecr","title":"Amazon Elastic Container Registry (ECR)","text":"<p>You must create an IAM OIDC identity provider for your cluster:</p> <pre><code>eksctl utils associate-iam-oidc-provider \\\n  --cluster &lt;cluster_name&gt; \\\n  --approve\n</code></pre> <p>Override the existing <code>trivy-operator</code> service account and attach the IAM policy to grant it permission to pull images from the ECR:</p> <pre><code>eksctl create iamserviceaccount \\\n  --name trivy-operator \\\n  --namespace trivy-system \\\n  --cluster &lt;cluster_name&gt; \\\n  --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\\n  --approve \\\n  --override-existing-serviceaccounts\n</code></pre>"},{"location":"docs/vulnerability-scanning/managed-registries/#azure-container-registry-acr-workload-identity-support","title":"Azure Container Registry (ACR) - Workload Identity support","text":"<p>Please make sure the following is set up before using operator User steps to setup an AKS cluster and delegate access to specific private ACR The official steps for setting up Workload Identity on AKS can be found here.</p> <ul> <li>Managed clusters or self-managed clusters installed, see docs</li> <li> <p>Mutating admission webhook installed, see docs</p> </li> <li> <p>update trivy-operator service accout to include workload identity annotation and lables (update clientID and tenantID), Example:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: trivy-operator\nnamespace: trivy-system\nlabels:\napp.kubernetes.io/name: trivy-operator\napp.kubernetes.io/instance: trivy-operator\napp.kubernetes.io/version: \"0.16.1\"\napp.kubernetes.io/managed-by: kubectl\nazure.workload.identity/use: \"true\"\nannotations:\nazure.workload.identity/client-id: \"client-id\"\nazure.workload.identity/tenant-id: \"tenant-id\"\n</code></pre> <ul> <li>Add the following label to podTemplateLabels in helm settings : <code>scanJob.podTemplateLabels=azure.workload.identity/use=true</code></li> </ul>"},{"location":"docs/vulnerability-scanning/managed-registries/#google-container-registry-gcr","title":"Google Container Registry (GCR)","text":"<p>Create an IAM service account for your application or use an existing IAM service account instead. You can use any IAM service account in any project in your organization. For Config Connector, apply the IAMServiceAccount object for your selected service account.</p> <pre><code>apiVersion: iam.cnrm.cloud.google.com/v1beta1\nkind: IAMServiceAccount\nmetadata:\nname: trivy-operator-gsa\nspec:\ndisplayName: trivy-operator-google-service-account\n</code></pre> <p>Ensure that your IAM service account has the roles you need. You can grant additional roles using the following command:</p> <pre><code>gcloud projects add-iam-policy-binding &lt;PROJECT_ID&gt; \\\n--member \"serviceAccount: trivy-operator-gsa@&lt;GSA_PROJECT&gt;.iam.gserviceaccount.com\" \\\n--role &lt;ROLE_NAME&gt;\n</code></pre> <p>Allow the Kubernetes service account to impersonate the IAM service account by adding an IAM policy binding between the two service accounts. This binding allows the Kubernetes service account to act as the IAM service account.</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding  trivy-operator-gsa@&lt;GSA_PROJECT&gt;.iam.gserviceaccount.com \\\n--role roles/iam.workloadIdentityUser \\\n--member \"serviceAccount:&lt;PROJECT_ID&gt;.svc.id.goog[trivy-system/trivy-operator]\"\n</code></pre> <p>Annotate the Kubernetes service account with the email address of the IAM service account.</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nannotations:\niam.gke.io/gcp-service-account: trivy-operator-gsa@&lt;PROJECT_ID&gt;.iam.gserviceaccount.com\nname: trivy-operator\nnamespace: trivy-system\n</code></pre> <p>Update your Pod spec to schedule the workloads on nodes that use Workload Identity and to use the annotated Kubernetes service account.</p> <pre><code>spec:\nserviceAccountName: trivy-operator-service\nnodeSelector:\niam.gke.io/gke-metadata-server-enabled: \"true\"\n</code></pre> <p>Replace the following :</p> <ul> <li>PROJECT_ID: your Google Cloud project ID.</li> <li>GSA_PROJECT: the project ID of the Google Cloud project of your IAM service account.</li> <li>ROLE_NAME: the IAM role to assign to your service account, like roles/spanner.viewer.</li> </ul> <p>Use Workload Identity Referance</p>"},{"location":"docs/vulnerability-scanning/private-registries/","title":"Private Registries","text":""},{"location":"docs/vulnerability-scanning/private-registries/#image-pull-secrets","title":"Image Pull Secrets","text":"<ol> <li>Find references to image pull secrets (direct references and via service account).</li> <li>Create the temporary secret with basic credentials for each container of the scanned workload.</li> <li>Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job.</li> <li>Watch the job until it's completed or failed.</li> <li>Parse logs and save vulnerability reports in etcd.</li> <li>Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.</li> </ol>"},{"location":"docs/vulnerability-scanning/trivy/","title":"Vulnerability Scanning Configuration","text":""},{"location":"docs/vulnerability-scanning/trivy/#standalone","title":"Standalone","text":"<p>The default configuration settings enable Trivy <code>vulnerabilityReports.scanner</code> in <code>Standalone</code> <code>trivy.mode</code>. Even though it doesn't require any additional setup, it's the least efficient method. Each Pod created by a scan Job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of the emptyDir volume. This volume is then shared with containers that perform the actual scanning. Finally, the Pod is deleted along with the emptyDir volume.</p> <p></p> <p>The number of containers defined by a scan Job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers.</p> <p>Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the <code>trivy.githubToken</code> key to the <code>trivy-operator</code> secret.</p> <pre><code>kubectl patch secret trivy-operator-trivy-config -n trivy-system \\\n--type merge \\\n-p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n &lt;GITHUB_TOKEN&gt; | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre>"},{"location":"docs/vulnerability-scanning/trivy/#clientserver","title":"ClientServer","text":"<p>You can connect Trivy to an external Trivy server by changing the default <code>trivy.mode</code> from <code>Standalone</code> to <code>ClientServer</code> and specifying <code>trivy.serverURL</code>.</p> <pre><code>kubectl patch cm trivy-operator-trivy-config -n trivy-system \\\n--type merge \\\n-p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.mode\":      \"ClientServer\",\n    \"trivy.serverURL\": \"&lt;TRIVY_SERVER_URL&gt;\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>The Trivy server could be your own deployment, or it could be an external service. See Trivy server documentation for more information.</p> <p>If the server requires access token and/or custom HTTP authentication headers, you may add <code>trivy.serverToken</code> and <code>trivy.serverCustomHeaders</code> properties to the Trivy Operator secret.</p> <pre><code>kubectl patch secret trivy-operator-trivy-config -n trivy-system \\\n--type merge \\\n-p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.serverToken\":         \"$(echo -n &lt;SERVER_TOKEN&gt; | base64)\",\n    \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:&lt;X_API_TOKEN&gt; | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre> <p></p>"},{"location":"docs/vulnerability-scanning/trivy/#settings","title":"Settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION <code>trivy.repository</code> <code>ghcr.io/aquasecurity/trivy</code> Repository of the Trivy image <code>trivy.tag</code> <code>0.36.0</code> Version of the Trivy image <code>trivy.imagePullSecret</code> N/A imagePullSecret is the secret name to be used when pulling trivy image from private registries example: <code>reg-secret</code>. It is the user responsibility to create the secret for the private registry in <code>trivy-operator</code> namespace. <code>trivy.dbRepository</code> <code>ghcr.io/aquasecurity/trivy-db</code> External OCI Registry to download the vulnerability database <code>trivy.javaDbRepository</code> <code>ghcr.io/aquasecurity/trivy-java-db</code> External OCI Registry to download the vulnerability database for Java <code>trivy.dbRepositoryInsecure</code> <code>false</code> The Flag to enable insecure connection for downloading trivy-db via proxy (air-gaped env) <code>trivy.mode</code> <code>Standalone</code> Trivy client mode. Either <code>Standalone</code> or <code>ClientServer</code>. Depending on the active mode other settings might be applicable or required. <code>additionalVulnerabilityReportFields</code> N/A A comma separated list of additional fields which can be added to the VulnerabilityReport. Possible values: <code>Description,Links,CVSS,Target,Class,PackagePath,PackageType</code>. Description will add more data about vulnerability. Links - all the references to a specific vulnerability. CVSS - data about CVSSv2/CVSSv3 scoring and vectors. Target - vulnerable element. Class - OS or library vulnerability <code>trivy.command</code> <code>image</code> command. One of <code>image</code>, <code>filesystem</code> or <code>rootfs</code> scanning. Depending on the target type required for the scan. <code>trivy.slow</code> <code>true</code> this flag is to use less CPU/memory for scanning though it takes more time than normal scanning. It fits small-footprint <code>trivy.severity</code> <code>UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL</code> A comma separated list of severity levels reported by Trivy <code>trivy.ignoreUnfixed</code> N/A Whether to show only fixed vulnerabilities in vulnerabilities reported by Trivy. Set to <code>\"true\"</code> to enable it. <code>trivy.vulnType</code> `` this flag can be used to tell Trivy to filter vulnerabilities by a pkg-type (library, os) <code>trivy.offlineScan</code> N/A Whether to enable the offline scan mode of Trivy preventing outgoing calls, e.g. to  for additional vulnerability information. Set to <code>\"true\"</code> to enable it. <code>trivy.skipFiles</code> N/A A comma separated list of file paths for Trivy to skip traversal. <code>trivy.skipDirs</code> N/A A comma separated list of directories for Trivy to skip traversal. <code>trivy.ignoreFile</code> N/A It specifies the <code>.trivyignore</code> file which contains a list of vulnerability IDs to be ignored from vulnerabilities reported by Trivy. <code>trivy.ignorePolicy</code> N/A It specifies a fallback policy file which allows to customize which vulnerabilities are reported by Trivy. <code>trivy.ignorePolicy.{ns}</code> N/A It specifies a namespace specific policy file which allows to customize which vulnerabilities are reported by Trivy. <code>trivy.ignorePolicy.{ns}.{wl}</code> N/A It specifies a namespace/workload specific policy file which allows to customize which vulnerabilities are reported by Trivy. <code>trivy.timeout</code> <code>5m0s</code> The duration to wait for scan completion <code>trivy.serverURL</code> N/A The endpoint URL of the Trivy server. Required in <code>ClientServer</code> mode. <code>node.collector.imageRef</code> ghcr.io/aquasecurity/node-collector:0.0.6 The imageRef use for node-collector job . <code>node.collector.imagePullSecret</code> N/A imagePullSecret is the secret name to be used when pulling trivy node-collector from private registries . <code>trivy.serverTokenHeader</code> <code>Trivy-Token</code> The name of the HTTP header to send the authentication token to Trivy server. Only application in <code>ClientServer</code> mode when <code>trivy.serverToken</code> is specified. <code>trivy.serverInsecure</code> N/A The Flag to enable insecure connection to the Trivy server. <code>trivy.insecureRegistry.&lt;id&gt;</code> N/A The registry to which insecure connections are allowed. There can be multiple registries with different registry <code>&lt;id&gt;</code>. <code>trivy.nonSslRegistry.&lt;id&gt;</code> N/A A registry without SSL. There can be multiple registries with different registry <code>&lt;id&gt;</code>. <code>trivy.sslCertDir</code> N/A sslCertDir can be used to override the system default locations for SSL certificate files directory , example: /ssl/certs <code>trivy.registry.mirror.&lt;registry&gt;</code> N/A Mirror for the registry <code>&lt;registry&gt;</code>, e.g. <code>trivy.registry.mirror.index.docker.io: mirror.io</code> would use <code>mirror.io</code> to get images originated from <code>index.docker.io</code> <code>trivy.httpProxy</code> N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub. <code>trivy.httpsProxy</code> N/A The HTTPS proxy used by Trivy to download the vulnerabilities database from GitHub. <code>trivy.noProxy</code> N/A A comma separated list of IPs and domain names that are not subject to proxy settings. <code>trivy.resources.requests.cpu</code> <code>100m</code> The minimum amount of CPU required to run Trivy scanner pod. <code>trivy.resources.requests.memory</code> <code>100M</code> The minimum amount of memory required to run Trivy scanner pod. <code>trivy.resources.requests.ephemeral-storage</code> `` The minimum amount of ephemeral-storage required to run Trivy scanner pod. <code>trivy.resources.limits.cpu</code> <code>500m</code> The maximum amount of CPU allowed to run Trivy scanner pod. <code>trivy.resources.limits.memory</code> <code>500M</code> The maximum amount of memory allowed to run Trivy scanner pod. <code>trivy.resources.limits.ephemeral-storage</code> `` The maximum amount of ephemeral-storage allowed to run Trivy scanner pod. <code>trivy.storageClassName</code> `` The name of the storage class to be used for Trivy server PVC. <code>trivy.podLabels</code> `` The extra pod labels to be used for Trivy server. <code>trivy.priorityClassName</code> `` PriorityClassName is the name of the priority class used for trivy server. <code>trivy.server.resources.requests.cpu</code> <code>200m</code> The minimum amount of CPU required to run trivy server. <code>trivy.server.resources.requests.memory</code> <code>512Mi</code> The minimum amount of memory required to run trivy server. <code>trivy.server.resources.limits.cpu</code> <code>1</code> The maximum amount of CPU allowed to run trivy server. <code>trivy.server.resources.limits.memory</code> <code>1Gi</code> The maximum amount of memory allowed to run trivy server. SECRET KEY DESCRIPTION <code>trivy.githubToken</code> The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in <code>Standalone</code> mode. <code>trivy.serverToken</code> The token to authenticate Trivy client with Trivy server. Only applicable in <code>ClientServer</code> mode. <code>trivy.serverCustomHeaders</code> A comma separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in <code>ClientServer</code> mode."},{"location":"getting-started/quick-start/","title":"Quick Start","text":""},{"location":"getting-started/quick-start/#before-you-begin","title":"Before you Begin","text":"<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube, kind or microk8s, or you can use the following Kubernetes playground.</p> <p>You also need the Trivy-Operator to be installed in the <code>trivy-system</code> namespace, e.g. with kubectl or Helm. Let's also assume that the operator is configured to discover built-in Kubernetes resources in all namespaces, except <code>kube-system</code> and <code>trivy-system</code>.</p>"},{"location":"getting-started/quick-start/#workloads-scanning","title":"Workloads Scanning","text":"<p>Let's create the <code>nginx</code> Deployment that we know is vulnerable:</p> <pre><code>kubectl create deployment nginx --image nginx:1.16\n</code></pre> <p>When the <code>nginx</code> Deployment is created, the operator immediately detects its current revision (aka active ReplicaSet) and scans the <code>nginx:1.16</code> image for vulnerabilities. It also audits the ReplicaSet's specification for common pitfalls such as running the <code>nginx</code> container as root.</p> <p>If everything goes fine, the operator saves scan reports as VulnerabilityReport and ConfigAuditReport resources in the <code>default</code> namespace. Reports are named after the scanned ReplicaSet. For image vulnerability scans, the operator creates a VulnerabilityReport for each different container. In this example there is just one container image called <code>nginx</code>:</p> <pre><code>kubectl get vulnerabilityreports -o wide\n</code></pre> Result <pre><code>NAME                                REPOSITORY      TAG    SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\nreplicaset-nginx-78449c65d4-nginx   library/nginx   1.16   Trivy     85s   33         62     49       114   1\n</code></pre> <pre><code>kubectl get configauditreports -o wide\n</code></pre> Result <pre><code>NAME                          SCANNER     AGE    CRITICAL  HIGH   MEDIUM   LOW\nreplicaset-nginx-78449c65d4   Trivy       2m7s      0         0      6        7\n</code></pre> <p>Notice that scan reports generated by the operator are controlled by Kubernetes workloads. In our example, VulnerabilityReport and ConfigAuditReport resources are controlled by the active ReplicaSet of the <code>nginx</code> Deployment:</p> <pre><code>kubectl tree deploy nginx\n</code></pre> Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h2m\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h2m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              2m31s\ndefault      \u251c\u2500Pod/nginx-78449c65d4-5wvdx                             True           7h2m\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              2m7s\n</code></pre> <p>Note</p> <p>The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree.</p> <p>Moving forward, let's update the container image of the <code>nginx</code> Deployment from <code>nginx:1.16</code> to <code>nginx:1.17</code>. This will trigger a rolling update of the Deployment and eventually create another ReplicaSet.</p> <pre><code>kubectl set image deployment nginx nginx=nginx:1.17\n</code></pre> <p>Even this time the operator will pick up changes and rescan our Deployment with updated configuration:</p> <pre><code>kubectl tree deploy nginx\n</code></pre> Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h5m\ndefault    \u251c\u2500ReplicaSet/nginx-5fbc65fff                               -              2m36s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-5fbc65fff           -              2m36s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-j7zl2                              True           2m36s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-5fbc65fff-nginx   -              2m22s\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h5m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              5m46s\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              5m22s\n</code></pre> <p>By following this guide you could realize that the operator knows how to attach VulnerabilityReport and ConfigAuditReport resources to build-in Kubernetes objects. What's more, in this approach where a custom resource inherits a life cycle of the built-in resource we could leverage Kubernetes garbage collection. For example, when the previous ReplicaSet named <code>nginx-78449c65d4</code> is deleted the VulnerabilityReport named <code>replicaset-nginx-78449c65d4-nginx</code> as well as the ConfigAuditReport named <code>replicaset-nginx-78449c65d46</code> are automatically garbage collected.</p> <p>Tip</p> <p>If you do not want only the latest ReplicaSet in your Deployment to be scanned for vulnerabilities, you can set the value of the <code>OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> environment variable to <code>false</code> in the operator's deployment descriptor.</p> <p>Tip</p> <p>If you do not want only the latest ReplicaSet in your Deployment to be scanned for config audit, you can set the value of the <code>OPERATOR_CONFIG_AUDIT_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> environment variable to <code>false</code> in the operator's deployment descriptor.</p> <p>Tip</p> <p>You can get and describe <code>vulnerabilityreports</code> and <code>configauditreports</code> as built-in Kubernetes objects: <pre><code>kubectl get vulnerabilityreport replicaset-nginx-5fbc65fff-nginx -o json\nkubectl describe configauditreport replicaset-nginx-5fbc65fff\n</code></pre></p> <p>Notice that scaling up the <code>nginx</code> Deployment will not schedule new scans because all replica Pods refer to the same Pod template defined by the <code>nginx-5fbc65fff</code> ReplicaSet.</p> <pre><code>kubectl scale deploy nginx --replicas 3\n</code></pre> <pre><code>kubectl tree deploy nginx\n</code></pre> Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h6m\ndefault    \u251c\u2500ReplicaSet/nginx-5fbc65fff                               -              4m7s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-5fbc65fff           -              4m7s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-458n7                              True           8s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-fk847                              True           8s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-j7zl2                              True           4m7s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-5fbc65fff-nginx   -              3m53s\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h6m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              7m17s\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              6m53s\n</code></pre> <p>Finally, when you delete the <code>nginx</code> Deployment, orphaned security reports will be deleted in the background by the Kubernetes garbage collection controller.</p> <pre><code>kubectl delete deploy nginx\n</code></pre> <pre><code>kubectl get vuln,configaudit\n</code></pre> Result <pre><code>No resources found in default namespace.\n</code></pre> <p>Tip</p> <p>Use <code>vuln</code> and <code>configaudit</code> as short names for <code>vulnerabilityreports</code> and <code>configauditreports</code> resources.</p> <p>Note</p> <p>The validity period for VulnerabilityReports by setting the duration as the value of the <code>OPERATOR_SCANNER_REPORT_TTL</code> environment variable. The value is set to <code>24h</code> by default.</p> <p>The reports will be deleted after 24 hours. When a VulnerabilityReport gets deleted Trivy-Operator will automatically rescan the resource.</p>"},{"location":"getting-started/quick-start/#whats-next","title":"What's Next?","text":"<ul> <li>Find out how the operator scans workloads that use container images from [Private Registries].</li> <li>By default, the operator uses Trivy as [Vulnerability Scanner] and Polaris as [Configuration Checker], but you can   choose other tools that are integrated with Trivy-Operator or even implement you own plugin.</li> </ul>"},{"location":"getting-started/installation/","title":"Trivy-Operator","text":""},{"location":"getting-started/installation/#overview","title":"Overview","text":"<p>This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan and configuration audit when a new Pod is started.</p> Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects. <p>In other words, the desired state for the controllers managed by this operator is that for each workload or node there are security reports stored in the cluster as custom resources. Each custom resource is owned by a built-in resource to inherit its life cycle. Beyond that, we take advantage of Kubernetes garbage collector to automatically delete stale reports and trigger rescan. For example, deleting a ReplicaSet will delete controlee VulnerabilityReports, whereas deleting a VulnerabilityReport owned by a ReplicaSet will rescan that ReplicaSet and eventually recreate the VulnerabilityReport.</p> <p>Rescan is also triggered whenever a config of a configuration audit plugin has changed. For example, when a new OPA policy script is added to the Confest plugin config. This is implemented by adding the label named <code>plugin-config-hash</code> to ConfigAuditReport instances. The plugins' config reconciler watches the ConfigMap that holds plugin settings and computes a hash from the ConfigMap's data. The hash is then compared with values of the <code>plugin-config-hash</code> labels. If hashes are not equal then affected ConfigAuditReport objects are deleted, which in turn triggers rescan - this time with new plugin's configuration.</p> Plugin configuration reconciler deletes ConfigAuditReports whenever the configuration changes. <p>Warning</p> <p>Currently, the operator supports vulnerabilityreports, configauditreports security resources.      We also plan to implement rescan on configurable schedule, for example every 24 hours.</p>"},{"location":"getting-started/installation/#whats-next","title":"What's Next?","text":"<ul> <li>Install the operator and follow the Getting Started guide.</li> </ul>"},{"location":"getting-started/installation/configuration/","title":"Configuration","text":"<p>You can configure Trivy-Operator to control it's behavior and adapt it to your needs. Aspects of the operator machinery are configured using environment variables on the operator Pod, while aspects of the scanning behavior are controlled by ConfigMaps and Secrets.</p>"},{"location":"getting-started/installation/configuration/#operator-configuration","title":"Operator Configuration","text":"NAME DEFAULT DESCRIPTION <code>OPERATOR_NAMESPACE</code> N/A See Install modes <code>OPERATOR_TARGET_NAMESPACES</code> N/A See Install modes <code>OPERATOR_EXCLUDE_NAMESPACES</code> N/A A comma separated list of namespaces (or glob patterns) to be excluded from scanning in all namespaces Install mode. <code>OPERATOR_TARGET_WORKLOADS</code> All workload resources A comma separated list of Kubernetes workloads to be included in the vulnerability and config-audit scans <code>OPERATOR_SERVICE_ACCOUNT</code> <code>trivy-operator</code> The name of the service account assigned to the operator's pod <code>OPERATOR_LOG_DEV_MODE</code> <code>false</code> The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc). <code>OPERATOR_SCAN_JOB_TTL</code> <code>\"\"</code> The set automatic cleanup time after the job is completed <code>OPERATOR_SCAN_JOB_TIMEOUT</code> <code>5m</code> The length of time to wait before giving up on a scan job <code>OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT</code> <code>10</code> The maximum number of scan jobs create by the operator <code>OPERATOR_CONCURRENT_NODE_COLLECTOR_LIMIT</code> <code>1</code> The maximum number of node collector jobs create by the operator <code>OPERATOR_SCAN_JOB_RETRY_AFTER</code> <code>30s</code> The duration to wait before retrying a failed scan job <code>OPERATOR_BATCH_DELETE_LIMIT</code> <code>10</code> The maximum number of config audit reports deleted by the operator when the plugin's config has changed. <code>OPERATOR_BATCH_DELETE_DELAY</code> <code>10s</code> The duration to wait before deleting another batch of config audit reports. <code>OPERATOR_METRICS_BIND_ADDRESS</code> <code>:8080</code> The TCP address to bind to for serving Prometheus metrics. It can be set to <code>0</code> to disable the metrics serving. <code>OPERATOR_HEALTH_PROBE_BIND_ADDRESS</code> <code>:9090</code> The TCP address to bind to for serving health probes, i.e. <code>/healthz/</code> and <code>/readyz/</code> endpoints. <code>OPERATOR_VULNERABILITY_SCANNER_ENABLED</code> <code>true</code> The flag to enable vulnerability scanner <code>OPERATOR_CONFIG_AUDIT_SCANNER_ENABLED</code> <code>false</code> The flag to enable configuration audit scanner <code>OPERATOR_RBAC_ASSESSMENT_SCANNER_ENABLED</code> <code>true</code> The flag to enable rbac assessment scanner <code>OPERATOR_CONFIG_AUDIT_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> <code>true</code> The flag to enable config audit scanner to only scan the current revision of a deployment <code>OPERATOR_CONFIG_AUDIT_SCANNER_BUILTIN</code> <code>true</code> The flag to enable built-in configuration audit scanner <code>OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> <code>true</code> The flag to enable vulnerability scanner to only scan the current revision of a deployment <code>OPERATOR_INFRA_ASSESSMENT_SCANNER_ENABLED</code> <code>true</code> The flag to enable cluster infra assessment scanner <code>OPERATOR_CLUSTER_COMPLIANCE_ENABLED</code> <code>true</code> The flag to enable cluster compliance scanner <code>OPERATOR_ACCESS_GLOBAL_SECRETS_SERVICE_ACCOUNTS</code> <code>true</code> The flag to enable access to global secrets/service accounts to allow <code>vulnerability scan job</code> to pull images from private registries <code>OPERATOR_SCANNER_REPORT_TTL</code> <code>\"24h\"</code> The flag to set how long a report should exist. When a old report is deleted a new one will be created by the controller. It can be set to <code>\"\"</code> to disabled the TTL for vulnerability scanner. <code>OPERATOR_LEADER_ELECTION_ENABLED</code> <code>false</code> The flag to enable operator replica leader election <code>OPERATOR_LEADER_ELECTION_ID</code> <code>trivy-operator-lock</code> The name of the resource lock for leader election <code>OPERATOR_EXPOSED_SECRET_SCANNER_ENABLED</code> <code>true</code> The flag to enable exposed secret scanner <code>OPERATOR_WEBHOOK_BROADCAST_URL</code> <code>\"\"</code> The flag to enable operator reports to be sent to a webhook endpoint. \"\" means that this feature is disabled <code>OPERATOR_BUILT_IN_TRIVY_SERVER</code> <code>false</code> The flag enable the usage of built-in trivy server in cluster ,its also overwrite the following trivy params with built-in values trivy.mode = ClientServer and serverURL = http://[server Service Name].[trivy Operator Namespace]:4975 <code>OPERATOR_WEBHOOK_BROADCAST_TIMEOUT</code> <code>30s</code> The flag to set operator webhook timeouts, if webhook broadcast is enabled <code>OPERATOR_SEND_DELETED_REPORTS</code> <code>false</code> The flag to enable sending deleted reports if webhookBroadcastURL is enabled <code>OPERATOR_PRIVATE_REGISTRY_SCAN_SECRETS_NAMES</code> <code>{}</code> The flag is map of namespace:secrets, secrets are comma seperated which can be used to authenticate in private registries in case if there no imagePullSecrets provided example : <code>OPERATOR_MERGE_RBAC_FINDING_WITH_CONFIG_AUDIT</code> <code>false</code> The flag to enable merging rbac finding with config-audit report <p>The values of the <code>OPERATOR_NAMESPACE</code> and <code>OPERATOR_TARGET_NAMESPACES</code> determine the install mode, which in turn determines the multitenancy support of the operator.</p> MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace <code>operators</code> <code>operators</code> The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace <code>operators</code> <code>foo</code> The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace <code>operators</code> <code>foo,bar,baz</code> The operator can be configured to watch for events in more than one namespace. AllNamespaces <code>operators</code> (blank string) The operator can be configured to watch for events in all namespaces."},{"location":"getting-started/installation/configuration/#example-configure-namespaces-to-scan","title":"Example - configure namespaces to scan","text":"<p>To change the target namespace from all namespaces to the <code>default</code> namespace edit the <code>trivy-operator</code> Deployment and change the value of the <code>OPERATOR_TARGET_NAMESPACES</code> environment variable from the blank string (<code>\"\"</code>) to the <code>default</code> value.</p>"},{"location":"getting-started/installation/configuration/#scanning-configuration","title":"Scanning configuration","text":"CONFIGMAP KEY DEFAULT DESCRIPTION <code>vulnerabilityReports.scanner</code> <code>Trivy</code> The name of the plugin that generates vulnerability reports. Either <code>Trivy</code> or <code>Aqua</code>. <code>vulnerabilityReports.scanJobsInSameNamespace</code> <code>\"false\"</code> Whether to run vulnerability scan jobs in same namespace of workload. Set <code>\"true\"</code> to enable. <code>scanJob.tolerations</code> N/A JSON representation of the tolerations to be applied to the scanner pods and node-collector so that they can run on nodes with matching taints. Example: <code>'[{\"key\":\"key1\", \"operator\":\"Equal\", \"value\":\"value1\", \"effect\":\"NoSchedule\"}]'</code> <code>nodeCollector.volumeMounts</code> see helm/values.yaml node-collector pod volumeMounts definition for collecting config files information <code>nodeCollector.volumes</code> see helm/values.yaml node-collector pod volumes definition for collecting config files information <code>scanJob.nodeSelector</code> N/A JSON representation of the [nodeSelector] to be applied to the scanner pods so that they can run on nodes with matching labels. Example: <code>'{\"example.com/node-type\":\"worker\", \"cpu-type\": \"sandylake\"}'</code> <code>scanJob.automountServiceAccountToken</code> <code>\"false\"</code> the flag to enable automount for service account token on scan job. Set <code>\"true\"</code> to enable. <code>scanJob.skipInitContainers</code> <code>\"false\"</code> when this flag is set to true, the initContainers will be skipped for the scanner and node collector pod. Set <code>\"true\"</code> to enable. <code>report.additionalLabels</code> <code>\"\"</code> additionalReportLabels comma-separated representation of the labels which the user wants the reports to be labeled with. Example: <code>foo=bar,env=stage</code> will labeled the reports with the labels <code>foo: bar</code> and <code>env: stage</code> <code>scanJob.annotations</code> N/A One-line comma-separated representation of the annotations which the user wants the scanner pods to be annotated with. Example: <code>foo=bar,env=stage</code> will annotate the scanner pods with the annotations <code>foo: bar</code> and <code>env: stage</code> <code>scanJob.templateLabel</code> N/A One-line comma-separated representation of the template labels which the user wants the scanner pods to be labeled with. Example: <code>foo=bar,env=stage</code> will labeled the scanner pods with the labels <code>foo: bar</code> and <code>env: stage</code> <code>scanJob.podTemplatePodSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner pods to be secured with. Example: <code>{\"RunAsUser\": 1000, \"RunAsGroup\": 1000, \"RunAsNonRoot\": true}</code> <code>scanJob.podTemplateContainerSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner containers (and their initContainers) to be amended with. Example: <code>{\"allowPrivilegeEscalation\": false, \"capabilities\": { \"drop\": [\"ALL\"]},\"privileged\": false, \"readOnlyRootFilesystem\": true }</code> <code>report.resourceLabels</code> N/A One-line comma-separated representation of the scanned resource labels which the user wants to include in the Prometheus metrics report. Example: <code>owner,app,tier</code> <code>metrics.resourceLabelsPrefix</code> <code>k8s_label</code> Prefix that will be prepended to the labels names indicated in <code>report.ResourceLabels</code> when including them in the Prometheus metrics <code>report.recordFailedChecksOnly</code> <code>\"true\"</code> this flag is to record only failed checks on misconfiguration reports (config-audit and rbac assessment) <code>skipResourceByLabels</code> N/A One-line comma-separated labels keys which trivy-operator will skip scanning on resources with matching labels. Example: <code>test,transient</code> <code>node.collector.imageRef</code> ghcr.io/aquasecurity/node-collector:0.0.6 The imageRef use for node-collector job . <code>node.collector.imagePullSecret</code> N/A imagePullSecret is the secret name to be used when pulling node-collector image from private registries . <code>nodeCollector.excludeNodes</code> <code>\"\"</code> excludeNodes comma-separated node labels that the node-collector job should exclude from scanning (example kubernetes.io/arch=arm64,team=dev)"},{"location":"getting-started/installation/configuration/#example-patch-configmap","title":"Example - patch ConfigMap","text":"<p>By default Trivy displays vulnerabilities with all severity levels (<code>UNKNOWN</code>, <code>LOW</code>, <code>MEDIUM</code>, <code>HIGH</code>, <code>CRITICAL</code>). To display only <code>HIGH</code> and <code>CRITICAL</code> vulnerabilities by patching the <code>trivy.severity</code> value in the <code>trivy-operator-trivy-config</code> ConfigMap:</p> <pre><code>kubectl patch cm trivy-operator-trivy-config -n trivy-system \\\n--type merge \\\n-p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.severity\": \"HIGH,CRITICAL\"\n  }\n}\nEOF\n)\"\n</code></pre>"},{"location":"getting-started/installation/configuration/#example-patch-secret","title":"Example - patch Secret","text":"<p>To set the GitHub token used by Trivy scanner add the <code>trivy.githubToken</code> value to the <code>trivy-operator-trivy-config</code> Secret:</p> <pre><code>kubectl patch secret trivy-operator-trivy-config -n trivy-system \\\n--type merge \\\n-p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n &lt;your token&gt; | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre>"},{"location":"getting-started/installation/configuration/#example-delete-a-key","title":"Example - delete a key","text":"<p>The following <code>kubectl patch</code> command deletes the <code>trivy.httpProxy</code> key:</p> <pre><code>kubectl patch cm trivy-operator-trivy-config -n trivy-system \\\n--type json \\\n-p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'\n</code></pre>"},{"location":"getting-started/installation/helm/","title":"Helm","text":"<p>Helm, which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts.</p> <p>To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Trivy-Operator. The Helm chart supports all Install Modes.</p> <p>As an example, let's install the operator in the <code>trivy-system</code> namespace and configure it to select all namespaces, except <code>kube-system</code> and <code>trivy-system</code>:</p> <ol> <li>Clone the chart directory:    <pre><code>git clone --depth 1 --branch v0.16.1 https://github.com/aquasecurity/trivy-operator.git\ncd trivy-operator\n</code></pre>    Or add Aqua chart repository:    <pre><code>helm repo add aqua https://aquasecurity.github.io/helm-charts/\nhelm repo update\n</code></pre></li> <li>Install the chart from a local directory:    <pre><code>helm install trivy-operator ./deploy/helm \\\n  --namespace trivy-system \\\n  --create-namespace \\\n</code></pre>    Or install the chart from the Aqua chart repository:    <pre><code>helm install trivy-operator aqua/trivy-operator \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --version 0.18.1\n</code></pre></li> </ol> <p>Configuration options can be passed using the <code>--set</code> parameter. To list only the fixed vulnerabilities in the cluster, one can use the following command.    <pre><code>   helm install trivy-operator ./deploy/helm \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --set=\"trivy.ignoreUnfixed=true\"\n</code></pre></p> <p>There are many values in the chart that can be set to configure Trivy-Operator. See the Customising section for more details. 4. Check that the <code>trivy-operator</code> Helm release is created in the <code>trivy-system</code> namespace, and it has status    <code>deployed</code>:    <pre><code>$ helm list -n trivy-system\nNAME                 NAMESPACE           REVISION    UPDATED                                 STATUS      CHART                       APP VERSION\ntrivy-operator   trivy-system    1           2021-01-27 20:09:53.158961 +0100 CET    deployed    trivy-operator-0.18.1   0.16.1\n</code></pre>    To confirm that the operator is running, check that the <code>trivy-operator</code> Deployment in the <code>trivy-system</code>    namespace is available and all its containers are ready:    <pre><code>$ kubectl get deployment -n trivy-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></p>"},{"location":"getting-started/installation/helm/#install-as-helm-dependency","title":"Install as Helm dependency","text":"<p>There are cases, when potential chart developers want to add the operator as dependency. An example would be the creation of an umbrella chart for an application, which depends on 3d-party charts.</p> <p>In this case, It maybe not suitable to install the Trivy Operator in the same namespace as the main application. Instead, we can use the Helm value <code>operator.namespace</code> to define a namespace where only the operator will be installed. The Operator chart will then either create a new namespace if not existing or use the existing one.</p>"},{"location":"getting-started/installation/helm/#uninstall","title":"Uninstall","text":"<p>You can uninstall the operator with the following command:</p> <pre><code>helm uninstall trivy-operator -n trivy-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the <code>helm install</code> command:</p> <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd exposedsecretreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd rbacassessmentreports.aquasecurity.github.io\nkubectl delete crd infraassessmentreports.aquasecurity.github.io\nkubectl delete crd clusterrbacassessmentreports.aquasecurity.github.io\nkubectl delete crd clustercompliancereports.aquasecurity.github.io\nkubectl delete crd clusterinfraassessmentreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd sbomreports.aquasecurity.github.io\n</code></pre>"},{"location":"getting-started/installation/helm/#customising-the-helm-chart","title":"Customising the Helm Chart","text":"<p>The Trivy Operator Helm Chart can be customised in the same way as other Helm Charts, by overwriting values in the <code>values.yaml</code> files.</p> <p>You can find all the values that can be customised in the README of the Helm Chart on GitHub.</p> <p>There are two ways to overwrite values in a Helm chart upon installation:</p> <p>Create a custom values.yaml file with your changes and give Helm the file upon installation</p> <p>e.g. to specfy that Trivy should ignore all unfixed vulnerabilities:    <pre><code>trivy:\nignoreUnfixed: true\n</code></pre></p> <p>The file can be passed into Trivy with the <code>--values</code> flag in Helm:</p> <pre><code>helm install trivy-operator aqua/trivy-operator \\\n--namespace trivy-system \\\n--create-namespace \\\n--values values.yaml\n</code></pre> <p>Set the values that you want to customise in the installation command</p> <p>This is done with the <code>--set</code> command in Helm:</p> <pre><code>helm install trivy-operator aqua/trivy-operator \\\n--namespace trivy-system \\\n--create-namespace \\\n--set=\"trivy.ignoreUnfixed=true\" \\\n</code></pre>"},{"location":"getting-started/installation/kubectl/","title":"kubectl","text":"<p>Kubernetes Yaml deployment files are available on GitHub in https://github.com/aquasecurity/trivy-operator under <code>/deploy/static</code>.</p>"},{"location":"getting-started/installation/kubectl/#example-deploy-from-github","title":"Example - Deploy from GitHub","text":"<p>This will install the operator in the <code>trivy-system</code> namespace and configure it to scan all namespaces, except <code>kube-system</code> and <code>trivy-system</code>:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/trivy-operator/v0.16.1/deploy/static/trivy-operator.yaml\n</code></pre> <p>To confirm that the operator is running, check that the <code>trivy-operator</code> Deployment in the <code>trivy-system</code> namespace is available and all its containers are ready:</p> <pre><code>$ kubectl get deployment -n trivy-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           11m\n</code></pre> <p>If for some reason it's not ready yet, check the logs of the <code>trivy-operator</code> Deployment for errors:</p> <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre>"},{"location":"getting-started/installation/kubectl/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure Trivy-Operator to control it's behavior and adapt it to your needs. Aspects of the operator machinery are configured using environment variables on the operator Pod, while aspects of the scanning behavior are controlled by ConfigMaps and Secrets. To learn more, please refer to the Configuration documentation.</p>"},{"location":"getting-started/installation/kubectl/#uninstall","title":"Uninstall","text":"<p>Danger</p> <p>Uninstalling the operator and deleting custom resource definitions will also delete all generated security reports.</p> <p>You can uninstall the operator with the following command:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/aquasecurity/trivy-operator/v0.16.1/deploy/static/trivy-operator.yaml\n</code></pre>"},{"location":"getting-started/installation/olm/","title":"Operator Lifecycle Manager","text":"<p>The Operator Lifecycle Manager (OLM) provides a declarative way to install and upgrade operators and their dependencies.</p> <p>You can install the Trivy operator from OperatorHub.io or ArtifactHUB by creating the OperatorGroup, which defines the operator's multitenancy, and Subscription that links everything together to run the operator's pod.</p> <p>As an example, let's install the operator from the OperatorHub catalog in the <code>trivy-system</code> namespace and configure it to watch the <code>default</code> namespaces:</p> <ol> <li> <p>Install the Operator Lifecycle Manager:    <pre><code>curl -L https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.20.0/install.sh -o install.sh\nchmod +x install.sh\n./install.sh v0.20.0\n</code></pre></p> </li> <li> <p>Create the namespace to install the operator in:    <pre><code>kubectl create ns trivy-system\n</code></pre></p> </li> <li>Create the OperatorGroup to select all namespaces:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: trivy-operator-group\n  namespace: trivy-system\nEOF\n</code></pre></li> <li> <p>Install the operator by creating the Subscription:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: trivy-operator-subscription\n  namespace: trivy-system\nspec:\n  channel: alpha\n  name: trivy-operator\n  source: operatorhubio-catalog\n  sourceNamespace: olm\n  installPlanApproval: Automatic\n  config:\n    env:\n    - name: OPERATOR_EXCLUDE_NAMESPACES\n     value: \"kube-system\"\nEOF\n</code></pre>    The operator will be installed in the <code>trivy-system</code> namespace and will select all namespaces, except    <code>kube-system</code> and <code>trivy-system</code>. </p> </li> <li> <p>After install, watch the operator come up using the following command:    <pre><code>$ kubectl get clusterserviceversions -n trivy-system\nNAME                        DISPLAY              VERSION   REPLACES                     PHASE\ntrivy-operator.v0.16.1  Trivy Operator   0.16.1    trivy-operator.v0.16.0   Succeeded\n</code></pre>    If the above command succeeds and the ClusterServiceVersion has transitioned from <code>Installing</code> to <code>Succeeded</code> phase    you will also find the operator's Deployment in the same namespace where the Subscription is:    <pre><code>$ kubectl get deployments -n trivy-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/olm/#uninstall","title":"Uninstall","text":"<p>To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup:</p> <pre><code>kubectl delete subscription trivy-operator-subscription -n trivy-system\nkubectl delete clusterserviceversion trivy-operator.v0.16.1 -n trivy-system\nkubectl delete operatorgroup trivy-operator-group -n trivy-system\nkubectl delete ns trivy-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the OLM operator:</p> <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd exposedsecrets.aquasecurity.github.io\n</code></pre>"},{"location":"getting-started/installation/troubleshooting/","title":"Troubleshooting the Trivy Operator","text":"<p>The Trivy Operator installs several Kubernetes resources into your Kubernetes cluster.</p> <p>Here are the common steps to check whether the operator is running correctly and to troubleshoot common issues.</p> <p>So in addition to this section, you might want to check issues, discussion forum, or Slack to see if someone from the community had similar problems before.</p> <p>Also note that Trivy Operator is based on existing Aqua OSS project - [Starboard], and shares some of the design, principles and code with it. Existing content that relates to Starboard Operator might also be relevant for Trivy Operator, and Starboard's issues, discussion forum, or Slack might also be interesting to check. In some cases you might want to refer to Starboard's Design documents</p>"},{"location":"getting-started/installation/troubleshooting/#installation","title":"Installation","text":"<p>Make sure that the latest version of the Trivy Operator is installed. For this, have a look at the installation options.</p> <p>For instance, if your are using the Helm deployment, you need to check the Helm Chart version deployed to your cluster. You can check the Helm Chart version with the following command: <pre><code>helm list -n trivy-system\n</code></pre></p>"},{"location":"getting-started/installation/troubleshooting/#operator-pod-not-running","title":"Operator Pod Not Running","text":"<p>The Trivy Operator will run a pod inside your cluster. If you have followed the installation guide, you will have installed the Operator to the <code>trivy-system</code>.</p> <p>Make sure that the pod is in the <code>Running</code> status: <pre><code>kubectl get pods -n trivy-system\n</code></pre></p> <p>This is how it will look if it is running okay:</p> <pre><code>NAMESPACE            NAME                                         READY   STATUS    RESTARTS      AGE\ntrivy-system     trivy-operator-6c9bd97d58-hsz4g          1/1     Running   5 (19m ago)   30h\n</code></pre> <p>If the pod is in <code>Failed</code>, <code>Pending</code>, or <code>Unknown</code> check the events and the logs of the pod.</p> <p>First, check the events, since they might be more descriptive of the problem. However, if the events do not give a clear reason why the pod cannot spin up, then you want to check the logs, which provide more detail.</p> <pre><code>kubectl describe pod &lt;POD-NAME&gt; -n trivy-system\n</code></pre> <p>To check the logs, use the following command: <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></p> <p>If your pod is not running, try to look for errors as they can give an indication on the problem.</p> <p>If there are too many logs messages, try deleting the Trivy pod and observe its behavior upon restarting. A new pod should spin up automatically after deleting the failed pod.</p>"},{"location":"getting-started/installation/troubleshooting/#imagepullbackoff-or-errimagepull","title":"ImagePullBackOff or ErrImagePull","text":"<p>Check the status of the Trivy Operator pod running inside of your Kubernetes cluster. If the Status is ImagePullBackOff or ErrImagePull, it means that the Operator either</p> <ul> <li>tries to access the wrong image</li> <li>cannot pull the image from the registry</li> </ul> <p>Make sure that you are providing the right resources upon installing the Trivy Operator.</p>"},{"location":"getting-started/installation/troubleshooting/#crashloopbackoff","title":"CrashLoopBackOff","text":"<p>If your pod is in <code>CrashLoopBackOff</code>, it is likely the case that the pod cannot be scheduled on the Kubernetes node that it is trying to schedule on. In this case, you want to investigate further whether there is an issue with the node. It could for instance be the case that the node does not have sufficient resources.</p>"},{"location":"getting-started/installation/troubleshooting/#reconciliation-error","title":"Reconciliation Error","text":"<p>It could happen that the pod appears to be running normally but does not reconcile the resources inside of your Kubernetes cluster.</p> <p>Check the logs for Reconciliation errors: <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></p> <p>If this is the case, the Trivy Operator likely does not have the right configurations to access your resource.</p>"},{"location":"getting-started/installation/troubleshooting/#operator-does-not-create-vulnerabilityreports","title":"Operator does not Create VulnerabilityReports","text":"<p>VulnerabilityReports are owned and controlled by the immediate Kubernetes workload. Every VulnerabilityReport of a pod is thus, linked to a ReplicaSet. In case the Trivy Operator does not create a VulnerabilityReport for your workloads, it could be that it is not monitoring the namespace that your workloads are running on.</p> <p>An easy way to check this is by looking for the <code>ClusterRoleBinding</code> for the Trivy Operator:</p> <pre><code>kubectl get ClusterRoleBinding | grep \"trivy-operator\"\n</code></pre> <p>Alternatively, you could use the <code>kubectl-who-can</code> plugin by Aqua:</p> <pre><code>$ kubectl who-can list vulnerabilityreports\nNo subjects found with permissions to list vulnerabilityreports assigned through RoleBindings\n\nCLUSTERROLEBINDING                           SUBJECT                         TYPE            SA-NAMESPACE\ncluster-admin                                system:masters                  Group\ntrivy-operator                           trivy-operator              ServiceAccount  trivy-system\nsystem:controller:generic-garbage-collector  generic-garbage-collector       ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller            ServiceAccount  kube-system\nsystem:controller:resourcequota-controller   resourcequota-controller        ServiceAccount  kube-system\nsystem:kube-controller-manager               system:kube-controller-manager  User\n</code></pre> <p>If the <code>ClusterRoleBinding</code> does not exist, Trivy currently cannot monitor any namespace outside of the <code>trivy-system</code> namespace.</p> <p>For instance, if you are using the Helm Chart, you want to make sure to set the <code>targetNamespace</code> to the namespace that you want the Operator to monitor.</p> <p>The operator also could not be configured to scan the workload you are expecting. Check to make sure <code>OPERATOR_TARGET_WORKLOADS</code> is set correctly in your configuration. This allows you to specify which workload types to be scanned. </p> <p>For example, by default in the Helm Chart values, the following Kubernetes workloads are configured to be scanned <code>\"pod,replicaset,replicationcontroller,statefulset,daemonset,cronjob,job\"</code>.</p>"},{"location":"getting-started/installation/upgrade/","title":"Upgrade","text":"<p>We recommend that you upgrade Trivy Operator often to stay up to date with the latest fixes and enhancements.</p> <p>However, at this stage we do not provide automated upgrades. Therefore, uninstall the previous version of the operator before you install the latest release.</p> <p>Warning</p> <p>Consult release notes and changelog to revisit and migrate configuration settings which may not be compatible between different versions.</p>"},{"location":"tutorials/grafana-dashboard/","title":"Trivy Operator Dashboard in Grafana","text":""},{"location":"tutorials/grafana-dashboard/#accessing-trivy-operator-metrics-through-a-grafana-dashboard","title":"Accessing Trivy Operator Metrics through a Grafana Dashboard","text":"<p>In this tutorial, we showcase how you can access the metrics from your Trivy Operator reports through Grafana.</p>"},{"location":"tutorials/grafana-dashboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>The Helm CLI installed</li> <li>Access a Kubernetes cluster through kubectl (any cluster will do, however, if you use microk8s or another local Kubernetes cluster, you need to make sure DNS is enabled. Most providers will have a guide on how to enable it.)</li> </ul>"},{"location":"tutorials/grafana-dashboard/#installing-prometheus-and-grafana","title":"Installing Prometheus and Grafana","text":"<p>Prometheus and Grafana can easily be installed through the kube-prometheus-stack Helm Chart.</p> <p>First, create a <code>monitoring</code> namespace in which we can install the Prometheus &amp; Grafana resources:</p> <pre><code>kubectl create ns monitoring\n</code></pre> <p>Add the chart to your Helm CLI:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n</code></pre> <p>Then update your charts to access the latest versions:</p> <pre><code>helm repo update\n</code></pre> <p>Our Prometheus installation needs to be slightly customised to discover ServiceMonitors by default. Create a values.yaml file with the following configuration:</p> <pre><code>prometheus:\n  prometheusSpec:\n    serviceMonitorSelectorNilUsesHelmValues: false\n    serviceMonitorSelector: {}\n    serviceMonitorNamespaceSelector: {}\n</code></pre> <p>If you are working on a more complex installation or you would like the Helm Chart to connect with other applications such as Promtail or other monitoring tools, the values.yaml file is a good place to set up those configuration.</p> <p>Next, install the Helm Chart:</p> <pre><code>helm upgrade --install prom prometheus-community/kube-prometheus-stack -n monitoring --values values.yaml\n</code></pre> <p>Note that if your values.yaml file is saved in a different directory than your current directory, then please modify its path.</p> <p>You should see a success message upon installation similar to the following:</p> <pre><code>Release \"prom\" does not exist. Installing it now.\nNAME: prom\nLAST DEPLOYED: Fri Nov 25 11:21:24 2022\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nNOTES:\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace monitoring get pods -l \"release=prom\"\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.\n</code></pre>"},{"location":"tutorials/grafana-dashboard/#installing-the-trivy-operator-helm-chart","title":"Installing the Trivy Operator Helm Chart","text":"<p>In this section, we will install the Trivy Operator Helm Chart. The commands are provided in the documentation.</p> <pre><code>helm repo add aqua https://aquasecurity.github.io/helm-charts/\nhelm repo update\n</code></pre> <p>Before we install the operator, we will need to create a values.yaml file for Trivy with some slight changes to the Helm installation:</p> <pre><code>serviceMonitor:\n  # enabled determines whether a serviceMonitor should be deployed\n  enabled: true\ntrivy:\n  ignoreUnfixed: true\n</code></pre> <p>In the changes above, we tell the Trivy Helm Chart to first, enable the ServiceMonitor and then to ignore all vulnerabilities that do not have a fix available yet. The ServiceMonitor is required to allow Prometheus to discover the Trivy Operator Service and scrape its metrics.</p> <p>Next, we can install the operator with the following command:</p> <pre><code>helm install trivy-operator aqua/trivy-operator \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --version 0.16.1 \\\n  --values trivy-values.yaml\n</code></pre> <p>Ensure that you can see the following success message:</p> <pre><code>NAME: trivy-operator\nLAST DEPLOYED: Fri Nov 25 12:46:35 2022\nNAMESPACE: trivy-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nYou have installed Trivy Operator in the trivy-system namespace.\nIt is configured to discover Kubernetes workloads and resources in\nall namespace(s).\n</code></pre>"},{"location":"tutorials/grafana-dashboard/#open-the-prometheus-and-the-grafana-dashboard","title":"Open the Prometheus and the Grafana Dashboard","text":"<p>With the following command, you can access the Prometheus Dashboard:</p> <pre><code>kubectl port-forward service/prom-kube-prometheus-stack-prometheus -n monitoring 9090:9090\n</code></pre> <p>Next, open a new terminal and access the Grafana Dashboard:</p> <pre><code>kubectl port-forward service/prom-grafana -n monitoring 3000:80\n</code></pre>"},{"location":"tutorials/grafana-dashboard/#access-trivy-operator-metrics","title":"Access Trivy Operator Metrics","text":"<p>In a new terminal, we are going to port-forward to the Trivy Operator service to access the metrics provided by the operator.</p> <p>Note that this operation is optional and just used to demonstrate where you can find the metrics to then query them in a better way through Prometheus and Grafana.</p> <p>Run the following command to remove the headless setting  <code>clusterIP: None</code> by editing <code>trivy-operator</code> service:</p> <pre><code>kubectl edit service trivy-operator -n trivy-system\n</code></pre> <p>Run the following command to port-forward the Trivy Operator Service:</p> <pre><code>kubectl port-forward service/trivy-operator -n trivy-system 5000:80\n</code></pre> <p>Once you open the 'http://localhost:5000/metrics' you should see all the metrics gathered from the operator. However, this is obviously not the prettiest way of looking at them. Thus, the next sections will show you how to query metrics through Prometheus and visualise them in Grafana.</p>"},{"location":"tutorials/grafana-dashboard/#query-trivy-operator-metrics-in-prometheus","title":"Query Trivy Operator Metrics in Prometheus","text":"<p>Open the Prometheus Dashboard at 'localhost:9090' through the port-forwarding done in the previous section of this tutorial.</p> <p>At this point, navigate to: <code>Status</code> &lt; <code>Targets</code> -- and make sure that the Trivy endpoint is healthy and Prometheus can scrape its metrics.</p> <p>Next, head back to 'Graph' -- http://localhost:9090/graph. Here you can already query certain metrics from the Trivy Operator. The query language used is basic PromQL. There are lots of guides online that can give you inspiration. Try for instance the following queries:</p> <p>Total vulnerabilities found in your cluster:</p> <pre><code>sum(trivy_image_vulnerabilities)\n</code></pre> <p>Total misconfiguration identified in your cluster:</p> <pre><code>sum(trivy_resource_configaudits)\n</code></pre> <p>Exposed Secrets discovered by the Trivy Operator in your cluster:</p> <pre><code>sum(trivy_image_exposedsecrets)\n</code></pre>"},{"location":"tutorials/grafana-dashboard/#set-up-grafana-dashboard-for-trivy-operator-metrics","title":"Set up Grafana Dashboard for Trivy Operator Metrics","text":"<p>Lastly, we want to visualise the security issues within our cluster in a Grafana Dashboard.</p> <p>For this, navigate to the Grafana URL 'http://localhost:3000'.</p> <p>Username: admin Password: prom-operator</p> <p>Note that the password will be different, depending on how you called the Helm Chart installation of the kube-prometheus-stack Helm Chart earlier in the tutorial.</p> <p>Next, navigate to <code>Dashboards</code> &lt; <code>Browse</code></p> <p>Once you see all the default Dashboards, click <code>New</code>, then <code>Import</code>.</p> <p>Here, we will paste the ID of the Aqua Trivy Dashboard: <code>17813</code></p> <p>The link to the Dashboard in Grafana is the following.</p> <p>Once pasted, you should see the following Dashboard as part of your Dashboard list: <code>Trivy Operator Dashboard</code></p> <p></p>"},{"location":"tutorials/manage_access_to_security_reports/","title":"Manage Access to Security Reports","text":"<p>In Trivy-Operator security reports are stored as CRD instances (e.g. VulnerabilityReport and ConfigAuditReport objects).</p> <p>With Kubernetes RBAC, a cluster administrator can choose the following levels of granularity to manage access to security reports:</p> <ol> <li>Grant administrative access to view any report in any namespace.</li> <li>Grant coarse-grained access to view any report in a specified namespace.</li> <li>Grant fine-grained access to view a specified report in a specified namespace.</li> </ol> <p>Even though you can achieve fine-grained access control with Kubernetes RBAC configuration, it is very impractical to do so with security reports. Mainly because security reports are associated with ephemeral Kubernetes objects such as Pods and ReplicaSets.</p> <p>To sum up, we only recommend using administrative and coarse-grained levels to manage access to security reports.</p> <p>Continue reading to see examples of managing access to VulnerabilityReport objects at different levels of granularity.</p>"},{"location":"tutorials/manage_access_to_security_reports/#create-namespaces-and-deployments","title":"Create Namespaces and Deployments","text":"<p>Let's consider a multitenant cluster with two <code>nginx</code> Deployments in <code>foo</code> and <code>bar</code> namespaces. There's also the <code>redis</code> Deployment in the <code>foo</code> namespace.</p> <pre><code>kubectl create namespace foo\nkubectl create deploy nginx --image nginx:1.16 --namespace foo\nkubectl create deploy redis --image redis:5 --namespace foo\n</code></pre> <pre><code>kubectl create namespace bar\nkubectl create deploy nginx --image nginx:1.16 --namespace bar\n</code></pre> <p>Tip</p> <p>For workloads with multiple containers we'll have multiple instances of VulnerabilityReports with the same prefix (<code>replicaset-nginx-7967dc8bfd-</code>) but different suffixes that correspond to container names.</p> <pre><code>$ kubectl tree deploy nginx --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/nginx                                           -              21m\nfoo        \u2514\u2500ReplicaSet/nginx-7967dc8bfd                              -              21m\nfoo          \u251c\u2500Pod/nginx-7967dc8bfd-gqw8h                             True           21m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-nginx-7967dc8bfd-nginx  -              4m36s\n</code></pre> <pre><code>$ kubectl tree deploy nginx --namespace bar\nNAMESPACE  NAME                                                      READY  REASON  AGE\nbar        Deployment/nginx                                          -              20m\nbar        \u2514\u2500ReplicaSet/nginx-f4cc56f6b                              -              20m\nbar          \u251c\u2500Pod/nginx-f4cc56f6b-9cd45                             True           20m\nbar          \u2514\u2500VulnerabilityReport/replicaset-nginx-f4cc56f6b-nginx  -              2m12s\n</code></pre> <pre><code>$ kubectl tree deploy redis --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/redis                                           -              74m\nfoo        \u2514\u2500ReplicaSet/redis-79c5cc7cf8                              -              74m\nfoo          \u251c\u2500Pod/redis-79c5cc7cf8-fz99f                             True           74m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-redis-79c5cc7cf8-redis  -              74m\n</code></pre>"},{"location":"tutorials/manage_access_to_security_reports/#choose-access-level","title":"Choose Access Level","text":"<p>To manage access to VulnerabilityReport instances a cluster administrator will typically create Role or ClusterRole objects and bind them to subjects (users, groups, or service accounts) by creating RoleBinding or ClusterRoleBinding objects.</p> <p>With Kubernetes RBAC there are three different granularity levels at which you can grant access to VulnerabilityReports:</p> <ol> <li>Cluster - a subject can view any report in any namespace</li> <li>Namespace - a subject can view any report in a specified namespace</li> <li>Security Report - a subject can view a specified report in a specified namespace</li> </ol>"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-any-namespace","title":"Grant Access to View any VulnerabilityReport in any Namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create clusterrolebinding dpacak-can-view-vulnerabilityreports \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as dpacak\nNAMESPACE   NAME                                REPOSITORY      TAG    SCANNER   AGE\nbar         replicaset-nginx-f4cc56f6b-nginx    library/nginx   1.16   Trivy     40m\nfoo         replicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     43m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as zpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"zpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" at the cluster scope\n</code></pre> <pre><code>$ kubectl who-can get vulnerabilityreports -A\nNo subjects found with permissions to get vulns assigned through RoleBindings\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\ndpacak-can-view-vulnerabilityreports         dpacak                     User\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre> <p>Note</p> <p>The who-can command is a kubectl plugin that shows who has RBAC permissions to perform actions on different resources in Kubernetes.</p>"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-the-foo-namespace","title":"Grant Access to View any VulnerabilityReport in the foo Namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-vulnerabilityreports \\\n  --namespace foo \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace foo --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     51m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace bar --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"dpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" in the namespace \"bar\"\n</code></pre>"},{"location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-the-replicaset-nginx-7967dc8bfd-nginx-vulnerabilityreport-in-the-foo-namespace","title":"Grant Access to View the replicaset-nginx-7967dc8bfd-nginx VulnerabilityReport in the foo Namespace","text":"<p>Even though you can grant access to a single VulnerabilityReport by specifying its name when you create Role or ClusterRole objects, in practice it's not manageable for these reasons:</p> <ol> <li>The name of a ReplicaSet (e.g. <code>nginx-7967dc8bfd</code>) and hence the name of the corresponding VulnerabilityReport (e.g.    <code>replicaset-nginx-7967dc8bfd-nginx</code>) change over time. This requires that Role or ClusterObject will be updated    respectively.</li> <li>We create a VulnerabilityReport for each container of a Kubernetes workload. Therefore, managing such fine-grained    permissions is even more cumbersome.</li> <li>Last but not least, the naming convention is an implementation details that's likely to change when we add support    for mutable tags or implement caching of scan results.</li> </ol> <pre><code>kubectl create role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --resource vulnerabilityreports \\\n  --resource-name replicaset-nginx-7967dc8bfd-nginx \\\n  --verb get\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-nginx-7967dc8bfd-nginx --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     163m\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-redis-79c5cc7cf8-redis --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io \"rep\nlicaset-redis-79c5cc7cf8-redis\" is forbidden: User \"dpacak\" cannot get resource\n\"vulnerabilityreports\" in API group \"aquasecurity.github.io\" in the namespace \"\nfoo\"\n</code></pre> <pre><code>$ kubectl who-can get vuln/replicaset-nginx-7967dc8bfd-nginx -n foo\nROLEBINDING                                        NAMESPACE  SUBJECT  TYPE  SA-NAMESPACE\ndpacak-can-view-replicaset-nginx-7967dc8bfd-nginx  foo        dpacak   User\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre>"},{"location":"tutorials/microk8s/","title":"Using the Trivy Operator addon in microk8s","text":""},{"location":"tutorials/microk8s/#using-the-trivy-operator-through-microk8s","title":"Using the Trivy Operator through Microk8s","text":"<p>Microk8s is a lightweight Kubernetes distribution that can be used on your personal machine, Raspberry Pi cluster, in data centres or edge devices; just to name a few use cases.</p> <p>One of the benefits of using microk8s is its add-on ecosystem. Once you have microk8s installed, you can spin up a variety of cloud native projects directly in your cluster through merely one command:</p> <pre><code>microk8s enable &lt;name of the addon&gt;\n</code></pre> <p>A list of addons is provided below. <pre><code>    dashboard-ingress    # (community) Ingress definition for Kubernetes dashboard\n    jaeger               # (community) Kubernetes Jaeger operator with its simple config\n    knative              # (community) Knative Serverless and Event Driven Applications\n    linkerd              # (community) Linkerd is a service mesh for Kubernetes and other frameworks\n    multus               # (community) Multus CNI enables attaching multiple network interfaces to pods\n    openebs              # (community) OpenEBS is the open-source storage solution for Kubernetes\n    osm-edge             # (community) osm-edge is a lightweight SMI compatible service mesh for the edge-computing.\n    portainer            # (community) Portainer UI for your Kubernetes cluster\n    trivy-operator       # (community) Kubernetes-native security toolkit\n    traefik              # (community) traefik Ingress controller for external access\n    dns                  # (core) CoreDNS\n    ha-cluster           # (core) Configure high availability on the current node\n    helm                 # (core) Helm - the package manager for Kubernetes\n    helm3                # (core) Helm 3 - the package manager for Kubernetes\n    trivy                # (core) Kubernetes-native security scanner\n    cert-manager         # (core) Cloud native certificate management\n    dashboard            # (core) The Kubernetes dashboard\n    host-access          # (core) Allow Pods connecting to Host services smoothly\n    hostpath-storage     # (core) Storage class; allocates storage from host directory\n    ingress              # (core) Ingress controller for external access\n    kube-ovn             # (core) An advanced network fabric for Kubernetes\n    mayastor             # (core) OpenEBS MayaStor\n    metallb              # (core) Loadbalancer for your Kubernetes cluster\n    metrics-server       # (core) K8s Metrics Server for API access to service metrics\n    observability        # (core) A lightweight observability stack for logs, traces and metrics\n    prometheus           # (core) Prometheus operator for monitoring and logging\n    rbac                 # (core) Role-Based Access Control for authorisation\n    registry             # (core) Private image registry exposed on localhost:32000\n    storage              # (core) Alias to hostpath-storage add-on, deprecated\n</code></pre></p> <p>This tutorial will showcase how to install and then remove the Trivy Operator addon.</p>"},{"location":"tutorials/microk8s/#prerequisites","title":"Prerequisites","text":"<p>You need to have microk8s installed. In our case, we have set up kubectl to use the microk8s cluster. You can find different guides, depending on your operating system, on the microk8s website.</p>"},{"location":"tutorials/microk8s/#install-the-trivy-operator","title":"Install the Trivy Operator","text":"<p>To install the Trivy Operator, simply run the following command: <pre><code>microk8s enable trivy\n</code></pre></p> <p>The confirmation should be similar to the following output: <pre><code>Infer repository core for addon trivy\nInfer repository core for addon helm3\nAddon core/helm3 is already enabled\nInfer repository core for addon dns\nAddon core/dns is already enabled\nInstalling Trivy\n\"aqua\" already exists with the same configuration, skipping\nRelease \"trivy-operator\" does not exist. Installing it now.\nNAME: trivy-operator\nLAST DEPLOYED: Sat Oct  8 16:39:59 2022\nNAMESPACE: trivy-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nYou have installed Trivy Operator in the trivy-system namespace.\nIt is configured to discover Kubernetes workloads and resources in\nall namespace(s).\n\nInspect created VulnerabilityReports by:\n\n    kubectl get vulnerabilityreports --all-namespaces -o wide\n\nInspect created ConfigAuditReports by:\n\n    kubectl get configauditreports --all-namespaces -o wide\n\nInspect the work log of trivy-operator by:\n\n    kubectl logs -n trivy-system deployment/trivy-operator\nTrivy is installed\n</code></pre></p> <p>You should now see the Trivy Operator pod running inside of the <code>trivy-system</code> namespace: <pre><code>kubectl get all -n trivy-system\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/trivy-operator-57c44575c4-ml2hw             1/1     Running   0          29s\npod/scan-vulnerabilityreport-5d55f55cd7-7l6kn   1/1     Running   0          27s\n\nNAME                     TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/trivy-operator   ClusterIP   None         &lt;none&gt;        80/TCP    29s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/trivy-operator   1/1     1            1           29s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/trivy-operator-57c44575c4   1         1         1       29s\n\nNAME                                            COMPLETIONS   DURATION   AGE\njob.batch/scan-vulnerabilityreport-5d55f55cd7   0/1           27s        27s\n</code></pre></p> <p>If you have any container images running in your microk8s cluster, Trivy will start a vulnerability scan on those right away. </p>"},{"location":"tutorials/microk8s/#cleaning-up","title":"Cleaning up","text":"<p>Removing the Trivy Operator from your cluster is as easy as installing it. Simply run: <pre><code>microk8s disable trivy\n</code></pre></p> <p>You should see an output similar to the following: <pre><code>Infer repository core for addon trivy\nDisabling Trivy\nrelease \"trivy-operator\" uninstalled\nTrivy disabled\n</code></pre></p>"},{"location":"tutorials/private-registries/","title":"Allow the Trivy Operator to access private registries","text":"<p>In this tutorial, we will detail multiple ways on setting up the Trivy Operator to access and scan container images from private container registries.</p>"},{"location":"tutorials/private-registries/#prerequisites","title":"Prerequisites","text":"<p>To follow this tutorial, you need the following installed on your machine:</p> <ul> <li>The Helm CLI tool</li> <li>kubectl and connected to a running Kubernetes cluster</li> <li>a container images in a private registy, running as a pod inside your cluster</li> </ul> <p>Note that we will be using a local Kubernetes KinD cluster and a private container image stored on a private GitHub repository for the examples.</p>"},{"location":"tutorials/private-registries/#first-option-filesystem-scanning","title":"First Option: Filesystem Scanning","text":"<p>For this tutorial, we will use the Operator Helm Chart.</p> <p>The configuration options for the Helm Chart can be found in the values.yaml manifest. Navigate to the section <code>Trivy.command</code>. The default will be:</p> <pre><code>trivy:\n# command. One of `image`, `filesystem` or `rootfs` scanning, depending on the target type required for the scan.\n  # For 'filesystem' and `rootfs` scanning, ensure that the `trivyOperator.scanJobPodTemplateContainerSecurityContext` is configured\n  # to run as the root user (runAsUser = 0).\n  command: image\n</code></pre> <p>By default, the command that trivy is supposed to run inside your cluster is <code>trivy image</code> for container image scanning. However, we want to change it to scan the filesystem in your nodes instead. Container images are ultimately stored as files on the node level of your cluster. This way, trivy is going to scan the files of your container images for vulnerabilities. This is a little bit of a work-around with the downside that the Trivy Operator will have to run as root. However, remember that security scanning already requires the operator to have lots of cluster privileges.</p> <p>Next, we will change the the <code>command</code> and the <code>trivyOperator.scanJobPodTemplateContainerSecurityContext</code>of the <code>values.yaml</code> manifest. For this, we can create a new values.yaml manifest with our desired modifications:</p> <pre><code>trivy:\n    command: fs\n    ignoreUnfixed: true\ntrivyOperator:\n    scanJobPodTemplateContainerSecurityContext:\n        # For filesystem scanning, Trivy needs to run as the root user\nrunAsUser: 0\n</code></pre> <p>Lastly, we can deploy the operator inside our cluster with referencing our new <code>values.yaml</code> manifest to override the default values:</p> <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n--namespace trivy-system \\\n--create-namespace \\\n--version 0.16.1\n  --values ./values.yaml\n</code></pre> <p>Alternatively, it is possible to set the values directly through Helm instead of referencing an additional <code>values.yaml</code> file:</p> <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n--namespace trivy-system \\\n--create-namespace \\\n--version 0.16.1\n  --set=\"trivy.command=fs\"\n--set=\"trivyOperator.scanJobPodTemplateContainerSecurityContext.runAsUser=0\"\n</code></pre> <p>Once installed, make sure that</p> <ol> <li>the operator is running in your cluster</li> <li>the operator has created a VulnerabilityReport for the container image from the private registry</li> </ol> <pre><code>\u276f kubectl get deployment -n trivy-system\n\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           99s\n</code></pre>"},{"location":"tutorials/private-registries/#second-option-using-an-imagepullsecret-to-access-containers-from-the-private-registry","title":"Second Option: Using an ImagePullSecret to access containers from the Private Registry","text":"<p>Note that you might be using an ImagePullSecret already to allow pods to pull the container images from a private registry.</p> <p>To set-up an ImagePullSecret, we first need an access token to our private registry. For GitHub private registries, you can create a new access token under the following link. In comparison, the official Kubernetes documentation shows how to create the ImagePullSecret for the DockerHub.</p> <p>Next, we will base64 encode the access token:</p> <pre><code>echo -n \"YOUR_GH_ACCOUNT_NAME:YOUR_TOKEN\" | base64\n</code></pre> <p>And take the output of the previous command and parse it into the following to base64 encode it again:</p> <pre><code>echo -n  '{\"auths\":{\"ghcr.io\":{\"auth\":\"OUTPUT\"}}}' | base64\n</code></pre> <p>Lastly, we are going to store the output in a Kubernetes Secret YAML manifest:</p> <p><code>imagepullsecret.yaml</code></p> <pre><code>kind: Secret\ntype: kubernetes.io/dockerconfigjson\napiVersion: v1\nmetadata:\nname: dockerconfigjson-github-com\nlabels:\napp: app-name\ndata:\n.dockerconfigjson: OUTPUT\n</code></pre> <p>Note that base64 encoding is not encryption, thust, you should not commit this file. If you are looking to store secrets in Kubernetes, have a look at e.g. Hashicorp Secret Vault, or with an External Secrets Operator (ESO).</p> <p>Make sure to reference the ImagePullSecret in your container <code>spec</code>:</p> <pre><code>containers:\n- name: cns-website\n  image: ghcr.io/account-name/image-id:tag\nimagePullSecrets:\n- name: dockerconfigjson-github-com\n</code></pre> <p>And finally, we can apply the secret to the same namespace as our application:</p> <pre><code>kubectl apply -f imagepullsecret.yaml -n app\n</code></pre> <p>If you have to modify your deployment.yaml manifest, make sure to update that as well.</p> <p>Once you have defined your ImagePullSecret, the Operator will have access to the container image automatically with the defaul configuration.</p>"},{"location":"tutorials/private-registries/#third-option-define-an-imagepullsecret-through-a-serviceaccount","title":"Third Option: Define an ImagePullSecret through a ServiceAccount","text":"<p>Alternatively to defining an ImagePullSecret on the pod level, we can also define the secret through a Kubernetes Service Account. Our workload will be associated with the service account and can pull the secret from our private registry. Similar to the <code>Second Option</code>, once we have the key associated to our workload, the Trivy operator scan job has access to the secret and can pull the image.</p> <p>Again, you can have a look at the official Kubernetes documentation for further details.</p> <p><code>imagepullsecret.yaml</code></p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: dockerconfigjson-sa-github-com\nannotations:\nkubernetes.io/service-account.name: cns-website\ntype: kubernetes.io/dockerconfigjson\ndata:\n.dockerconfigjson: OUTPUT\n</code></pre> <p><code>serviceaccount.yaml</code></p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cns-website\nimagePullSecrets:\n- name: dockerconfigjson-sa-github-com\n</code></pre> <p><code>deployment.yaml</code> *or where you have defined your container</p> <pre><code>spec:\ncontainers:\n- name: cns-website\nimage: ghcr.io/account-name/image-id:tag\nserviceAccountName: cns-website\n</code></pre>"},{"location":"tutorials/private-registries/#fourth-option-define-secrets-through-trivy-operator-configuration","title":"Fourth Option: Define Secrets through Trivy-Operator configuration","text":"<p>If there are no ImagePullSecret on pod or Service Account level (for example, valid credentials are placed in container runtime configuration) you can add them in Trivy-Operator configuration.</p> <p>It's very similar to <code>Second Option</code>. First of all you need to create a secret. To do it, we first need an access token to our private registry. For GitHub private registries, you can create a new access token under the following link. In comparison, the official Kubernetes documentation shows how to create the ImagePullSecret for the DockerHub.</p> <p>Next, we will base64 encode the access token:</p> <pre><code>echo -n \"YOUR_GH_ACCOUNT_NAME:YOUR_TOKEN\" | base64\n</code></pre> <p>And take the output of the previous command and parse it into the following to base64 encode it again:</p> <pre><code>echo -n  '{\"auths\":{\"ghcr.io\":{\"auth\":\"OUTPUT\"}}}' | base64\n</code></pre> <p>Lastly, we are going to store the output in a Kubernetes Secret YAML manifest:</p> <p><code>imagepullsecret.yaml</code></p> <pre><code>kind: Secret\ntype: kubernetes.io/dockerconfigjson\napiVersion: v1\nmetadata:\nname: dockerconfigjson-github-com\nlabels:\napp: app-name\ndata:\n.dockerconfigjson: OUTPUT\n</code></pre> <p>Note that base64 encoding is not encryption, thust, you should not commit this file. If you are looking to store secrets in Kubernetes, have a look at e.g. Hashicorp Secret Vault, or with an External Secrets Operator (ESO).</p> <p>And finally, we can apply the secret to the same namespace as our application:</p> <pre><code>kubectl apply -f imagepullsecret.yaml -n app\n</code></pre> <p>Next, we will change the <code>privateRegistryScanSecretsNames</code> of the <code>values.yaml</code> manifest. For this, we can create a new <code>values.yaml</code> manifest with our desired modification. We need to provide desired namespace and secret name. In our example they are <code>app</code> and <code>dockerconfigjson-github-com</code> accordingly.</p> <pre><code>operator:\n    privateRegistryScanSecretsNames: {\"app\":\"dockerconfigjson-github-com\"}\n</code></pre> <p>If you want you can add additional namespaces and secret names to <code>privateRegistryScanSecretsNames</code> separated by comma.</p> <p>Lastly, we can deploy the operator inside our cluster with referencing our new <code>values.yaml</code> manifest to override the default values:</p> <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n--namespace trivy-system \\\n--create-namespace \\\n--version 0.16.1\n  --values ./values.yaml\n</code></pre> <p>Alternatively, it is possible to set the values directly through Helm instead of referencing an additional <code>values.yaml</code> file:</p> <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n--namespace trivy-system \\\n--create-namespace \\\n--version 0.16.1\n  --set-json='operator.privateRegistryScanSecretsNames={\"app\":\"dockerconfigjson-github-com\"}'\n</code></pre> <p>Works only with helm 3.10+, because <code>--set-json</code> flag was added in 3.10.0. Otherwise you can use <code>values.yaml</code> instead.</p> <p>Once installed, make sure that</p> <ol> <li>the operator is running in your cluster</li> <li>the operator has created a VulnerabilitReport for the container image from the private registry</li> </ol> <pre><code>\u276f kubectl get deployment -n trivy-system\n\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           99s\n</code></pre>"},{"location":"tutorials/private-registries/#fifth-option-configure-gcr-service-account-json","title":"Fifth Option: configure gcr service account json","text":"<p>this option is similar to forth option but using gcr json_key for gcr service account and creating a secret</p> <pre><code>echo -n \"_json_key:{\n  \"type\": \"service_account\",\n  \"project_id\": \"test\",\n  \"private_key_id\": \"3adas34asdas34wadad\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"&lt;test@test.iam.gserviceaccount.com&gt;\",\n  \"client_id\": \"34324234324324\",\n  \"auth_uri\": \"&lt;https://accounts.google.com/o/oauth2/auth&gt;\",\n  \"token_uri\": \"&lt;https://oauth2.googleapis.com/token&gt;\",\n  \"auth_provider_x509_cert_url\": \"&lt;https://www.googleapis.com/oauth2/v1/certs&gt;\",\n  \"client_x509_cert_url\": \"&lt;https://www.googleapis.com/robot/v1/metadata/x509/test-gcr-f5dh3h5g%40test.iam.gserviceaccount.com&gt;\"\n}\" | base64\n</code></pre> <p>And take the output of the previous command and parse it into the following to base64 encode it again:</p> <pre><code>echo -n '{\"auths\":{\"us.gcr.io\":{\"auth\":\"OUTPUT\"}}}' | base64\n</code></pre> <p>Lastly, we are going to store the output in a Kubernetes Secret YAML manifest:</p> <p><code>gcr_secret.yaml</code></p> <pre><code>kind: Secret\ntype: kubernetes.io/dockerconfigjson\napiVersion: v1\nmetadata:\nname: dockerconfigjson-github-com\nlabels:\napp: app-name\ndata:\n.dockerconfigjson: OUTPUT\n</code></pre>"},{"location":"tutorials/private-registries/#sixth-option-grant-access-through-managed-registries","title":"Sixth Option: Grant access through managed registries","text":"<p>The last way that you could give the Trivy operator access to your private container registry is through managed registries. In this case, the container registry and your Kubernetes cluster would have to be on the same cloud provider; then you can define access to your container namespace as part of the IAM account. Once defined, trivy will already have the permissions for the registry.</p> <p>For additional information, please refer to the documentation on managed registries.</p>"},{"location":"tutorials/writing-custom-configuration-audit-policies/","title":"Writing Custom Configuration Audit Policies","text":"<p>trivy-operator ships with a set of Built-in Configuration Audit Policies defined as OPA Rego policies. You can also define custom policies and associate them with applicable Kubernetes resources to extend basic configuration audit functionality.</p> <p>This tutorial will walk through the process of creating and testing a new configuration audit policy that fails whenever a Kubernetes resource doesn't specify <code>app.kubernetes.io/name</code> or <code>app.kubernetes.io/version</code> labels.</p>"},{"location":"tutorials/writing-custom-configuration-audit-policies/#writing-a-policy","title":"Writing a Policy","text":"<p>To define such a policy, you must first define its metadata. This includes setting a unique identifier, title, severity (<code>CRITICAL</code>, <code>HIGH</code>, <code>MEDIUM</code>, <code>LOW</code>), descriptive text, and remediation steps. In Rego it's defined as the <code>__rego_metadata__</code> rule, which defines the following composite value:</p> <pre><code>package trivyoperator.policy.k8s.custom\n\nimport data.lib.result\nimport future.keywords.in\n\n__rego_metadata__ := {\n    \"id\": \"recommended_labels\",\n    \"title\": \"Recommended labels\",\n    \"severity\": \"LOW\",\n    \"type\": \"Kubernetes Security Check\",\n    \"description\": \"A common set of labels allows tools to work interoperably, describing objects in a common manner that all tools can understand.\",\n    \"recommended_actions\": \"Take full advantage of using recommended labels and apply them on every resource object.\",\n    \"url\": \"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\",\n}\n</code></pre> <p>Note that the <code>recommended_labels</code> policy in scoped to the <code>trivyoperator.policy.k8s.custom</code> package to avoid naming collision with built-in policies that are pre-installed with trivy-operator.</p> <p>Once we've got our metadata defined, we need to create the logic of the policy, which is done in the <code>deny</code> or <code>warn</code> rule.</p> <pre><code>__rego_input__ := {\n    \"combine\": false,\n    \"selector\": [{\"type\": \"kubernetes\"}],\n}\n\ndeny[res] {\n    input.kind == \"Pod\"\n    some container in input.spec.containers\n    not startswith(container.image, \"hooli.com\")\n    msg := sprintf(\"Image '%v' comes from untrusted registry\", [container.image])\n    res := result.new(msg, container)\n}\n</code></pre> <p>These matches are essentially Rego assertions, so anyone familiar with writing rules for OPA or other tools that use Rego should find the process familiar. In this case, it\u2019s pretty straightforward. We subtract the set of labels specified by the <code>input</code> resource object from the set of recommended labels. The resulting set is stored in the variable called <code>missing</code>. Finally, we check if the <code>missing</code> set is empty. If not, the <code>deny</code> rule fails with the appropriate message.</p> <p>The <code>input</code> document is set by trivy-operator to a Kubernetes resource when the policy is evaluated. For pods, it would look something like the following listing:</p> <pre><code>{\n\"apiVersion\": \"v1\",\n\"kind\": \"Pod\",\n\"metadata\": {\n\"name\": \"nginx\",\n\"labels\": {\n\"run\": \"nginx\"\n}\n},\n\"spec\": {\n\"containers\": [\n{\n\"name\": \"nginx\",\n\"image\": \"nginx:1.16\",\n}\n]\n}\n}\n</code></pre> <p>The labels set on the pod resource above can be retrieved with the following Rego expression:</p> <pre><code>provided := {label | input.metadata.labels[label]}\n</code></pre> <p>You can find the complete Rego code listing in recommended_labels.rego.</p>"},{"location":"tutorials/writing-custom-configuration-audit-policies/#testing-a-policy","title":"Testing a Policy","text":"<p>Now that you've created the policy, you need to test it to make sure it works as intended. To do that, add policy code to the <code>trivy-operator-policies-config</code> ConfigMap and associate it with any (<code>*</code>) Kubernetes resource kind:</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: trivy-operator-policies-config\nnamespace: trivy-system\nlabels:\napp.kubernetes.io/name: trivy-operator\napp.kubernetes.io/instance: trivy-operator\napp.kubernetes.io/version: \"0.16.1\"\napp.kubernetes.io/managed-by: kubectl\ndata:\npolicy.recommended_labels.kinds: \"*\"\npolicy.recommended_labels.rego: |\npackage trivyoperator.policy.k8s.custom\n\nimport data.lib.result\nimport future.keywords.in\n\n__rego_metadata__ := {\n\"id\": \"recommended_labels\",\n\"title\": \"Recommended labels\",\n\"severity\": \"LOW\",\n\"type\": \"Kubernetes Security Check\",\n\"description\": \"A common set of labels allows tools to work interoperably, describing objects in a common manner that all tools can understand.\",\n\"recommended_actions\": \"Take full advantage of using recommended labels and apply them on every resource object.\",\n\"url\": \"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\",\n}\n\n__rego_input__ := {\n\"combine\": false,\n\"selector\": [{\"type\": \"kubernetes\"}],\n}\n\ndeny[res] {\ninput.kind == \"Pod\"\nsome container in input.spec.containers\nnot startswith(container.image, \"hooli.com\")\nmsg := sprintf(\"Image '%v' comes from untrusted registry\", [container.image])\nres := result.new(msg, container)\n}\n</code></pre> <p>In this example, to add a new policy, you must define two data entries in the <code>trivy-operator-policies-config</code> ConfigMap:</p> <ol> <li>The <code>policy.&lt;your_policy_name&gt;.kinds</code> entry is used to designate applicable Kubernetes resources as a comma separated    list of Kubernetes kinds (e.g., <code>Pod,ConfigMap,NetworkPolicy</code>). There is also a special value (<code>Workload</code>) that you    can use to select all Kubernetes workloads, and (<code>*</code>) to select all Kubernetes resources recognized by trivy-operator.</li> <li>The <code>policy.&lt;your_policy_name&gt;.rego</code> entry holds the policy Rego code.</li> </ol> <p>trivy-operator automatically detects policies added to the <code>trivy-operator-policies-config</code> ConfigMap and immediately rescans applicable Kubernetes resources.</p> <p>Let's create the <code>test</code> ConfigMap without recommended labels:</p> <pre><code>$ kubectl create cm test --from-literal=foo=bar\nconfigmap/test created\n</code></pre> <p>When you retrieve the corresponding configuration audit report, you'll see that there is one check with <code>LOW</code> severity that's failing:</p> <pre><code>$ kubectl get configauditreport configmap-test -o wide\nNAME             SCANNER     AGE   CRITICAL  HIGH   MEDIUM   LOW\nconfigmap-test   trivy-operator   24s   0         0      0        1\n</code></pre> <p>If you describe the report you'll see that it's failing because of our custom policy:</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\nlabels:\ntrivy-operator.resource.kind: ConfigMap\ntrivy-operator.resource.name: test\ntrivy-operator.resource.namespace: default\nplugin-config-hash: df767ff5f\nresource-spec-hash: 7c96769cf\nname: configmap-test\nnamespace: default\nownerReferences:\n- apiVersion: v1\nblockOwnerDeletion: false\ncontroller: true\nkind: ConfigMap\nname: test\nreport:\nscanner:\nname: trivy-operator\nvendor: Aqua Security\nversion: v0.16.1\nsummary:\ncriticalCount: 0\nhighCount: 0\nlowCount: 1\nmediumCount: 0\nchecks:\n- checkID: recommended_labels  # (1)\ntitle: Recommended labels    # (2)\nseverity: LOW                # (3)\ncategory: Kubernetes Security Check  # (4)\ndescription: |                       # (5)\nA common set of labels allows tools to work interoperably,\ndescribing objects in a common manner that all tools can\nunderstand.\nsuccess: false  # (6)\nmessages:       # (7)\n- 'You must provide labels: {\"app.kubernetes.io/name\", \"app.kubernetes.io/version\"}'\n</code></pre> <ol> <li>The <code>checkID</code> property corresponds to the policy identifier, i.e. <code>__rego_meatadata__.id</code>.</li> <li>The <code>title</code> property as defined by the policy metadata in <code>__rego_metadata__.title</code>.</li> <li>The <code>severity</code> property as defined by the policy metadata in <code>__rego_metadata__.severity</code>.</li> <li>The <code>category</code> property as defined by the policy metadata in <code>__rego_metadata__.type</code>.</li> <li>The <code>description</code> property as defined by the policy metadata in <code>__rego_metadata__.description</code>.</li> <li>The flag indicating whether the configuration audit check has failed or passed.</li> <li>The array of messages with details in case of failure.</li> </ol>"},{"location":"tutorials/integrations/lens/","title":"Lens Extension","text":"<p>Lens provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. Lens Extensions API is used to add custom visualizations and functionality to accelerate development workflows for all the technologies and services that integrate with Kubernetes.</p> <p>Trivy-Operator Lens Extension provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.</p>"},{"location":"tutorials/integrations/metrics/","title":"Metrics","text":"<p><code>trivy-operator</code> exposed a <code>/metrics</code> endpoint by default  with metrics for vulnerabilities, exposed secrets,rbacassessment and configaudits.</p>"},{"location":"tutorials/integrations/metrics/#report-summary","title":"Report Summary","text":""},{"location":"tutorials/integrations/metrics/#vunerability","title":"Vunerability","text":"<p>A report summary series exposes the count of checks of each status reported in a given <code>VulnerabilityReport</code>. For example:</p> <pre><code>trivy_image_vulnerabilities{\ncontainer_name=\"coredns\",image_digest=\"\",image_registry=\"index.docker.io\",image_repository=\"rancher/coredns-coredns\",image_tag=\"1.8.3\",name=\"replicaset-coredns-6488c6fcc6-coredns\",namespace=\"kube-system\",resource_kind=\"ReplicaSet\",resource_name=\"coredns-6488c6fcc6\",severity=\"High\"\n} 10\n</code></pre>"},{"location":"tutorials/integrations/metrics/#configaudit","title":"ConfigAudit","text":"<p>A report summary series exposes the count of checks of each status reported in a given <code>ConfigAuditReport</code>. For example:</p> <pre><code>trivy_resource_configaudits{\nname=\"daemonset-svclb-traefik\",namespace=\"kube-system\",resource_kind=\"DaemonSet\",resource_name=\"svclb-traefik\",severity=\"High\"\n} 2\n</code></pre>"},{"location":"tutorials/integrations/metrics/#configauditinfo","title":"ConfigAuditInfo","text":"<p>Exposes details about ConfigAudit that were discovered in images, enable by setting the EnvVar: <code>OPERATOR_METRICS_CONFIG_AUDIT_INFO_ENABLED\" envDefault:\"false\"</code> . For example:</p> <pre><code>trivy_configaudits_info{\nconfig_audit_category=\"car1 category for config audit\",config_audit_description=\"car1 description for config audit\",config_audit_id=\"car1 Id\",config_audit_success=\"false\",config_audit_title=\"car1 config audit title\",name=\"replicaset-nginx-6d4cf56db6\",namespace=\"default\",resource_kind=\"ReplicaSet\",resource_name=\"nginx-6d4cf56db6\",severity=\"Critical\"} 1\n</code></pre>"},{"location":"tutorials/integrations/metrics/#rbacassessments","title":"RbacAssessments","text":"<p>A report summary series exposes the count of checks of each status reported in a given <code>RbacAssessmentsReport</code>. For example:</p> <pre><code>trivy_role_rbacassessments{\nname=\"role-6fbccbcb9d\",namespace=\"kube-system\",resource_kind=\"Role\",resource_name=\"6fbccbcb9d\",severity=\"Medium\"\n} 1\n</code></pre>"},{"location":"tutorials/integrations/metrics/#rbacassessmentsinfo","title":"RbacAssessmentsInfo","text":"<p>Exposes details about RbacAssessments that were discovered in images, enable by setting the EnvVar: <code>OPERATOR_METRICS_RBAC_ASSESSMENT_INFO_ENABLED\" envDefault:\"false\"</code> . For example:</p> <pre><code>trivy_rbacassessments_info{\nname=\"role-admin-6d4cf56db6\",namespace=\"default\",rbac_assessment_category=\"car1 category for rbac assessment\",rbac_assessment_description=\"car1 description for rbac assessment\",rbac_assessment_id=\"car1 Id\",rbac_assessment_success=\"true\",rbac_assessment_title=\"car1 rbac assessment title\",resource_kind=\"Role\",resource_name=\"admin-6d4cf56db6\",severity=\"Critical\"} 1\n</code></pre>"},{"location":"tutorials/integrations/metrics/#exposedssecrets","title":"ExposedsSecrets","text":"<p>A report summary series exposes the count of checks of each status reported in a given <code>ExposedsSecretsReport</code>. For example:</p> <pre><code>trivy_image_exposedsecrets{\ncontainer_name=\"trivy\",image_digest=\"\",image_registry=\"index.docker.io\",image_repository=\"josedonizetti/trivy\",image_tag=\"secrettest\",name=\"pod-tt-reg-test\",namespace=\"default\",resource_kind=\"Pod\",resource_name=\"tt-reg-test\",severity=\"Critical\"\n} 1\n</code></pre>"},{"location":"tutorials/integrations/metrics/#exposedssecretsinfo","title":"ExposedsSecretsInfo","text":"<p>Exposes details about secrets that were discovered in images, enable by setting the EnvVar: <code>OPERATOR_METRICS_EXPOSED_SECRET_INFO_ENABLED\" envDefault:\"false\"</code> . For example:</p> <pre><code>trivy_exposedsecrets_info{\ncontainer_name=\"trivy\",image_digest=\"\",image_registry=\"index.docker.io\",image_repository=\"josedonizetti/trivy\",image_tag=\"secrettest\",name=\"pod-tt-reg-test\",namespace=\"default\",resource_kind=\"Pod\",resource_name=\"tt-reg-test\",secret_category=\"AWS\",secret_rule_id=\"aws-access-key-id\",secret_target=\"/etc/apt/s3auth.conf\",secret_title=\"AWS Access Key ID\",severity=\"Critical\"\n} 1\n</code></pre>"},{"location":"tutorials/integrations/metrics/#infraassessments","title":"InfraAssessments","text":"<p>A report summary series exposes the count of checks of each status reported in a given <code>InfraAssessmentsReport</code>. For example:</p> <pre><code>trivy_resource_infraassessments{\nname=\"pod-kube-controller-manager-minikube\",namespace=\"kube-system\",resource_kind=\"Pod\",resource_name=\"kube-controller-manager-minikube\",severity=\"Low\"\n} 3\n</code></pre>"},{"location":"tutorials/integrations/metrics/#infraassessmentsinfo","title":"InfraAssessmentsInfo","text":"<p>Exposes details about InfraAssessments that were discovered in images, enable by setting the EnvVar: <code>OPERATOR_METRICS_INFRA_ASSESSMENT_INFO_ENABLED\" envDefault:\"false\"</code> . For example: <pre><code>trivy_infraassessments_info{\nname=\"pod-kube-apiserver-minikube-6d4cf56db6\",namespace=\"kube-system\",infra_assessment_category=\"car1 category for infra assessment\",infra_assessment_description=\"car1 description for infra assessment\",infra_assessment_id=\"car1 Id\",infra_assessment_success=\"true\",infra_assessment_title=\"car1 infra assessment title\",resource_kind=\"Pod\",resource_name=\"kube-apiserver-minikube-6d4cf56db6\",severity=\"Critical\"\n} 1\n</code></pre></p>"},{"location":"tutorials/integrations/metrics/#clustercompliancereport","title":"ClusterComplianceReport","text":"<p>A report summary series exposes the count of checks of each status reported in a given <code>ClusterComplianceReport</code>. For example:</p> <pre><code>trivy_cluster_compliance{description=\"National Security Agency - Kubernetes Hardening Guidance\",status=\"Fail\",title=\"nsa\"} 12\ntrivy_cluster_compliance{description=\"National Security Agency - Kubernetes Hardening Guidance\",status=\"Pass\",title=\"nsa\"} 17\n</code></pre>"},{"location":"tutorials/integrations/metrics/#vulnerability-id","title":"Vulnerability ID","text":"<p>Exposing vulnerability ID on metrics by setting the EnvVar: <code>OPERATOR_METRICS_VULN_ID_ENABLED\" envDefault:\"false\"</code></p> <pre><code>trivy_vulnerability_id{\nclass=\"os-pkgs\",container_name=\"nginx\",fixed_version=\"\",image_digest=\"\",image_registry=\"index.docker.io\",image_repository=\"library/nginx\",image_tag=\"1.16.1\",installed_version=\"5.3.28+dfsg1-0.5\",last_modified_date=\"2023-06-28T21:16:00Z\",name=\"replicaset-nginx-deployment-559d658b74-nginx\",namespace=\"default\",package_type=\"debian\",pkg_path=\"/app/local\",published_date=\"2023-06-28T21:15:00Z\",resource=\"libdb5.3\",resource_kind=\"ReplicaSet\",resource_name=\"nginx-deployment-559d658b74\",severity=\"Critical\",vuln_id=\"CVE-2019-8457\",vuln_score=\"7.5\",vuln_title=\"sqlite: heap out-of-bound read in function rtreenode()\"\n} 1\n</code></pre>"},{"location":"tutorials/integrations/metrics/#adding-custom-label-to-metrics","title":"Adding Custom Label to Metrics","text":"<p>User might wants to include custom labels to resource that can be exposed and associated with the Prometheus metrics. this capability can be added by setting the following helm param.</p> <p>Example:</p> <p><code>--set=\"trivyOperator.reportResourceLabels\": \"owner\"</code></p> <p><code>k8s_label_</code> prefix will be added to custom label</p> <pre><code>trivy_resource_configaudits{k8s_label_owner=\"platform\",name=\"daemonset-svclb-traefik\",namespace=\"kube-system\",resource_kind=\"DaemonSet\",resource_name=\"svclb-traefik\",severity=\"Critical\"} 2\n</code></pre>"},{"location":"tutorials/integrations/policy-reporter/","title":"Policy Reporter Integration","text":"<p>Policy Reporter adds observability and monitoring possibilities to cluster security  based on the PolicyReport CRD, developed by the Kubernetes Policy Working Group.</p> <p>Key features are:</p> <ul> <li>New results can be send to different clients like Grafana Loki, Slack, Discord or MS Teams</li> <li>A webbased dashboard to get a graphical overview of all results with optional filters</li> <li>E-Mail Reports on a regular basis about the current status of your cluster security</li> <li>Provide metrics to observe violations in well known monitoring solutions like Grafana</li> <li>Can also be integrated with other tools such as Tracee, Kyverno or Falco</li> </ul>"},{"location":"tutorials/integrations/policy-reporter/#integration","title":"Integration","text":"<p>With fjogeleit/trivy-operator-polr-adapter it is possible to create generalized (Cluster)PolicyReport CRD from the various Trivy Operator CRDs, which allows integration with Policy Reporter.</p>"},{"location":"tutorials/integrations/policy-reporter/#screenshots","title":"Screenshots","text":""},{"location":"tutorials/integrations/policy-reporter/#vulnerabilityreports","title":"VulnerabilityReports","text":""},{"location":"tutorials/integrations/policy-reporter/#configauditreports","title":"ConfigAuditReports","text":""},{"location":"tutorials/integrations/webhook/","title":"Webhook Integration","text":"<p>Trivy Operator allows you to send reports externally to a webhook as they get produced. This is useful in cases where you would like to \"set-and-forget\" the operator and monitor the reports elsewhere. It's also useful when you have to make decisions based on a report, e.g. prune a vulnerable image, remove a deployment with exposed secrets etc.</p> <p>The latter use case can be fulfilled by using a SOAR tool Postee. Out of the box, Postee offers a variety of integrations with other third party services such as ServiceNow, Slack, AWS Security Hub and many more.  </p> <p></p> <p>You can enable the Webhook integration as follows:</p> <ol> <li>Required: Set <code>OPERATOR_WEBHOOK_BROADCAST_URL</code> to the webhook endpoint you'd like to send the reports to.</li> <li>Optional: Set <code>OPERATOR_WEBHOOK_BROADCAST_TIMEOUT</code> to a time limit that suites your use case. Default is <code>30s</code>.</li> <li>Optional: Set <code>OPERATOR_SEND_DELETED_REPORTS</code> to <code>true</code> to send webhook notifications when reports are deleted. Default is <code>false</code>.</li> </ol> <p>The Webhook integration support the following reports types:</p> <ul> <li><code>vulnerabilityreport</code></li> <li><code>exposedsecretreport</code></li> <li><code>configAuditReport</code></li> <li><code>infraAssessmentReport</code></li> <li><code>rbacAssessmentReport</code></li> <li><code>clusterRbacAssessmentReport</code></li> <li><code>clusterConfigAuditReport</code></li> <li><code>clusterInfraAssessmentReport</code></li> <li><code>clusterComplianceReport</code></li> <li><code>sbomReport</code></li> </ul>"}]}