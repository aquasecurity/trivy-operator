package vulnerabilityreport

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"reflect"
	"strings"

	"github.com/aquasecurity/trivy-operator/pkg/apis/aquasecurity/v1alpha1"
	"github.com/aquasecurity/trivy-operator/pkg/docker"
	"github.com/aquasecurity/trivy-operator/pkg/exposedsecretreport"
	"github.com/aquasecurity/trivy-operator/pkg/kube"
	"github.com/aquasecurity/trivy-operator/pkg/operator/etc"
	"github.com/aquasecurity/trivy-operator/pkg/operator/jobs"
	. "github.com/aquasecurity/trivy-operator/pkg/operator/predicate"
	"github.com/aquasecurity/trivy-operator/pkg/operator/workload"
	"github.com/aquasecurity/trivy-operator/pkg/trivyoperator"
	"github.com/go-logr/logr"
	"go.uber.org/multierr"
	batchv1 "k8s.io/api/batch/v1"
	k8sapierror "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

// WorkloadController watches Kubernetes workloads and generates
// v1alpha1.VulnerabilityReport instances using vulnerability scanner that that
// implements the Plugin interface.
type WorkloadController struct {
	logr.Logger
	etc.Config
	client.Client
	kube.ObjectResolver
	jobs.LimitChecker
	kube.LogsReader
	kube.SecretsReader
	Plugin
	trivyoperator.PluginContext
	trivyoperator.ConfigData
	VulnerabilityReadWriter ReadWriter
	ExposedSecretReadWriter exposedsecretreport.ReadWriter
}

//+kubebuilder:rbac:groups="",resources=pods,verbs=get;list;watch
//+kubebuilder:rbac:groups=apps,resources=replicasets,verbs=get;list;watch
//+kubebuilder:rbac:groups="",resources=replicationcontrollers,verbs=get;list;watch
//+kubebuilder:rbac:groups=apps,resources=statefulsets,verbs=get;list;watch
//+kubebuilder:rbac:groups=apps,resources=daemonsets,verbs=get;list;watch
//+kubebuilder:rbac:groups=batch,resources=cronjobs,verbs=get;list;watch
//+kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch
//+kubebuilder:rbac:groups=aquasecurity.github.io,resources=vulnerabilityreports,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=aquasecurity.github.io,resources=exposedsecretreports,verbs=get;list;watch;create;update;patch;delete

// Manage scan jobs with image pull secrets
// kubebuilder:rbac:groups="",resources=secrets,verbs=create;update
//+kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch;create;delete

func (r *WorkloadController) SetupWithManager(mgr ctrl.Manager) error {
	installModePredicate, err := InstallModePredicate(r.Config)
	if err != nil {
		return err
	}

	var resources []kube.Resource

	// Determine which Kubernetes workloads the controller will reconcile and add them to resources
	targetWorkloads := r.Config.GetTargetWorkloads()
	for _, tw := range targetWorkloads {
		var resource kube.Resource
		err := resource.GetWorkloadResource(tw, nil, r.ObjectResolver)
		if err != nil {
			return err
		}
		resources = append(resources, resource)
	}

	for _, resource := range resources {
		err = ctrl.NewControllerManagedBy(mgr).
			For(resource.ForObject, builder.WithPredicates(
				Not(ManagedByTrivyOperator),
				Not(IsBeingTerminated),
				installModePredicate,
			)).
			Owns(&v1alpha1.VulnerabilityReport{}).
			Owns(&v1alpha1.ExposedSecretReport{}).
			Complete(r.reconcileWorkload(resource.Kind))
		if err != nil {
			return err
		}
	}
	var predicates []predicate.Predicate
	if !r.ConfigData.VulnerabilityScanJobsInSameNamespace() {
		predicates = append(predicates, InNamespace(r.Config.Namespace))
	}
	predicates = append(predicates, ManagedByTrivyOperator, IsVulnerabilityReportScan, JobHasAnyCondition)
	return ctrl.NewControllerManagedBy(mgr).
		For(&batchv1.Job{}, builder.WithPredicates(predicates...)).
		Complete(r.reconcileJobs())
}

func (r *WorkloadController) reconcileWorkload(workloadKind kube.Kind) reconcile.Func {
	return func(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
		if r.BuiltInTrivyServer {
			trivyServerAvaliable, err := r.trivyServerAvaliable(r.GeTrivyServerURL())
			if err != nil {
				return ctrl.Result{}, err
			}
			if !trivyServerAvaliable {
				return ctrl.Result{RequeueAfter: r.Config.ScanJobRetryAfter}, nil
			}
		}
		log := r.Logger.WithValues("kind", workloadKind, "name", req.NamespacedName)
		workloadRef := kube.ObjectRefFromKindAndObjectKey(workloadKind, req.NamespacedName)
		log.V(1).Info("Getting workload from cache")
		workloadObj, err := r.ObjectFromObjectRef(ctx, workloadRef)
		if err != nil {
			if k8sapierror.IsNotFound(err) {
				log.V(1).Info("Ignoring cached workload that must have been deleted")
				return ctrl.Result{}, nil
			}
			return ctrl.Result{}, fmt.Errorf("getting %s from cache: %w", workloadKind, err)
		}

		// Skip processing if it's a Pod controlled by a built-in K8s workload.
		if skip, err := workload.SkipProcessing(ctx, workloadObj, r.ObjectResolver,
			r.Config.VulnerabilityScannerScanOnlyCurrentRevisions, log, r.ConfigData.GetSkipResourceByLabels()); skip {
			return ctrl.Result{}, err
		}

		podSpec, err := kube.GetPodSpec(workloadObj)
		if err != nil {
			return ctrl.Result{}, err
		}

		containerImages := kube.GetContainerImagesFromPodSpec(podSpec)
		hash := kube.ComputeHash(podSpec)

		log = log.WithValues("podSpecHash", hash)

		// Check if containers of the Pod have corresponding VulnerabilityReports.
		hasReports, err := r.hasReports(ctx, workloadRef, hash, containerImages)
		if err != nil {
			return ctrl.Result{}, fmt.Errorf("getting vulnerability reports: %w", err)
		}

		if hasReports {
			log.V(1).Info("VulnerabilityReports already exist")
			return ctrl.Result{}, nil
		}

		_, job, err := r.hasActiveScanJob(ctx, workloadRef, hash)
		if err != nil {
			return ctrl.Result{}, fmt.Errorf("checking scan job: %w", err)
		}

		if job != nil {
			log.V(1).Info("Scan job already exists",
				"job", fmt.Sprintf("%s/%s", job.Namespace, job.Name))
			return ctrl.Result{}, nil
		}

		limitExceeded, scanJobsCount, err := r.LimitChecker.Check(ctx)
		if err != nil {
			return ctrl.Result{}, err
		}
		log.V(1).Info("Checking scan jobs limit", "count", scanJobsCount, "limit", r.ConcurrentScanJobsLimit)

		if limitExceeded {
			log.V(1).Info("Pushing back scan job", "count", scanJobsCount, "retryAfter", r.ScanJobRetryAfter)
			return ctrl.Result{RequeueAfter: r.Config.ScanJobRetryAfter}, nil
		}

		return ctrl.Result{}, r.submitScanJob(ctx, workloadObj)
	}
}

func (r *WorkloadController) trivyServerAvaliable(serverURL string) (bool, error) {
	resp, err := http.Get(fmt.Sprintf("%s/%s", strings.TrimSuffix(serverURL, "/"), "healthz"))
	if err != nil {
		return false, err
	}
	if resp.StatusCode == http.StatusOK {
		return true, nil
	}
	return false, nil
}

func (r *WorkloadController) hasReports(ctx context.Context, owner kube.ObjectRef, hash string, images kube.ContainerImages) (bool, error) {
	hasVulnerabilityReports, err := r.hasVulnerabilityReports(ctx, owner, hash, images)
	if err != nil {
		return false, err
	}

	hasSecretReports, err := r.hasSecretReports(ctx, owner, hash, images)
	if err != nil {
		return false, err
	}

	return hasVulnerabilityReports && hasSecretReports, nil
}

func (r *WorkloadController) hasVulnerabilityReports(ctx context.Context, owner kube.ObjectRef, hash string, images kube.ContainerImages) (bool, error) {
	// TODO FindByOwner should accept optional label selector to further narrow down search results
	list, err := r.VulnerabilityReadWriter.FindByOwner(ctx, owner)
	if err != nil {
		return false, err
	}

	actual := map[string]bool{}
	for _, report := range list {
		if containerName, ok := report.Labels[trivyoperator.LabelContainerName]; ok {
			if hash == report.Labels[trivyoperator.LabelResourceSpecHash] {
				actual[containerName] = true
			}
		}
	}

	return compareReports(actual, images), nil
}

func (r *WorkloadController) hasSecretReports(ctx context.Context, owner kube.ObjectRef, hash string, images kube.ContainerImages) (bool, error) {
	// TODO FindByOwner should accept optional label selector to further narrow down search results
	list, err := r.ExposedSecretReadWriter.FindByOwner(ctx, owner)
	if err != nil {
		return false, err
	}

	actual := map[string]bool{}
	for _, report := range list {
		if containerName, ok := report.Labels[trivyoperator.LabelContainerName]; ok {
			if hash == report.Labels[trivyoperator.LabelResourceSpecHash] {
				actual[containerName] = true
			}
		}
	}

	return compareReports(actual, images), nil
}

func compareReports(actual map[string]bool, images kube.ContainerImages) bool {
	expected := map[string]bool{}
	for containerName := range images {
		expected[containerName] = true
	}

	return reflect.DeepEqual(actual, expected)
}

func (r *WorkloadController) hasActiveScanJob(ctx context.Context, owner kube.ObjectRef, hash string) (bool, *batchv1.Job, error) {
	jobName := fmt.Sprintf("scan-vulnerabilityreport-%s", kube.ComputeHash(owner))
	job := &batchv1.Job{}
	err := r.Get(ctx, client.ObjectKey{Namespace: r.Config.Namespace, Name: jobName}, job)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			return false, nil, nil
		}
		return false, nil, fmt.Errorf("getting job from cache: %w", err)
	}
	if job.Labels[trivyoperator.LabelResourceSpecHash] == hash {
		return true, job, nil
	}
	return false, nil, nil
}

func (r *WorkloadController) submitScanJob(ctx context.Context, owner client.Object) error {
	log := r.Logger.WithValues("kind", owner.GetObjectKind().GroupVersionKind().Kind,
		"name", owner.GetName(), "namespace", owner.GetNamespace())
	var err error
	credentials := make(map[string]docker.Auth, 0)
	if r.AccessGlobalSecretsAndServiceAccount {
		privateRegistrySecrets, err := r.Config.GetPrivateRegistryScanSecretsNames()
		if err != nil {
			return err
		}

		credentials, err = r.CredentialsByWorkloadAndEnv(ctx, owner, privateRegistrySecrets)
		if err != nil {
			return err
		}
	}

	scanJobTolerations, err := r.GetScanJobTolerations()
	if err != nil {
		return fmt.Errorf("getting scan job tolerations: %w", err)
	}

	scanJobAnnotations, err := r.GetScanJobAnnotations()
	if err != nil {
		return fmt.Errorf("getting scan job annotations: %w", err)
	}

	scanJobNodeSelector, err := r.GetScanJobNodeSelector()
	if err != nil {
		return fmt.Errorf("getting scan job nodeSelector: %w", err)
	}

	scanJobSecurityContext, err := r.GetScanJobPodSecurityContext()
	if err != nil {
		return fmt.Errorf("getting scan job podSecurityContext: %w", err)
	}

	scanJobContainerSecurityContext, err := r.GetScanJobContainerSecurityContext()
	if err != nil {
		return fmt.Errorf("getting scan job [container] securityContext: %w", err)
	}

	scanJobPodTemplateLabels, err := r.GetScanJobPodTemplateLabels()
	if err != nil {
		return fmt.Errorf("getting scan job template labels: %w", err)
	}

	scanJob, secrets, err := NewScanJobBuilder().
		WithPlugin(r.Plugin).
		WithPluginContext(r.PluginContext).
		WithTimeout(r.Config.ScanJobTimeout).
		WithObject(owner).
		WithTolerations(scanJobTolerations).
		WithAnnotations(scanJobAnnotations).
		WithNodeSelector(scanJobNodeSelector).
		WithPodSecurityContext(scanJobSecurityContext).
		WithSecurityContext(scanJobContainerSecurityContext).
		WithPodTemplateLabels(scanJobPodTemplateLabels).
		WithCredentials(credentials).
		Get()

	if err != nil {
		if errors.Is(err, kube.ErrReplicaSetNotFound) || errors.Is(err, kube.ErrNoRunningPods) ||
			errors.Is(err, kube.ErrUnSupportedKind) {
			log.V(1).Info("ignoring vulnerability scan", "reason", err)
			return nil
		}
		return fmt.Errorf("constructing scan job: %w", err)
	}

	for _, secret := range secrets {
		err = r.Client.Create(ctx, secret)
		if err != nil {
			if k8sapierror.IsAlreadyExists(err) {
				return nil
			}
			return fmt.Errorf("creating secret used by scan job failed: %s: %w", secret.Namespace+"/"+secret.Name, err)
		}
	}

	err = r.Client.Create(ctx, scanJob)
	if err != nil {
		if k8sapierror.IsAlreadyExists(err) {
			// TODO Delete secrets that were created in the previous step. Alternatively we can delete them on schedule.
			return nil
		}
		return fmt.Errorf("creating scan job failed: %s: %w", scanJob.Namespace+"/"+scanJob.Name, err)
	}

	for _, secret := range secrets {
		err = controllerutil.SetOwnerReference(scanJob, secret, r.Client.Scheme())
		if err != nil {
			return fmt.Errorf("setting owner reference: %w", err)
		}
		err := r.Client.Update(ctx, secret)
		if err != nil {
			return fmt.Errorf("setting owner reference of secret used by scan job failed: %s: %w", secret.Namespace+"/"+secret.Name, err)
		}
	}

	return nil
}

func (r *WorkloadController) reconcileJobs() reconcile.Func {
	return func(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
		log := r.Logger.WithValues("job", req.NamespacedName)

		job := &batchv1.Job{}
		err := r.Client.Get(ctx, req.NamespacedName, job)
		if err != nil {
			if k8sapierror.IsNotFound(err) {
				log.V(1).Info("Ignoring cached job that must have been deleted")
				return ctrl.Result{}, nil
			}
			return ctrl.Result{}, fmt.Errorf("getting job from cache: %w", err)
		}

		if len(job.Status.Conditions) == 0 {
			log.V(1).Info("Ignoring Job without conditions")
			return ctrl.Result{}, nil
		}

		switch jobCondition := job.Status.Conditions[0].Type; jobCondition {
		case batchv1.JobComplete:
			err = r.processCompleteScanJob(ctx, job)
		case batchv1.JobFailed:
			err = r.processFailedScanJob(ctx, job)
		default:
			err = fmt.Errorf("unrecognized scan job condition: %v", jobCondition)
		}

		return ctrl.Result{}, err
	}

}

func (r *WorkloadController) processCompleteScanJob(ctx context.Context, job *batchv1.Job) error {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", job.Namespace, job.Name))

	ownerRef, err := kube.ObjectRefFromObjectMeta(job.ObjectMeta)
	if err != nil {
		return fmt.Errorf("getting owner ref from scan job metadata: %w", err)
	}

	owner, err := r.ObjectFromObjectRef(ctx, ownerRef)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Report owner must have been deleted", "owner", owner)
			return r.deleteJob(ctx, job)
		}
		return fmt.Errorf("getting object from object ref: %w", err)
	}

	containerImages, err := kube.GetContainerImagesFromJob(job)
	if err != nil {
		return fmt.Errorf("getting container images: %w", err)
	}

	podSpecHash, ok := job.Labels[trivyoperator.LabelResourceSpecHash]
	if !ok {
		return fmt.Errorf("expected label %s not set", trivyoperator.LabelResourceSpecHash)
	}

	hasReports, err := r.hasReports(ctx, ownerRef, podSpecHash, containerImages)
	if err != nil {
		return err
	}

	if hasReports {
		log.V(1).Info("VulnerabilityReports already exist", "owner", owner)
		log.V(1).Info("Deleting complete scan job", "owner", owner)
		return r.deleteJob(ctx, job)
	}

	var vulnerabilityReports []v1alpha1.VulnerabilityReport
	var secretReports []v1alpha1.ExposedSecretReport

	var merr error
	for containerName, containerImage := range containerImages {
		vulnReports, secReports, err := r.processScanJobResults(ctx, job, containerName, containerImage, owner)
		if err != nil {
			merr = multierr.Append(merr, err)
		}
		vulnerabilityReports = append(vulnerabilityReports, vulnReports...)
		secretReports = append(secretReports, secReports...)
	}
	if merr != nil {
		return merr
	}

	if r.Config.VulnerabilityScannerEnabled {
		err = r.VulnerabilityReadWriter.Write(ctx, vulnerabilityReports)
		if err != nil {
			return err
		}
	}

	if r.Config.ExposedSecretScannerEnabled {
		err = r.ExposedSecretReadWriter.Write(ctx, secretReports)
		if err != nil {
			return err
		}
	}

	log.V(1).Info("Deleting complete scan job", "owner", owner)
	return r.deleteJob(ctx, job)
}

func (r *WorkloadController) processScanJobResults(ctx context.Context, job *batchv1.Job, containerName, containerImage string, owner client.Object) ([]v1alpha1.VulnerabilityReport, []v1alpha1.ExposedSecretReport, error) {
	log := r.Logger.WithValues("job-results-processor", fmt.Sprintf("%s/%s", job.Namespace, job.Name))

	var vulnerabilityReports []v1alpha1.VulnerabilityReport
	var secretReports []v1alpha1.ExposedSecretReport

	podSpecHash, ok := job.Labels[trivyoperator.LabelResourceSpecHash]
	if !ok {
		return nil, nil, fmt.Errorf("expected label %s not set", trivyoperator.LabelResourceSpecHash)
	}

	logsStream, err := r.LogsReader.GetLogsByJobAndContainerName(ctx, job, containerName)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Cached job must have been deleted")
			return nil, nil, nil
		}
		if kube.IsPodControlledByJobNotFound(err) {
			log.V(1).Info("Pod must have been deleted")
			return nil, nil, r.deleteJob(ctx, job)
		}
		return nil, nil, fmt.Errorf("getting logs for pod %q: %w", job.Namespace+"/"+job.Name, err)
	}

	defer func() {
		err := logsStream.Close()
		if err != nil {
			log.V(1).Error(err, "could not close log stream")
		}
	}()

	vulnReportData, secretReportData, err := r.Plugin.ParseReportData(r.PluginContext, containerImage, logsStream)
	if err != nil {
		return nil, nil, err
	}

	resourceLabelsToInclude := r.GetReportResourceLabels()

	reportBuilder := NewReportBuilder(r.Client.Scheme()).
		Controller(owner).
		Container(containerName).
		Data(vulnReportData).
		PodSpecHash(podSpecHash).
		ResourceLabelsToInclude(resourceLabelsToInclude)

	if r.Config.ScannerReportTTL != nil {
		reportBuilder.ReportTTL(r.Config.ScannerReportTTL)
	}

	report, err := reportBuilder.Get()
	if err != nil {
		return nil, nil, err
	}

	secretReportBuilder := exposedsecretreport.NewReportBuilder(r.Client.Scheme()).
		Controller(owner).
		Container(containerName).
		Data(secretReportData).
		PodSpecHash(podSpecHash).
		ResourceLabelsToInclude(resourceLabelsToInclude)
	if r.Config.ScannerReportTTL != nil {
		secretReportBuilder.ReportTTL(r.Config.ScannerReportTTL)
	}
	secretReport, err := secretReportBuilder.Get()
	if err != nil {
		return nil, nil, err
	}

	vulnerabilityReports = append(vulnerabilityReports, report)
	secretReports = append(secretReports, secretReport)

	return vulnerabilityReports, secretReports, nil
}

func (r *WorkloadController) processFailedScanJob(ctx context.Context, scanJob *batchv1.Job) error {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", scanJob.Namespace, scanJob.Name))

	statuses, err := r.GetTerminatedContainersStatusesByJob(ctx, scanJob)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Cached job must have been deleted")
			return nil
		}
		if kube.IsPodControlledByJobNotFound(err) {
			log.V(1).Info("Pod must have been deleted")
			return r.deleteJob(ctx, scanJob)
		}
		return err
	}
	for container, status := range statuses {
		if status.ExitCode == 0 {
			continue
		}
		log.Error(nil, "Scan job container", "container", container, "status.reason", status.Reason, "status.message", status.Message)
	}
	log.V(1).Info("Deleting failed scan job")
	return r.deleteJob(ctx, scanJob)
}

func (r *WorkloadController) deleteJob(ctx context.Context, job *batchv1.Job) error {
	err := r.Client.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground))
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			return nil
		}
		return fmt.Errorf("deleting job: %w", err)
	}
	return nil
}
