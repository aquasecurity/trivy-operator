package controller

import (
	"context"
	"crypto/sha256"
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/go-logr/logr"
	batchv1 "k8s.io/api/batch/v1"
	k8sapierror "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/types"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"

	"github.com/aquasecurity/trivy-operator/pkg/apis/aquasecurity/v1alpha1"
	"github.com/aquasecurity/trivy-operator/pkg/docker"
	"github.com/aquasecurity/trivy-operator/pkg/exposedsecretreport"
	"github.com/aquasecurity/trivy-operator/pkg/kube"
	"github.com/aquasecurity/trivy-operator/pkg/operator/etc"
	"github.com/aquasecurity/trivy-operator/pkg/operator/jobs"
	"github.com/aquasecurity/trivy-operator/pkg/operator/workload"
	"github.com/aquasecurity/trivy-operator/pkg/plugins/trivy"
	"github.com/aquasecurity/trivy-operator/pkg/sbomreport"
	"github.com/aquasecurity/trivy-operator/pkg/trivyoperator"
	"github.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport"

	. "github.com/aquasecurity/trivy-operator/pkg/operator/predicate"
)

const trivyServerUp = "trivy_server_up"

// WorkloadController watches Kubernetes workloads and generates
// v1alpha1.VulnerabilityReport instances using vulnerability scanner that that
// implements the Plugin interface.
type WorkloadController struct {
	logr.Logger
	etc.Config
	kube.ObjectResolver
	client.Client
	jobs.LimitChecker
	vulnerabilityreport.Plugin
	trivyoperator.PluginContext
	kube.SecretsReader
	trivyoperator.ConfigData
	ServerHealthChecker     ServerHealthChecker
	VulnerabilityReadWriter vulnerabilityreport.ReadWriter
	ExposedSecretReadWriter exposedsecretreport.ReadWriter
	SbomReadWriter          sbomreport.ReadWriter
	SubmitScanJobChan       chan ScanJobRequest
	ResultScanJobChan       chan ScanJobResult
	CacheSyncTimeout        time.Duration
}

// ScanJobResult encapsulate processing result and error
type ScanJobResult struct {
	Result ctrl.Result
	Error  error
}

// ScanJobRequest encapsulate workload and context for processing
type ScanJobRequest struct {
	Workload          client.Object
	Context           context.Context
	ClusterSbomReport map[string]v1alpha1.SbomReportData
}

// +kubebuilder:rbac:groups="",resources=pods,verbs=get;list;watch
// +kubebuilder:rbac:groups=apps,resources=replicasets,verbs=get;list;watch
// +kubebuilder:rbac:groups="",resources=replicationcontrollers,verbs=get;list;watch
// +kubebuilder:rbac:groups=apps,resources=statefulsets,verbs=get;list;watch
// +kubebuilder:rbac:groups=apps,resources=daemonsets,verbs=get;list;watch
// +kubebuilder:rbac:groups=batch,resources=cronjobs,verbs=get;list;watch
// +kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch
// +kubebuilder:rbac:groups=aquasecurity.github.io,resources=vulnerabilityreports,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=aquasecurity.github.io,resources=exposedsecretreports,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=aquasecurity.github.io,resources=sbomreports,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=aquasecurity.github.io,resources=clustersbomreports,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=aquasecurity.github.io,resources=imagevulnerabilityreports,verbs=get;list;watch;create;update;patch;delete

// Manage scan jobs with image pull secrets
// kubebuilder:rbac:groups="",resources=secrets,verbs=create;update

func (r *WorkloadController) SetupWithManager(mgr ctrl.Manager) error {
	installModePredicate, err := InstallModePredicate(r.Config)
	if err != nil {
		return err
	}

	var resources []kube.Resource

	// Determine which Kubernetes workloads the controller will reconcile and add them to resources
	targetWorkloads := r.Config.GetTargetWorkloads()
	for _, tw := range targetWorkloads {
		var resource kube.Resource
		err := resource.GetWorkloadResource(tw, nil, r.ObjectResolver)
		if err != nil {
			return err
		}
		resources = append(resources, resource)
	}

	for _, resource := range resources {
		err = ctrl.NewControllerManagedBy(mgr).WithOptions(controller.Options{
			CacheSyncTimeout: r.CacheSyncTimeout,
		}).
			For(resource.ForObject, builder.WithPredicates(
				Not(ManagedByTrivyOperator),
				Not(IsBeingTerminated),
				installModePredicate,
			)).
			Owns(&v1alpha1.VulnerabilityReport{}).
			Owns(&v1alpha1.ExposedSecretReport{}).
			Owns(&v1alpha1.SbomReport{}).
			Complete(r.reconcileWorkload(resource.Kind))
		if err != nil {
			return err
		}
	}
	// process scan jobs
	go r.ProcessScanJob()
	return nil
}

func (r *WorkloadController) reconcileWorkload(workloadKind kube.Kind) reconcile.Func {
	return func(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
		log := r.Logger.WithValues("kind", workloadKind, "name", req.NamespacedName)
		workloadRef := kube.ObjectRefFromKindAndObjectKey(workloadKind, req.NamespacedName)
		log.V(1).Info("Getting workload from cache", "workload", workloadRef.Name)
		workloadObj, err := r.ObjectFromObjectRef(ctx, workloadRef)
		if err != nil {
			if k8sapierror.IsNotFound(err) {
				log.V(1).Info("Ignoring cached workload that must have been deleted", "workload", workloadRef.Name)
				return ctrl.Result{}, nil
			}
			return ctrl.Result{}, fmt.Errorf("getting %s from cache: %w", workloadKind, err)
		}

		// Skip processing if it's a Pod controlled by a built-in K8s workload.
		if skip, err := workload.SkipProcessing(ctx, workloadObj, r.ObjectResolver,
			r.Config.VulnerabilityScannerScanOnlyCurrentRevisions, log, r.ConfigData.GetSkipResourceByLabels()); skip {
			return ctrl.Result{}, err
		}

		podSpec, err := kube.GetPodSpec(workloadObj)
		if err != nil {
			return ctrl.Result{}, err
		}

		containerImages := kube.GetContainerImagesFromPodSpec(podSpec, r.GetSkipInitContainers())
		hash := kube.ComputeHash(podSpec)

		hasVulnReports := true
		if r.Config.VulnerabilityScannerEnabled {
			hasVulnReports, err = hasVulnerabilityReports(ctx, r.VulnerabilityReadWriter, workloadRef, hash, containerImages)
			if err != nil {
				return ctrl.Result{}, err
			}
		}
		hasExposedSecretReports := true
		if r.Config.ExposedSecretScannerEnabled {
			hasExposedSecretReports, err = hasSecretReports(ctx, r.ExposedSecretReadWriter, workloadRef, hash, containerImages)
			if err != nil {
				return ctrl.Result{}, err
			}
		}

		if hasVulnReports && hasExposedSecretReports {
			log.V(1).Info("VulnerabilityReports or Secret Reports already exist for the workload", "workload", workloadRef.Name)
			return ctrl.Result{}, nil
		}

		// Check for cached ImageVulnerabilityReport for each container image if cache is enabled
		if r.Config.VulnerabilityScannerEnabled && r.ConfigData.VulnerabilityCacheEnabled() {
			if err := r.processCachedVulnerabilityReports(ctx, workloadObj, containerImages, log); err != nil {
				return ctrl.Result{}, err
			}
			// If all containers have stub reports, we can return early
			if len(containerImages) == 0 {
				log.V(1).Info("All containers have cached reports, skipping scan job submission", "workload", workloadRef.Name)
				return ctrl.Result{}, nil
			}
		}

		var reportsData map[string]v1alpha1.SbomReportData
		if hasActive, job, err := r.hasActiveScanJob(ctx, workloadRef, hash); err != nil {
			return ctrl.Result{}, fmt.Errorf("checking scan job: %w", err)
		} else if hasActive {
			log.V(1).Info("Scan job already exists", "job", fmt.Sprintf("%s/%s", job.Namespace, job.Name))
			return ctrl.Result{}, nil
		}

		if r.BuiltInTrivyServer {
			if available, err := r.ServerHealthChecker.TrivyServerAvaliable(r.GeTrivyServerURL()); err != nil {
				return ctrl.Result{}, err
			} else if !available {
				return ctrl.Result{RequeueAfter: r.Config.ScanJobRetryAfter}, nil
			}
		}

		if r.Config.ClusterSbomCacheEnable {
			reportsData = getGlobalSbomReports(ctx, r.SbomReadWriter, containerImages, log)
			if len(reportsData) > 0 {
				if err := r.reuseSbomReport(ctx, workloadObj, reportsData); err != nil {
					log.V(1).Info("unable to reuse report for image containers from cached sbom - running standard scan-job", "image containers", containerImages)
					reportsData = make(map[string]v1alpha1.SbomReportData)
				}
			}
		}

		log.V(1).Info("Submitting a scan for the workload", "workload", workloadRef.Name)
		// Sync all potential workload for scanning
		r.SubmitScanJobChan <- ScanJobRequest{Workload: workloadObj, Context: ctx, ClusterSbomReport: reportsData}
		// Collect scan job processing results
		scanJobResult := <-r.ResultScanJobChan
		return scanJobResult.Result, scanJobResult.Error
	}
}

// processCachedVulnerabilityReports checks for cached ImageVulnerabilityReports for container images
// and creates stub reports if found and not expired, removing processed containers from the map.
func (r *WorkloadController) processCachedVulnerabilityReports(ctx context.Context, workloadObj client.Object, containerImages map[string]string, log logr.Logger) error {
	for containerName, image := range containerImages {
		digest := ""
		if parts := strings.Split(image, "@"); len(parts) > 1 {
			digest = parts[1]
		}
		ivar := &v1alpha1.ImageVulnerabilityReport{}
		var cacheName string
		if digest != "" {
			cacheName = digestToName(digest)
			log.V(1).Info("Digest found in image reference, using it for cache lookup", "container", containerName, "image", image, "digest", digest, "cacheName", cacheName)
		} else {
			cacheName = imageToName(image)
			log.V(1).Info("No digest found in image reference, using full image reference for cache lookup. To avoid unnecessary scans, ensure images are referenced with digests in manifests (e.g., image:tag@sha256:digest)", "container", containerName, "image", image, "cacheName", cacheName)
		}
		err := r.Client.Get(ctx, types.NamespacedName{Name: cacheName}, ivar)
		switch {
		case err == nil && !isExpired(ivar):
			log.V(1).Info("Cache hit for ImageVulnerabilityReport, creating stub report", "container", containerName, "digest", digest)
			if err = r.createStubVulnReport(ctx, workloadObj, containerName, ivar); err != nil {
				log.Error(err, "Failed to create stub VulnerabilityReport, falling back to scan", "container", containerName)
			} else {
				delete(containerImages, containerName)
			}
		case err == nil:
			log.V(1).Info("Cached ImageVulnerabilityReport exists but is expired, proceeding with scan", "container", containerName, "digest", digest, "creationTimestamp", ivar.CreationTimestamp, "ttl", ivar.Annotations["trivy-operator.aquasecurity.github.io/report-ttl"])
		case err != nil && !k8sapierror.IsNotFound(err):
			log.Error(err, "Failed to lookup ImageVulnerabilityReport", "digest", digest)
			return err
		case digest == "":
			if err := r.checkFallbackImageReferences(ctx, workloadObj, containerName, image, cacheName, log, containerImages); err != nil {
				return err
			}
		default:
			log.V(1).Info("No cached ImageVulnerabilityReport found, proceeding with scan", "container", containerName, "digest", digest)
		}
	}
	return nil
}

// checkFallbackImageReferences checks for cached reports using full image references
// when no digest is available, using label selectors for efficient lookup.
func (r *WorkloadController) checkFallbackImageReferences(ctx context.Context, workloadObj client.Object, containerName, image, cacheName string, log logr.Logger, containerImages map[string]string) error {
	ivarList := &v1alpha1.ImageVulnerabilityReportList{}
	listOptions := &client.ListOptions{}
	labelHash := computeHashForLabel(cacheName)
	selector := labels.SelectorFromSet(labels.Set{
		"trivy-operator.cache.full-image-ref-hash": labelHash,
	})
	listOptions.LabelSelector = selector
	log.V(1).Info("Using label selector for cache lookup", "container", containerName, "labelHash", labelHash)
	err := r.Client.List(ctx, ivarList, listOptions)
	if err == nil {
		matched := false
		for _, report := range ivarList.Items {
			if report.Annotations["trivy-operator.cache.full-image-ref-name"] == cacheName {
				matched = true
				if !isExpired(&report) {
					log.V(1).Info("Cache hit via label selector and annotation match, creating stub report", "container", containerName, "reportName", report.Name)
					if err = r.createStubVulnReport(ctx, workloadObj, containerName, &report); err != nil {
						log.Error(err, "Failed to create stub VulnerabilityReport from label match, falling back to scan", "container", containerName)
					} else {
						delete(containerImages, containerName)
					}
				} else {
					log.V(1).Info("Cached ImageVulnerabilityReport via label exists but is expired, proceeding with scan", "container", containerName, "reportName", report.Name)
				}
				break
			}
		}
		if !matched {
			log.V(1).Info("No matching ImageVulnerabilityReport found via label selector for full image reference", "container", containerName, "cacheName", cacheName)
		}
	} else {
		log.Error(err, "Failed to list ImageVulnerabilityReports for full image reference lookup with label selector", "image", image)
	}

	if _, stillInMap := containerImages[containerName]; stillInMap {
		log.V(1).Info("No cached ImageVulnerabilityReport found, proceeding with scan", "container", containerName)
	}
	return nil
}

func (r *WorkloadController) ProcessScanJob() {
	for workloadRequest := range r.SubmitScanJobChan {
		log := r.Logger.WithValues("pre scan job processing for workload:", workloadRequest.Workload.GetName())
		limitExceeded, scanJobsCount, err := r.LimitChecker.Check(workloadRequest.Context)
		if err != nil {
			r.ResultScanJobChan <- ScanJobResult{Result: ctrl.Result{}, Error: err}
			continue
		}
		log.V(1).Info("Checking scan jobs limit", "count", scanJobsCount, "limit", r.ConcurrentScanJobsLimit)

		if limitExceeded {
			log.V(1).Info("Pushing back scan job", "count", scanJobsCount, "retryAfter", r.ScanJobRetryAfter)
			r.ResultScanJobChan <- ScanJobResult{Result: ctrl.Result{RequeueAfter: r.Config.ScanJobRetryAfter}, Error: nil}
			continue
		}
		err = r.SubmitScanJob(workloadRequest.Context, workloadRequest.Workload, workloadRequest.ClusterSbomReport)
		r.ResultScanJobChan <- ScanJobResult{Result: ctrl.Result{}, Error: err}
	}
}

func (r *WorkloadController) hasActiveScanJob(ctx context.Context, owner kube.ObjectRef, hash string) (bool, *batchv1.Job, error) {
	jobName := fmt.Sprintf("scan-vulnerabilityreport-%s", kube.ComputeHash(owner))
	job := &batchv1.Job{}
	err := r.Get(ctx, client.ObjectKey{Namespace: r.Config.Namespace, Name: jobName}, job)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			return false, nil, nil
		}
		return false, nil, fmt.Errorf("getting job from cache: %w", err)
	}
	if job.Labels[trivyoperator.LabelResourceSpecHash] == hash {
		return true, job, nil
	}
	return false, nil, nil
}

func (r *WorkloadController) SubmitScanJob(ctx context.Context, owner client.Object, reusedReports map[string]v1alpha1.SbomReportData) error {

	log := r.Logger.WithValues("kind", owner.GetObjectKind().GroupVersionKind().Kind,
		"name", owner.GetName(), "namespace", owner.GetNamespace())
	var err error

	credentials := make(map[string]docker.Auth, 0)
	if len(reusedReports) == 0 {
		privateRegistrySecrets, err := r.Config.GetPrivateRegistryScanSecretsNames()
		if err != nil {
			return err
		}

		pConfig, err := r.PluginContext.GetConfig()
		if err != nil {
			return err
		}

		multiSecretSupport := trivy.MultiSecretSupport(trivy.Config{PluginConfig: pConfig})

		if r.AccessGlobalSecretsAndServiceAccount {
			// Global access is enabled - therefore imagePullSecrets from the podSpec can be used
			credentials, err = r.CredentialsByServer(ctx, owner, privateRegistrySecrets, multiSecretSupport, true)
			if err != nil {
				return err
			}
		} else {
			// Global access is disabled - check if privateRegistrySecrets references an imagePullSecret in the operator namespace
			imagePullSecretName, ok := privateRegistrySecrets[r.Config.Namespace]
			if ok {
				// privateRegistrySecrets references an imagePullSecret in the operator namespace - use this for pulling private images
				credentials, err = r.CredentialsByServer(ctx, owner, map[string]string{r.Config.Namespace: imagePullSecretName}, multiSecretSupport, false)
				if err != nil {
					return err
				}
			}
		}
	}

	scanJobAffinity, err := r.GetScanJobAffinity()
	if err != nil {
		return fmt.Errorf("getting scan job affinity: %w", err)
	}

	scanJobTolerations, err := r.GetScanJobTolerations()
	if err != nil {
		return fmt.Errorf("getting scan job tolerations: %w", err)
	}

	scanJobAnnotations, err := r.GetScanJobAnnotations()
	if err != nil {
		return fmt.Errorf("getting scan job annotations: %w", err)
	}

	scanJobNodeSelector, err := r.GetScanJobNodeSelector()
	if err != nil {
		return fmt.Errorf("getting scan job nodeSelector: %w", err)
	}

	scanJobSecurityContext, err := r.GetScanJobPodSecurityContext()
	if err != nil {
		return fmt.Errorf("getting scan job podSecurityContext: %w", err)
	}

	scanJobContainerSecurityContext, err := r.GetScanJobContainerSecurityContext()
	if err != nil {
		return fmt.Errorf("getting scan job [container] securityContext: %w", err)
	}

	scanJobPodTemplateLabels, err := r.GetScanJobPodTemplateLabels()
	if err != nil {
		return fmt.Errorf("getting scan job template labels: %w", err)
	}

	scanJobPodPriorityClassName, err := r.GetScanJobPodPriorityClassName()
	if err != nil {
		return fmt.Errorf("getting scan job priority class name: %w", err)
	}

	cvm, err := r.GetScanJobCustomVolumeMounts()
	if err != nil {
		return fmt.Errorf("getting scan job scan job custom volume mounts: %w", err)
	}
	cv, err := r.GetScanJobCustomVolumes()
	if err != nil {
		return fmt.Errorf("getting scan job scan job custom volume: %w", err)
	}

	scanJobBuilder := vulnerabilityreport.NewScanJobBuilder().
		WithPlugin(r.Plugin).
		WithPluginContext(r.PluginContext).
		WithTimeout(r.Config.ScanJobTimeout).
		WithTTL(r.Config.ScanJobTTL).
		WithScanSecretTTL(r.Config.ScanSecretTTL).
		WithObject(owner).
		WithAffinity(scanJobAffinity).
		WithTolerations(scanJobTolerations).
		WithAnnotations(scanJobAnnotations).
		WithNodeSelector(scanJobNodeSelector).
		WithPodSecurityContext(scanJobSecurityContext).
		WithCustomVolumes(cv).
		WithCustomVolumesMount(cvm).
		WithSecurityContext(scanJobContainerSecurityContext).
		WithSkipInitContainers(r.GetSkipInitContainers()).
		WithPodTemplateLabels(scanJobPodTemplateLabels).
		WithCredentials(credentials).
		WithPodPriorityClassName(scanJobPodPriorityClassName)
	if len(reusedReports) > 0 {
		scanJobBuilder.WithSbomClusterReports(reusedReports)
	}
	scanJob, secrets, err := scanJobBuilder.Get()

	if err != nil {
		if errors.Is(err, kube.ErrReplicaSetNotFound) || errors.Is(err, kube.ErrNoRunningPods) ||
			errors.Is(err, kube.ErrUnSupportedKind) {
			log.V(1).Info("ignoring vulnerability scan", "reason", err)
			return nil
		}
		return fmt.Errorf("constructing scan job: %w", err)
	}
	// If there are no containers in the pod spec, we don't need to create a scan job.
	if len(scanJob.Spec.Template.Spec.Containers) == 0 {
		log.V(1).Info("ignoring vulnerability scan", "no containers in the pod spec for resource", owner.GetName())
		return nil
	}

	for _, secret := range secrets {
		err = r.Client.Create(ctx, secret)
		if err != nil {
			if k8sapierror.IsAlreadyExists(err) {
				return nil
			}
			return fmt.Errorf("creating secret used by scan job failed: %s: %w", secret.Namespace+"/"+secret.Name, err)
		}
	}

	log = log.WithValues("podSpecHash", scanJob.Labels[trivyoperator.LabelResourceSpecHash])

	log.V(1).Info("Creating scan job for the workload")
	err = r.Client.Create(ctx, scanJob)
	if err != nil {
		if k8sapierror.IsAlreadyExists(err) {
			// Delete secrets that were created in the previous step
			for _, secret := range secrets {
				deleteErr := r.Client.Delete(ctx, secret)
				if deleteErr != nil && !k8sapierror.IsNotFound(deleteErr) {
					log.Error(deleteErr, "failed to delete secret", "secret", secret.Namespace+"/"+secret.Name)
				} else {
					log.V(1).Info("deleted secret after job creation failure", "secret", secret.Namespace+"/"+secret.Name)
				}
			}
			return nil
		}
		return fmt.Errorf("creating scan job failed: %s: %w", scanJob.Namespace+"/"+scanJob.Name, err)
	}

	for _, secret := range secrets {
		err = controllerutil.SetOwnerReference(scanJob, secret, r.Client.Scheme())
		if err != nil {
			return fmt.Errorf("setting owner reference: %w", err)
		}
		err := r.Client.Update(ctx, secret)
		if err != nil {
			return fmt.Errorf("setting owner reference of secret used by scan job failed: %s: %w", secret.Namespace+"/"+secret.Name, err)
		}
	}

	return nil
}

// digestToName converts an image digest to a valid resource name.
func digestToName(d string) string {
	// sha256:<64hex>  -> sha256-<first 63>
	d = strings.ReplaceAll(d, ":", "-")
	if len(d) > 252 {
		return d[:252]
	}
	return d
}

// imageToName converts a full image reference to a valid resource name by hashing if necessary.
func imageToName(image string) string {
	// Replace invalid characters for Kubernetes resource names
	image = strings.ReplaceAll(image, "/", "-")
	image = strings.ReplaceAll(image, ":", "-")
	image = strings.ReplaceAll(image, "@", "-")
	// Kubernetes resource names must be lowercase and no longer than 253 characters
	if len(image) > 252 {
		return fmt.Sprintf("img-%x", sha256.Sum256([]byte(image)))[:252]
	}
	return image
}

// isExpired checks if the ImageVulnerabilityReport has exceeded its TTL.
func isExpired(report *v1alpha1.ImageVulnerabilityReport) bool {
	ttlStr, ok := report.Annotations["trivy-operator.aquasecurity.github.io/report-ttl"]
	if !ok {
		return false
	}
	ttl, err := time.ParseDuration(ttlStr)
	if err != nil {
		return false
	}
	creationTime := report.CreationTimestamp.Time
	age := time.Since(creationTime)
	return age > ttl
}

// createStubVulnReport creates a stub VulnerabilityReport from a cached ImageVulnerabilityReport.
// This function copies essential data from the cached report to avoid a full rescan, ensuring
// consistent naming and metadata with reports generated from fresh scans.
func (r *WorkloadController) createStubVulnReport(ctx context.Context, workloadObj client.Object, containerName string, ivar *v1alpha1.ImageVulnerabilityReport) error {
	hash, err := kube.ComputeSpecHash(workloadObj)
	if err != nil {
		return fmt.Errorf("computing spec hash for stub report: %w", err)
	}
	vr := &v1alpha1.VulnerabilityReport{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-%s-%s", strings.ToLower(workloadObj.GetObjectKind().GroupVersionKind().Kind), workloadObj.GetName(), containerName),
			Namespace: workloadObj.GetNamespace(),
			Labels: map[string]string{
				trivyoperator.LabelResourceKind:      workloadObj.GetObjectKind().GroupVersionKind().Kind,
				trivyoperator.LabelResourceName:      workloadObj.GetName(),
				trivyoperator.LabelResourceNamespace: workloadObj.GetNamespace(),
				trivyoperator.LabelContainerName:     containerName,
				trivyoperator.LabelResourceSpecHash:  hash,
			},
			Annotations: map[string]string{
				"trivy-operator.cache.image-digest":                ivar.Name,
				"trivy-operator.cache.full-image-ref":              ivar.Report.Artifact.Repository + ":" + ivar.Report.Artifact.Tag,
				"trivy-operator.aquasecurity.github.io/report-ttl": r.Config.ScannerReportTTL.String(),
			},
			OwnerReferences: []metav1.OwnerReference{
				{
					APIVersion:         workloadObj.GetObjectKind().GroupVersionKind().GroupVersion().String(),
					Kind:               workloadObj.GetObjectKind().GroupVersionKind().Kind,
					Name:               workloadObj.GetName(),
					UID:                workloadObj.GetUID(),
					BlockOwnerDeletion: new(bool),
					Controller:         new(bool),
				},
			},
		},
		Report: v1alpha1.VulnerabilityReportData{
			UpdateTimestamp: metav1.Now(),
			Artifact:        ivar.Report.Artifact,
			OS:              ivar.Report.OS,
			Registry:        ivar.Report.Registry,
			Summary:         ivar.Report.Summary,
			Vulnerabilities: ivar.Report.Vulnerabilities,
			Scanner: v1alpha1.Scanner{
				Name:    "Trivy",
				Vendor:  "Aqua Security",
				Version: ivar.Report.Scanner.Version, // Use cached version if available, otherwise it will be empty
			},
		},
	}
	*vr.OwnerReferences[0].BlockOwnerDeletion = false
	*vr.OwnerReferences[0].Controller = true
	err = r.Client.Create(ctx, vr)
	if err != nil && !k8sapierror.IsAlreadyExists(err) {
		return fmt.Errorf("creating stub VulnerabilityReport: %w", err)
	}
	return nil
}

func (r *WorkloadController) reuseSbomReport(ctx context.Context, owner client.Object, sbomReportDataMap map[string]v1alpha1.SbomReportData) error {
	hash, err := kube.ComputeSpecHash(owner)
	if err != nil {
		return err
	}
	resourceLabelsToInclude := r.GetReportResourceLabels()
	additionalCustomLabels, err := r.GetAdditionalReportLabels()
	if err != nil {
		return err
	}
	sbomReports := make([]v1alpha1.SbomReport, 0)
	for containerName, sbomReportData := range sbomReportDataMap {
		sbomReportBuilder := sbomreport.NewReportBuilder(r.Client.Scheme()).
			Controller(owner).
			Container(containerName).
			Data(sbomReportData).
			PodSpecHash(hash).
			ResourceLabelsToInclude(resourceLabelsToInclude).
			AdditionalReportLabels(additionalCustomLabels)
		sbomReport, err := sbomReportBuilder.NamespacedReport()
		if err != nil {
			return err
		}
		sbomReports = append(sbomReports, sbomReport)
	}
	return r.SbomReadWriter.Write(ctx, sbomReports)
}
