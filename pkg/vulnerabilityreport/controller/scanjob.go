package controller

import (
	"context"
	"crypto/sha256"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"

	"github.com/go-logr/logr"
	"go.uber.org/multierr"
	batchv1 "k8s.io/api/batch/v1"
	k8sapierror "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"

	"github.com/aquasecurity/trivy-operator/pkg/apis/aquasecurity/v1alpha1"
	"github.com/aquasecurity/trivy-operator/pkg/exposedsecretreport"
	"github.com/aquasecurity/trivy-operator/pkg/kube"
	"github.com/aquasecurity/trivy-operator/pkg/operator/etc"
	"github.com/aquasecurity/trivy-operator/pkg/sbomreport"
	"github.com/aquasecurity/trivy-operator/pkg/trivyoperator"
	"github.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport"

	. "github.com/aquasecurity/trivy-operator/pkg/operator/predicate"
)

// ScanJobController watches Kubernetes workloads and generates
// v1alpha1.VulnerabilityReport instances using vulnerability scanner that that
// implements the Plugin interface.
type ScanJobController struct {
	logr.Logger
	etc.Config
	kube.ObjectResolver
	kube.LogsReader
	vulnerabilityreport.Plugin
	trivyoperator.PluginContext
	trivyoperator.ConfigData
	SbomReadWriter          sbomreport.ReadWriter
	VulnerabilityReadWriter vulnerabilityreport.ReadWriter
	ExposedSecretReadWriter exposedsecretreport.ReadWriter
}

// Manage scan jobs with image pull secrets
// kubebuilder:rbac:groups="",resources=secrets,verbs=create;update
// +kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch;create;delete

func (r *ScanJobController) SetupWithManager(mgr ctrl.Manager) error {
	var predicates []predicate.Predicate
	if !r.ConfigData.VulnerabilityScanJobsInSameNamespace() {
		predicates = append(predicates, InNamespace(r.Config.Namespace))
	}
	predicates = append(predicates, ManagedByTrivyOperator, IsVulnerabilityReportScan, JobHasAnyCondition)
	return ctrl.NewControllerManagedBy(mgr).
		Named("scan-job-controller").
		For(&batchv1.Job{}, builder.WithPredicates(predicates...)).
		Complete(r.reconcileJobs())
}

func (r *ScanJobController) reconcileJobs() reconcile.Func {
	return func(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
		log := r.Logger.WithValues("job", req.NamespacedName)

		job := &batchv1.Job{}
		err := r.Client.Get(ctx, req.NamespacedName, job)
		if err != nil {
			if k8sapierror.IsNotFound(err) {
				log.V(1).Info("Ignoring cached job that must have been deleted")
				return ctrl.Result{}, nil
			}
			return ctrl.Result{}, fmt.Errorf("getting job from cache: %w", err)
		}

		if len(job.Status.Conditions) == 0 {
			log.V(1).Info("Ignoring Job without conditions")
			return ctrl.Result{}, nil
		}

		switch jobCondition := job.Status.Conditions[0].Type; jobCondition {
		case batchv1.JobComplete, batchv1.JobSuccessCriteriaMet, batchv1.JobFailed, batchv1.JobFailureTarget:
			completedContainers, err := r.completedContainers(ctx, job)
			if err != nil {
				return ctrl.Result{}, r.deleteJob(ctx, job)
			}
			if len(completedContainers) == 0 {
				return ctrl.Result{}, r.deleteJob(ctx, job)
			}
			return ctrl.Result{}, r.processCompleteScanJob(ctx, job, completedContainers...)

		default:
			return ctrl.Result{}, fmt.Errorf("unrecognized scan job condition: %v", jobCondition)
		}
	}
}

func (r *ScanJobController) processCompleteScanJob(ctx context.Context, job *batchv1.Job, completedContainers ...string) error {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", job.Namespace, job.Name))
	log.V(1).Info("Processing completed scan job", "completedContainers", completedContainers)

	ownerRef, err := kube.ObjectRefFromObjectMeta(job.ObjectMeta)
	if err != nil {
		return fmt.Errorf("getting owner ref from scan job metadata: %w", err)
	}

	owner, err := r.ObjectFromObjectRef(ctx, ownerRef)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Report owner must have been deleted", "owner", owner)
			return r.deleteJob(ctx, job)
		}
		return fmt.Errorf("getting object from object ref: %w", err)
	}
	podSpecHash, ok := job.Labels[trivyoperator.LabelResourceSpecHash]
	if !ok {
		return fmt.Errorf("expected label %s not set", trivyoperator.LabelResourceSpecHash)
	}

	log = log.WithValues("kind", owner.GetObjectKind().GroupVersionKind().Kind,
		"name", owner.GetName(), "namespace", owner.GetNamespace(), "podSpecHash", podSpecHash)

	log.V(1).Info("Job complete")

	hasVulnReports := true
	containerImages, err := kube.GetContainerImagesFromJob(job, completedContainers...)
	if err != nil {
		return fmt.Errorf("getting container images: %w", err)
	}
	if r.Config.VulnerabilityScannerEnabled {
		hasVulnReports, err = hasVulnerabilityReports(ctx, r.VulnerabilityReadWriter, ownerRef, podSpecHash, containerImages)
		if err != nil {
			return err
		}
	}
	hasExposedSecretReports := true
	if r.Config.ExposedSecretScannerEnabled {
		hasExposedSecretReports, err = hasSecretReports(ctx, r.ExposedSecretReadWriter, ownerRef, podSpecHash, containerImages)
		if err != nil {
			return err
		}
	}
	if hasVulnReports && hasExposedSecretReports {
		log.V(1).Info("VulnerabilityReports already exist", "owner", owner)
		log.V(1).Info("Deleting complete scan job", "owner", owner)
		return r.deleteJob(ctx, job)
	}

	var vulnerabilityReports []v1alpha1.VulnerabilityReport
	var clusterVulnerabilityReports []v1alpha1.ClusterVulnerabilityReport
	var secretReports []v1alpha1.ExposedSecretReport
	var sbomNameSpacedReports []v1alpha1.SbomReport
	var sbomClusterReports []v1alpha1.ClusterSbomReport

	var merr error
	for containerName, containerImage := range containerImages {
		vulnReports, secReports, sbomReports, err := r.processScanJobResults(ctx, job, containerName, containerImage, owner)
		if err != nil {
			merr = multierr.Append(merr, err)
			continue
		}
		if vulnReports.vulnerabilityNamespaceReports != nil {
			vulnerabilityReports = append(vulnerabilityReports, *vulnReports.vulnerabilityNamespaceReports)
		}
		if vulnReports.vulnerabilityClusterReports != nil {
			clusterVulnerabilityReports = append(clusterVulnerabilityReports, *vulnReports.vulnerabilityClusterReports)
		}
		secretReports = append(secretReports, secReports...)
		if sbomReports != nil {
			sbomNameSpacedReports = append(sbomNameSpacedReports, sbomReports.sbomNamespaceReports...)
			sbomClusterReports = append(sbomClusterReports, sbomReports.sbomClusterReports...)
		}
	}
	if merr != nil {
		return merr
	}

	if r.Config.VulnerabilityScannerEnabled {
		if len(vulnerabilityReports) > 0 {
			err = r.VulnerabilityReadWriter.Write(ctx, vulnerabilityReports)
			if err != nil {
				return err
			}
		}
		if len(clusterVulnerabilityReports) > 0 {
			err = r.VulnerabilityReadWriter.WriteCluster(ctx, clusterVulnerabilityReports)
			if err != nil {
				return err
			}
		}
	}

	if r.Config.ExposedSecretScannerEnabled {
		err = r.ExposedSecretReadWriter.Write(ctx, secretReports)
		if err != nil {
			return err
		}
	}

	if r.Config.SbomGenerationEnable {
		err = r.SbomReadWriter.Write(ctx, sbomNameSpacedReports)
		if err != nil {
			return err
		}
	}
	if r.Config.ClusterSbomCacheEnable {
		if !r.Config.AltReportStorageEnabled || r.Config.AltReportDir == "" {
			err = r.SbomReadWriter.WriteCluster(ctx, sbomClusterReports)
			if err != nil {
				return err
			}
		}
	}

	log.V(1).Info("Deleting complete scan job", "owner", owner)
	return r.deleteJob(ctx, job)
}

type SbomReports struct {
	sbomNamespaceReports []v1alpha1.SbomReport
	sbomClusterReports   []v1alpha1.ClusterSbomReport
}

type VulnerabilityReports struct {
	vulnerabilityNamespaceReports *v1alpha1.VulnerabilityReport
	vulnerabilityClusterReports   *v1alpha1.ClusterVulnerabilityReport
}

func (r *ScanJobController) processScanJobResults(ctx context.Context,
	job *batchv1.Job,
	containerName,
	containerImage string,
	owner client.Object) (VulnerabilityReports, []v1alpha1.ExposedSecretReport, *SbomReports, error) {
	log := r.Logger.WithValues("job-results-processor", fmt.Sprintf("%s/%s", job.Namespace, job.Name))

	podSpecHash, ok := job.Labels[trivyoperator.LabelResourceSpecHash]
	if !ok {
		return VulnerabilityReports{}, nil, nil, fmt.Errorf("expected label %s not set", trivyoperator.LabelResourceSpecHash)
	}

	logsStream, err := r.LogsReader.GetLogsByJobAndContainerName(ctx, job, containerName)
	if err != nil {
		return r.handleLogsError(ctx, log, job, err)
	}
	defer r.closeLogsStream(log, logsStream)

	vulnReportData, secretReportData, sbomReportData, err := r.Plugin.ParseReportData(r.PluginContext, containerImage, logsStream)
	if err != nil {
		return VulnerabilityReports{}, nil, nil, err
	}

	resourceLabelsToInclude := r.GetReportResourceLabels()
	additionalCustomLabels, err := r.GetAdditionalReportLabels()
	if err != nil {
		return VulnerabilityReports{}, nil, nil, err
	}

	var vulnerabilityReports VulnerabilityReports
	if r.Config.VulnerabilityScannerEnabled {
		vulnerabilityReports, err = r.buildVulnerabilityReports(ctx, log, owner, containerName, containerImage, vulnReportData, podSpecHash, resourceLabelsToInclude, additionalCustomLabels, job)
		if err != nil {
			return VulnerabilityReports{}, nil, nil, err
		}
	}

	var secretReports []v1alpha1.ExposedSecretReport
	_, reused := job.Labels[trivyoperator.LabelReusedReport]
	if r.Config.ExposedSecretScannerEnabled && !reused {
		secretReports, err = r.buildSecretReports(owner, containerName, secretReportData, podSpecHash, resourceLabelsToInclude, additionalCustomLabels)
		if err != nil {
			return VulnerabilityReports{}, nil, nil, err
		}
	}

	sbomReports := &SbomReports{}
	if r.Config.SbomGenerationEnable && sbomReportData != nil && !reused {
		sbomReports, err = r.buildSbomReports(owner, containerName, *sbomReportData, podSpecHash, resourceLabelsToInclude, additionalCustomLabels)
		if err != nil {
			return VulnerabilityReports{}, nil, nil, err
		}
	}

	if r.Config.AltReportStorageEnabled && r.Config.AltReportDir != "" {
		return r.writeReportsToAlternateStorage(log, owner, containerName, vulnerabilityReports, secretReports, sbomReports)
	}

	return vulnerabilityReports, secretReports, sbomReports, nil
}

func (r *ScanJobController) handleLogsError(ctx context.Context, log logr.Logger, job *batchv1.Job, err error) (VulnerabilityReports, []v1alpha1.ExposedSecretReport, *SbomReports, error) {
	if k8sapierror.IsNotFound(err) {
		log.V(1).Info("Cached job must have been deleted")
		return VulnerabilityReports{}, nil, nil, nil
	}
	if kube.IsPodControlledByJobNotFound(err) {
		log.V(1).Info("Pod must have been deleted")
		return VulnerabilityReports{}, nil, nil, r.deleteJob(ctx, job)
	}
	return VulnerabilityReports{}, nil, nil, fmt.Errorf("getting logs for pod %q: %w", job.Namespace+"/"+job.Name, err)
}

func (r *ScanJobController) closeLogsStream(log logr.Logger, logsStream io.Closer) {
	err := logsStream.Close()
	if err != nil {
		log.V(1).Error(err, "could not close log stream")
	}
}

func (r *ScanJobController) buildVulnerabilityReports(ctx context.Context, log logr.Logger, owner client.Object, containerName, containerImage string, vulnReportData v1alpha1.VulnerabilityReportData, podSpecHash string, resourceLabelsToInclude []string, additionalCustomLabels map[string]string, job *batchv1.Job) (VulnerabilityReports, error) {
	reportBuilder := vulnerabilityreport.NewReportBuilder(r.Client.Scheme()).
		Controller(owner).
		Container(containerName).
		Data(vulnReportData).
		PodSpecHash(podSpecHash).
		ResourceLabelsToInclude(resourceLabelsToInclude).
		AdditionalReportLabels(additionalCustomLabels)

	if r.Config.ScannerReportTTL != nil {
		reportBuilder.ReportTTL(r.Config.ScannerReportTTL)
	}

	report, clusterReport, err := reportBuilder.Get()
	if err != nil {
		return VulnerabilityReports{}, err
	}
	vulnerabilityReports := VulnerabilityReports{vulnerabilityNamespaceReports: report, vulnerabilityClusterReports: clusterReport}

	if report != nil && r.ConfigData.VulnerabilityCacheEnabled() {
		err = r.updateImageVulnerabilityReport(ctx, log, vulnReportData, containerName, containerImage, job)
		if err != nil {
			log.Error(err, "Failed to update ImageVulnerabilityReport")
		}
	} else if report != nil {
		log.V(1).Info("Vulnerability cache is disabled, skipping ImageVulnerabilityReport creation", "container", containerName, "image", containerImage)
	}

	return vulnerabilityReports, nil
}

func (r *ScanJobController) updateImageVulnerabilityReport(ctx context.Context, log logr.Logger, vulnReportData v1alpha1.VulnerabilityReportData, containerName, containerImage string, job *batchv1.Job) error {
	log.V(1).Info("Vulnerability cache is enabled, attempting to create or update ImageVulnerabilityReport")
	imageDigest := vulnReportData.Artifact.Digest
	var ivarName string
	if imageDigest == "" {
		log.V(1).Info("No image digest found in scan data, using fallback to full image reference", "container", containerName, "image", containerImage)
	} else {
		log.V(1).Info("Image digest found from scan data, using it for caching", "digest", imageDigest)
		ivarName = digestToResourceName(imageDigest)
	}

	fullImageRef := vulnReportData.Artifact.Repository + ":" + vulnReportData.Artifact.Tag
	fullImageRefName := imageToResourceName(fullImageRef)

	if imageDigest != "" && job.Annotations != nil {
		if containerImages, ok := job.Annotations["trivy-operator.container-images"]; ok {
			var containerImagesMap map[string]string
			if err := json.Unmarshal([]byte(containerImages), &containerImagesMap); err == nil {
				if originalImage, exists := containerImagesMap[containerName]; exists && !strings.Contains(originalImage, "@") {
					ivarName = imageToResourceName(originalImage)
					log.V(1).Info("Original image lacked digest, using full original image reference for ImageVulnerabilityReport name", "container", containerName, "originalImage", originalImage, "name", ivarName)
				}
			}
		}
	} else if imageDigest == "" {
		ivarName = imageToResourceName(fullImageRef)
		log.V(1).Info("No digest available, using full image reference for ImageVulnerabilityReport name", "container", containerName, "image", fullImageRef, "name", ivarName)
	}

	labelHash := computeHashForLabel(fullImageRefName)
	ivar := &v1alpha1.ImageVulnerabilityReport{
		ObjectMeta: metav1.ObjectMeta{
			Name: ivarName,
			Labels: map[string]string{
				"trivy-operator.cache.full-image-ref-hash": labelHash,
			},
			Annotations: map[string]string{
				"trivy-operator.aquasecurity.github.io/report-ttl": r.ConfigData.VulnerabilityCacheTTL(),
				"trivy-operator.cache.full-image-ref":              fullImageRef,
				"trivy-operator.cache.full-image-ref-name":         fullImageRefName,
			},
		},
		Report: vulnReportData,
	}

	err := r.Client.Create(ctx, ivar)
	if err != nil && k8sapierror.IsAlreadyExists(err) {
		log.V(1).Info("ImageVulnerabilityReport already exists, attempting update", "digest", imageDigest)
		existingIvar := &v1alpha1.ImageVulnerabilityReport{}
		err = r.Client.Get(ctx, client.ObjectKey{Name: ivar.Name}, existingIvar)
		if err == nil {
			ivar.ResourceVersion = existingIvar.ResourceVersion
			err = r.Client.Update(ctx, ivar, client.DryRunAll)
			if err == nil {
				err = r.Client.Update(ctx, ivar)
			}
		}
	}

	if err != nil {
		log.Error(err, "Failed to create or update ImageVulnerabilityReport", "digest", imageDigest)
		return err
	}

	log.V(1).Info("Successfully cached ImageVulnerabilityReport", "digest", imageDigest)
	return nil
}

func (r *ScanJobController) buildSecretReports(owner client.Object, containerName string, secretReportData v1alpha1.ExposedSecretReportData, podSpecHash string, resourceLabelsToInclude []string, additionalCustomLabels map[string]string) ([]v1alpha1.ExposedSecretReport, error) {
	secretReport, err := exposedsecretreport.NewReportBuilder(r.Client.Scheme()).
		Controller(owner).
		Container(containerName).
		Data(secretReportData).
		PodSpecHash(podSpecHash).
		ResourceLabelsToInclude(resourceLabelsToInclude).
		AdditionalReportLabels(additionalCustomLabels).
		Get()
	if err != nil {
		return nil, err
	}
	return []v1alpha1.ExposedSecretReport{secretReport}, nil
}

func (r *ScanJobController) buildSbomReports(owner client.Object, containerName string, sbomReportData v1alpha1.SbomReportData, podSpecHash string, resourceLabelsToInclude []string, additionalCustomLabels map[string]string) (*SbomReports, error) {
	sbomReportBuilder := sbomreport.NewReportBuilder(r.Client.Scheme()).
		Controller(owner).
		Container(containerName).
		Data(sbomReportData).
		PodSpecHash(podSpecHash).
		CacheTTL(r.Config.CacheReportTTL).
		ResourceLabelsToInclude(resourceLabelsToInclude).
		AdditionalReportLabels(additionalCustomLabels)
	sbomReport, clusterReport, err := sbomReportBuilder.Get()
	if err != nil {
		return nil, err
	}
	sbomReports := &SbomReports{}
	if r.Config.ClusterSbomCacheEnable {
		sbomReports.sbomClusterReports = []v1alpha1.ClusterSbomReport{clusterReport}
	}
	sbomReports.sbomNamespaceReports = []v1alpha1.SbomReport{sbomReport}
	return sbomReports, nil
}

func (r *ScanJobController) writeReportsToAlternateStorage(log logr.Logger, owner client.Object, containerName string, vulnerabilityReports VulnerabilityReports, secretReports []v1alpha1.ExposedSecretReport, sbomReports *SbomReports) (VulnerabilityReports, []v1alpha1.ExposedSecretReport, *SbomReports, error) {
	log.V(1).Info("Writing vulnerability reports to alternate storage", "dir", r.Config.AltReportDir)
	reportDir := r.Config.AltReportDir
	clusterVulnerabilityDir := filepath.Join(reportDir, "cluster_vulnerability_reports")
	vulnerabilityDir := filepath.Join(reportDir, "vulnerability_reports")
	secretDir := filepath.Join(reportDir, "secret_reports")
	clusterSbomDir := filepath.Join(reportDir, "cluster_sbom_reports")
	sbomDir := filepath.Join(reportDir, "sbom_reports")

	for _, dir := range []string{vulnerabilityDir, secretDir, sbomDir, clusterSbomDir, clusterVulnerabilityDir} {
		if err := os.MkdirAll(dir, 0o750); err != nil {
			return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to make directory %s: %w", dir, err)
		}
	}

	workloadKind := owner.GetObjectKind().GroupVersionKind().Kind
	workloadName := owner.GetName()

	if vulnerabilityReports.vulnerabilityClusterReports != nil {
		if err := r.writeReportToFile(clusterVulnerabilityDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName), vulnerabilityReports.vulnerabilityClusterReports); err != nil {
			return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to write cluster vulnerability report: %w", err)
		}
	}

	if vulnerabilityReports.vulnerabilityNamespaceReports != nil {
		if err := r.writeReportToFile(vulnerabilityDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName), vulnerabilityReports.vulnerabilityNamespaceReports); err != nil {
			return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to write vulnerability report: %w", err)
		}
	}

	if len(secretReports) > 0 {
		if err := r.writeReportToFile(secretDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName), secretReports); err != nil {
			return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to write exposed secrets report: %w", err)
		}
	}

	if len(sbomReports.sbomClusterReports) > 0 {
		if err := r.writeReportToFile(clusterSbomDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName), sbomReports.sbomClusterReports); err != nil {
			return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to write sbom cluster report: %w", err)
		}
	}

	if len(sbomReports.sbomNamespaceReports) > 0 {
		if err := r.writeReportToFile(sbomDir, fmt.Sprintf("%s-%s-%s.json", workloadKind, workloadName, containerName), sbomReports.sbomNamespaceReports); err != nil {
			return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, fmt.Errorf("failed to write sbom report: %w", err)
		}
	}

	return VulnerabilityReports{}, []v1alpha1.ExposedSecretReport{}, &SbomReports{}, nil
}

func (r *ScanJobController) writeReportToFile(dir, filename string, data any) error {
	reportData, err := json.Marshal(data)
	if err != nil {
		return fmt.Errorf("failed to marshal report: %w", err)
	}
	reportPath := filepath.Join(dir, filename)
	err = os.WriteFile(reportPath, reportData, 0o600)
	if err != nil {
		return fmt.Errorf("failed to write report to %s: %w", reportPath, err)
	}
	return nil
}

func digestToResourceName(d string) string {
	// sha256:<64hex>  -> sha256-<first 63>
	d = strings.ReplaceAll(d, ":", "-")
	if len(d) > 252 {
		return d[:252]
	}
	return d
}

// imageToResourceName converts a full image reference to a valid resource name by hashing if necessary.
// This ensures the name is compatible with Kubernetes resource naming constraints.
func imageToResourceName(image string) string {
	// Replace invalid characters for Kubernetes resource names
	image = strings.ReplaceAll(image, "/", "-")
	image = strings.ReplaceAll(image, ":", "-")
	image = strings.ReplaceAll(image, "@", "-")
	// Kubernetes resource names must be lowercase and no longer than 253 characters
	if len(image) > 252 {
		return fmt.Sprintf("img-%x", sha256.Sum256([]byte(image)))[:252]
	}
	return image
}

// computeHashForLabel creates a short hash of the input string for use in Kubernetes labels.
// Labels have stricter length and character constraints, so we ensure a fixed-length hash.
func computeHashForLabel(input string) string {
	hash := sha256.Sum256([]byte(input))
	return fmt.Sprintf("hash-%x", hash)[:63] // Kubernetes label values must be 63 chars or less
}

func (r *ScanJobController) completedContainers(ctx context.Context, scanJob *batchv1.Job) ([]string, error) {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", scanJob.Namespace, scanJob.Name))

	statuses, err := r.GetTerminatedContainersStatusesByJob(ctx, scanJob)
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			log.V(1).Info("Cached job must have been deleted")
			return []string{}, nil
		}
		if kube.IsPodControlledByJobNotFound(err) {
			log.V(1).Info("Pod must have been deleted")
			return []string{}, nil
		}
		return nil, err
	}

	// Retrieve the annotation to get the container image getting scanned
	containerImagesAnnotation, ok := scanJob.Annotations["trivy-operator.container-images"]
	if !ok {
		log.Error(nil, "Missing trivy-operator.container-images annotation")
		return nil, errors.New("missing trivy-operator.container-images annotation")
	}

	// Parse the JSON string into a map
	containerImages := make(map[string]string)
	err = json.Unmarshal([]byte(containerImagesAnnotation), &containerImages)
	if err != nil {
		log.Error(err, "Failed to parse trivy-operator.container-images annotation")
		return nil, fmt.Errorf("failed to parse trivy-operator.container-images annotation: %w", err)
	}

	completedContainers := make([]string, 0)
	for containerName, status := range statuses {
		if status.ExitCode == 0 {
			completedContainers = append(completedContainers, containerName)
			continue
		}

		// Get the container image for logging
		image, ok := containerImages[containerName]
		if !ok {
			image = "unknown"
		}

		if strings.Contains(status.Message, "no child with platform linux") {
			// Override the reason to be more descriptive for unsupported images
			status.Reason = "UnsupportedPlatform"
			log.Info("Scan job container",
				"container", containerName,
				"image", image,
				"status.reason", status.Reason,
				"status.message", "Container image is using an unsupported platform.")
		} else {
			log.Error(nil, "Scan job container",
				"container", containerName,
				"image", image,
				"status.reason", status.Reason,
				"status.message", status.Message)
		}
	}
	return completedContainers, nil
}

func (r *ScanJobController) deleteJob(ctx context.Context, job *batchv1.Job) error {
	if job.Spec.TTLSecondsAfterFinished != nil {
		return nil
	}

	err := r.Client.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground))
	if err != nil {
		if k8sapierror.IsNotFound(err) {
			return nil
		}
		return fmt.Errorf("deleting job: %w", err)
	}
	return nil
}
